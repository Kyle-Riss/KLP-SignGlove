#!/usr/bin/env python3
"""
Scale-Aware GRU ëª¨ë¸ ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""
import re
import os
import json
from pathlib import Path
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend

def parse_log_file(log_file: str) -> Dict:
    """ë¡œê·¸ íŒŒì¼ì—ì„œ í•™ìŠµ ê²°ê³¼ ì¶”ì¶œ"""
    if not os.path.exists(log_file):
        print(f"âš ï¸  ë¡œê·¸ íŒŒì¼ ì—†ìŒ: {log_file}")
        return None
    
    with open(log_file, 'r') as f:
        content = f.read()
    
    results = {
        'model_name': Path(log_file).stem.replace('training_output_', ''),
        'train_acc': [],
        'val_acc': [],
        'test_acc': None,
        'train_loss': [],
        'val_loss': [],
        'test_loss': None,
        'best_val_acc': None,
        'best_val_loss': None,
        'final_epoch': None
    }
    
    # Extract test results (ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼)
    test_acc_match = re.search(r'test/accuracy.*?â”‚\s+([\d.]+)', content)
    test_loss_match = re.search(r'test/loss.*?â”‚\s+([\d.]+)', content)
    
    if test_acc_match:
        results['test_acc'] = float(test_acc_match.group(1))
    if test_loss_match:
        results['test_loss'] = float(test_loss_match.group(1))
    
    # Extract epoch-wise results
    epoch_pattern = r'Epoch \d+: 100%.*?val/loss=([\d.]+).*?val/accuracy=([\d.]+).*?train/loss=([\d.]+).*?train/accuracy=([\d.]+)'
    
    for match in re.finditer(epoch_pattern, content):
        val_loss, val_acc, train_loss, train_acc = match.groups()
        results['val_loss'].append(float(val_loss))
        results['val_acc'].append(float(val_acc))
        results['train_loss'].append(float(train_loss))
        results['train_acc'].append(float(train_acc))
    
    if results['val_acc']:
        results['best_val_acc'] = max(results['val_acc'])
        results['best_val_loss'] = min(results['val_loss'])
        results['final_epoch'] = len(results['val_acc'])
    
    return results

def count_parameters(model_name: str) -> int:
    """ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶”ì •"""
    # ì‹¤ì œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ëŠ” ë¡œê·¸ë‚˜ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥
    # ì—¬ê¸°ì„œëŠ” ì´ì „ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ ì¶”ì •
    param_counts = {
        'MSCSGRU': 71800,
        'MSCSGRU_ScaleAware': 95992,
        'MSCSGRU_ScaleHard': 95992,
        'MSCGRU_ScaleAware': 46648,
    }
    return param_counts.get(model_name, 0)

def create_comparison_plots(all_results: List[Dict]):
    """ë¹„êµ í”Œë¡¯ ìƒì„±"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Scale-Aware GRU Models Comparison', fontsize=16, fontweight='bold')
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    # 1. Validation Accuracy over Epochs
    ax = axes[0, 0]
    for i, result in enumerate(all_results):
        if result and result['val_acc']:
            epochs = range(1, len(result['val_acc']) + 1)
            ax.plot(epochs, result['val_acc'], label=result['model_name'], 
                   color=colors[i], linewidth=2, marker='o', markersize=3)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Validation Accuracy', fontsize=12)
    ax.set_title('Validation Accuracy over Epochs', fontsize=13, fontweight='bold')
    ax.legend(loc='lower right')
    ax.grid(True, alpha=0.3)
    
    # 2. Validation Loss over Epochs
    ax = axes[0, 1]
    for i, result in enumerate(all_results):
        if result and result['val_loss']:
            epochs = range(1, len(result['val_loss']) + 1)
            ax.plot(epochs, result['val_loss'], label=result['model_name'], 
                   color=colors[i], linewidth=2, marker='o', markersize=3)
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Validation Loss', fontsize=12)
    ax.set_title('Validation Loss over Epochs', fontsize=13, fontweight='bold')
    ax.legend(loc='upper right')
    ax.grid(True, alpha=0.3)
    
    # 3. Final Test Accuracy Comparison
    ax = axes[1, 0]
    model_names = [r['model_name'] for r in all_results if r and r['test_acc'] is not None]
    test_accs = [r['test_acc'] for r in all_results if r and r['test_acc'] is not None]
    
    bars = ax.bar(range(len(model_names)), test_accs, color=colors[:len(model_names)], alpha=0.7)
    ax.set_xticks(range(len(model_names)))
    ax.set_xticklabels(model_names, rotation=15, ha='right')
    ax.set_ylabel('Test Accuracy', fontsize=12)
    ax.set_title('Final Test Accuracy Comparison', fontsize=13, fontweight='bold')
    ax.set_ylim(0, 1.0)
    ax.grid(True, alpha=0.3, axis='y')
    
    # Add value labels on bars
    for bar, acc in zip(bars, test_accs):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{acc:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # 4. Parameter Count vs Test Accuracy
    ax = axes[1, 1]
    param_counts = [count_parameters(r['model_name']) for r in all_results if r and r['test_acc'] is not None]
    
    scatter = ax.scatter(param_counts, test_accs, c=colors[:len(model_names)], 
                        s=200, alpha=0.7, edgecolors='black', linewidth=2)
    
    for i, (x, y, name) in enumerate(zip(param_counts, test_accs, model_names)):
        ax.annotate(name, (x, y), xytext=(10, 10), textcoords='offset points',
                   fontsize=9, bbox=dict(boxstyle='round,pad=0.5', fc=colors[i], alpha=0.3))
    
    ax.set_xlabel('Parameter Count', fontsize=12)
    ax.set_ylabel('Test Accuracy', fontsize=12)
    ax.set_title('Parameter Efficiency', fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('scale_aware_comparison_plots.png', dpi=300, bbox_inches='tight')
    print("âœ… ë¹„êµ í”Œë¡¯ ì €ì¥: scale_aware_comparison_plots.png")

def print_comparison_table(all_results: List[Dict]):
    """ë¹„êµ í…Œì´ë¸” ì¶œë ¥"""
    print("\n" + "="*120)
    print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí‘œ")
    print("="*120)
    
    header = f"{'Model Name':<25} {'Params':<12} {'Best Val Acc':<15} {'Best Val Loss':<15} {'Test Acc':<12} {'Test Loss':<12}"
    print(header)
    print("-"*120)
    
    for result in all_results:
        if result:
            model_name = result['model_name']
            params = count_parameters(model_name)
            best_val_acc = result['best_val_acc'] if result['best_val_acc'] else 0.0
            best_val_loss = result['best_val_loss'] if result['best_val_loss'] else 0.0
            test_acc = result['test_acc'] if result['test_acc'] else 0.0
            test_loss = result['test_loss'] if result['test_loss'] else 0.0
            
            print(f"{model_name:<25} {params:<12,} {best_val_acc:<15.4f} {best_val_loss:<15.4f} {test_acc:<12.4f} {test_loss:<12.4f}")
    
    print("="*120)

def generate_markdown_report(all_results: List[Dict]):
    """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
    report = """# Scale-Aware GRU ëª¨ë¸ ë¹„êµ ê²°ê³¼

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| Model Name | Parameters | Best Val Acc | Best Val Loss | Test Acc | Test Loss |
|------------|------------|--------------|---------------|----------|-----------|
"""
    
    for result in all_results:
        if result:
            model_name = result['model_name']
            params = count_parameters(model_name)
            best_val_acc = result['best_val_acc'] if result['best_val_acc'] else 0.0
            best_val_loss = result['best_val_loss'] if result['best_val_loss'] else 0.0
            test_acc = result['test_acc'] if result['test_acc'] else 0.0
            test_loss = result['test_loss'] if result['test_loss'] else 0.0
            
            report += f"| {model_name} | {params:,} | {best_val_acc:.4f} | {best_val_loss:.4f} | {test_acc:.4f} | {test_loss:.4f} |\n"
    
    # Find best model
    best_model = max([r for r in all_results if r and r['test_acc']], 
                     key=lambda x: x['test_acc'] if x['test_acc'] else 0)
    
    report += f"""
## ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸

**{best_model['model_name']}**
- Test Accuracy: **{best_model['test_acc']:.4f}**
- Test Loss: {best_model['test_loss']:.4f}
- Parameters: {count_parameters(best_model['model_name']):,}

## ğŸ“ˆ ì£¼ìš” ë°œê²¬ì‚¬í•­

### 1. Scale-Aware êµ¬ì¡°ì˜ íš¨ê³¼
"""
    
    # Compare baseline vs scale-aware
    baseline = next((r for r in all_results if r and r['model_name'] == 'MSCSGRU'), None)
    scale_aware = next((r for r in all_results if r and r['model_name'] == 'MSCSGRU_ScaleAware'), None)
    
    if baseline and scale_aware and baseline['test_acc'] and scale_aware['test_acc']:
        improvement = (scale_aware['test_acc'] - baseline['test_acc']) * 100
        param_increase = ((count_parameters('MSCSGRU_ScaleAware') - count_parameters('MSCSGRU')) / 
                         count_parameters('MSCSGRU')) * 100
        
        report += f"""
- **ì •í™•ë„ í–¥ìƒ**: {improvement:+.2f}% ({baseline['test_acc']:.4f} â†’ {scale_aware['test_acc']:.4f})
- **íŒŒë¼ë¯¸í„° ì¦ê°€**: +{param_increase:.1f}% ({count_parameters('MSCSGRU'):,} â†’ {count_parameters('MSCSGRU_ScaleAware'):,})
- **íš¨ìœ¨ì„±**: {improvement/param_increase:.4f} (ì •í™•ë„ í–¥ìƒ / íŒŒë¼ë¯¸í„° ì¦ê°€ ë¹„ìœ¨)
"""
    
    report += """
### 2. Hard Functionsì˜ ì˜í–¥
"""
    
    scale_hard = next((r for r in all_results if r and r['model_name'] == 'MSCSGRU_ScaleHard'), None)
    
    if scale_aware and scale_hard and scale_aware['test_acc'] and scale_hard['test_acc']:
        hard_diff = (scale_hard['test_acc'] - scale_aware['test_acc']) * 100
        report += f"""
- **ì •í™•ë„ ì°¨ì´**: {hard_diff:+.2f}% (ScaleAware: {scale_aware['test_acc']:.4f} vs ScaleHard: {scale_hard['test_acc']:.4f})
- **ê²°ë¡ **: Hard functionsëŠ” ì •í™•ë„ë¥¼ {'ìœ ì§€í•˜ë©´ì„œ' if abs(hard_diff) < 1 else 'ì•½ê°„ ê°ì†Œì‹œí‚¤ì§€ë§Œ'} ê³„ì‚° íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒ
"""
    
    report += """
### 3. Single vs Stacked GRU
"""
    
    single_gru = next((r for r in all_results if r and r['model_name'] == 'MSCGRU_ScaleAware'), None)
    
    if scale_aware and single_gru and scale_aware['test_acc'] and single_gru['test_acc']:
        stacked_advantage = (scale_aware['test_acc'] - single_gru['test_acc']) * 100
        report += f"""
- **Stacked GRU ì´ì **: {stacked_advantage:+.2f}% (Single: {single_gru['test_acc']:.4f} vs Stacked: {scale_aware['test_acc']:.4f})
- **íŒŒë¼ë¯¸í„° ëŒ€ë¹„ ì„±ëŠ¥**: Stacked êµ¬ì¡°ê°€ ë” ë§ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ ì„±ëŠ¥ í–¥ìƒ ì œê³µ
"""
    
    report += """
## ğŸ’¡ ê²°ë¡ 

Scale-Aware GRU êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì„ ì œê³µí•©ë‹ˆë‹¤:

1. **í–¥ìƒëœ í‘œí˜„ë ¥**: ê° CNN ìŠ¤ì¼€ì¼ì— ë…ë¦½ì ì¸ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ë” í’ë¶€í•œ íŠ¹ì§• í•™ìŠµ
2. **í•´ì„ ê°€ëŠ¥ì„±**: ìŠ¤ì¼€ì¼ë³„ ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•˜ì—¬ ëª¨ë¸ì˜ ì˜ì‚¬ê²°ì • ê³¼ì • ì´í•´ ê°€ëŠ¥
3. **ì„ë² ë””ë“œ ìµœì í™”**: Hard functions ì‚¬ìš©ìœ¼ë¡œ ê³„ì‚° íš¨ìœ¨ì„± í–¥ìƒ (ì •í™•ë„ ì†ì‹¤ ìµœì†Œí™”)
4. **ìœ ì—°ì„±**: Single/Stacked êµ¬ì¡° ì„ íƒìœ¼ë¡œ ì„±ëŠ¥-íš¨ìœ¨ì„± íŠ¸ë ˆì´ë“œì˜¤í”„ ì¡°ì ˆ ê°€ëŠ¥

## ğŸ“Š ì‹œê°í™”

![Comparison Plots](scale_aware_comparison_plots.png)

---
*ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    with open('SCALE_AWARE_RESULTS.md', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥: SCALE_AWARE_RESULTS.md")

def main():
    from datetime import datetime
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  Scale-Aware GRU ê²°ê³¼ ë¶„ì„                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    log_files = [
        "training_output_MSCSGRU.log",
        "training_output_MSCSGRU_ScaleAware.log",
        "training_output_MSCSGRU_ScaleHard.log",
        "training_output_MSCGRU_ScaleAware.log",
    ]
    
    print("ğŸ“ ë¡œê·¸ íŒŒì¼ ë¶„ì„ ì¤‘...")
    all_results = []
    
    for log_file in log_files:
        print(f"  - {log_file}...", end=' ')
        result = parse_log_file(log_file)
        if result:
            print("âœ…")
            all_results.append(result)
        else:
            print("âŒ")
    
    if not all_results:
        print("\nâŒ ë¶„ì„í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nâœ… {len(all_results)}ê°œ ëª¨ë¸ ê²°ê³¼ ë¶„ì„ ì™„ë£Œ")
    
    # Print comparison table
    print_comparison_table(all_results)
    
    # Create plots
    print("\nğŸ“Š ë¹„êµ í”Œë¡¯ ìƒì„± ì¤‘...")
    create_comparison_plots(all_results)
    
    # Generate markdown report
    print("\nğŸ“ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    generate_markdown_report(all_results)
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                         ë¶„ì„ ì™„ë£Œ!                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ìƒì„±ëœ íŒŒì¼:
  ğŸ“Š scale_aware_comparison_plots.png - ë¹„êµ í”Œë¡¯
  ğŸ“ SCALE_AWARE_RESULTS.md - ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸

ë‹¤ìŒ ë‹¨ê³„:
  1. SCALE_AWARE_RESULTS.md í™•ì¸
  2. scale_aware_comparison_plots.png í™•ì¸
  3. ìŠ¤ì¼€ì¼ ì¤‘ìš”ë„ ë¶„ì„: python3 analyze_scale_importance.py
    """)

if __name__ == "__main__":
    main()

