"""
SignGlove ì¶”ë¡  ì—”ì§„

ëª¨ë¸ ë¡œë”©, ì „ì²˜ë¦¬, ì¶”ë¡ , í›„ì²˜ë¦¬ë¥¼ í†µí•©í•œ ê³ ìˆ˜ì¤€ API
"""

import torch
import numpy as np
from pathlib import Path
from typing import Union, List, Dict, Optional
import warnings

from .models.mscsgru_inference import MSCSGRUInference
from .models.ms3dgru_inference import MS3DGRUInference
from .models.ms3dstackedgru_inference import MS3DStackedGRUInference
from .models.gru_inference import GRUInference
from .utils.preprocessor import InferencePreprocessor
from .utils.postprocessor import InferencePostprocessor


class SignGloveInference:
    """
    SignGlove í†µí•© ì¶”ë¡  ì—”ì§„
    
    ëª¨ë¸ ë¡œë”©ë¶€í„° ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ ê´€ë¦¬
    ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ê³ ìˆ˜ì¤€ API ì œê³µ
    
    Example:
        >>> engine = SignGloveInference(
        ...     model_path='best_model.ckpt',
        ...     model_type='MS3DGRU',
        ...     device='cpu'
        ... )
        >>> result = engine.predict_single(sensor_data)
    """
    
    # ëª¨ë¸ë³„ ê¸°ë³¸ ì„¤ì •
    MODEL_CONFIGS = {
        'GRU': {
            'class': GRUInference,
            'default_params': {'layers': 2, 'dropout': 0.2}
        },
        'MS3DGRU': {
            'class': MS3DGRUInference,
            'default_params': {'cnn_filters': 32, 'dropout': 0.1}
        },
        'MS3DStackedGRU': {
            'class': MS3DStackedGRUInference,
            'default_params': {'cnn_filters': 32, 'dropout': 0.05}
        },
        'MSCSGRU': {
            'class': MSCSGRUInference,
            'default_params': {'cnn_filters': 32, 'dropout': 0.3}
        }
    }
    
    def __init__(
        self,
        model_path: str,
        model_type: str = 'MS3DGRU',
        input_size: int = 8,
        hidden_size: int = 64,
        classes: int = 24,
        cnn_filters: Optional[int] = None,
        dropout: Optional[float] = None,
        target_timesteps: int = 87,
        device: Optional[str] = None,
        class_names: Optional[List[str]] = None,
        scaler_path: Optional[str] = None,
        single_predict_device: str = 'cpu',
        enable_dtw: bool = False
    ):
        """
        Args:
            model_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            model_type: ëª¨ë¸ íƒ€ìž… ('GRU', 'MS3DGRU', 'MS3DStackedGRU', 'MSCSGRU')
            input_size: ìž…ë ¥ ì±„ë„ ìˆ˜ (default: 8)
            hidden_size: ížˆë“  ì‚¬ì´ì¦ˆ (default: 64)
            classes: í´ëž˜ìŠ¤ ìˆ˜ (default: 24)
            cnn_filters: CNN í•„í„° ìˆ˜ (Noneì´ë©´ ëª¨ë¸ë³„ ê¸°ë³¸ê°’ ì‚¬ìš©)
            dropout: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ (Noneì´ë©´ ëª¨ë¸ë³„ ê¸°ë³¸ê°’ ì‚¬ìš©)
            target_timesteps: íƒ€ìž„ìŠ¤í… ê¸¸ì´ (default: 87)
            device: ë””ë°”ì´ìŠ¤ ('cuda', 'cpu', None=ìžë™)
            class_names: í´ëž˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            scaler_path: StandardScaler íŒŒì¼ ê²½ë¡œ
            single_predict_device: ë‹¨ì¼ ì˜ˆì¸¡ ì‹œ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
            enable_dtw: DTW ì‚¬ìš© ì—¬ë¶€ (í˜„ìž¬ ë¯¸êµ¬í˜„)
        """
        self.model_path = Path(model_path)
        self.model_type = model_type
        self.target_timesteps = target_timesteps
        
        # ëª¨ë¸ íƒ€ìž… ê²€ì¦
        if model_type not in self.MODEL_CONFIGS:
            raise ValueError(
                f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ìž…: {model_type}. "
                f"ì§€ì› ëª¨ë¸: {list(self.MODEL_CONFIGS.keys())}"
            )
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        print(f"ðŸš€ SignGlove ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”...")
        print(f"  ëª¨ë¸ íƒ€ìž…: {model_type}")
        print(f"  ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ë¡œë”©
        self.model = self._load_model(
            input_size=input_size,
            hidden_size=hidden_size,
            classes=classes,
            cnn_filters=cnn_filters,
            dropout=dropout
        )
        self.model.to(self.device)
        
        # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.preprocessor = self._init_preprocessor(
            scaler_path=scaler_path,
            target_timesteps=target_timesteps,
            input_size=input_size
        )
        
        # í›„ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.postprocessor = InferencePostprocessor(class_names=class_names)

        # ì˜µì…˜ ì €ìž¥
        self.single_predict_device = single_predict_device or 'cpu'
        self.enable_dtw = bool(enable_dtw)
        
        print(f"âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
        print(f"  íŒŒë¼ë¯¸í„° ìˆ˜: {self.model.count_parameters():,}")
        print(f"  í´ëž˜ìŠ¤ ìˆ˜: {classes}")
    
    def _load_checkpoint_state_dict(self, checkpoint_path: Path) -> dict:
        """
        ì²´í¬í¬ì¸íŠ¸ì—ì„œ state_dict ë¡œë“œ (ê³µí†µ ë¡œì§)
        
        Args:
            checkpoint_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            state_dict: ì •ì œëœ state_dict
        """
        try:
            checkpoint = torch.load(str(checkpoint_path), map_location='cpu')
        except Exception as e:
            raise RuntimeError(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {checkpoint_path}\nì˜¤ë¥˜: {e}")
        
        # state_dict ì¶”ì¶œ
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # 'model.' ì ‘ë‘ì‚¬ ì œê±°
        cleaned_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('model.'):
                cleaned_key = key[6:]  # 'model.' ì œê±°
                cleaned_state_dict[cleaned_key] = value
            else:
                cleaned_state_dict[key] = value
        
        return cleaned_state_dict
    
    def _load_model(self, **model_kwargs):
        """
        ëª¨ë¸ ë¡œë”© (ê°œì„ ëœ ë²„ì „ - ì¤‘ë³µ ì½”ë“œ ì œê±°)
        
        Args:
            **model_kwargs: ëª¨ë¸ ì´ˆê¸°í™” ì¸ìž
            
        Returns:
            model: ë¡œë“œëœ ëª¨ë¸
        """
        model_config = self.MODEL_CONFIGS[self.model_type]
        model_class = model_config['class']
        default_params = model_config['default_params']
        
        # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° ì¤€ë¹„ (ê¸°ë³¸ê°’ ì‚¬ìš©)
        final_params = {
            'input_size': model_kwargs.get('input_size', 8),
            'hidden_size': model_kwargs.get('hidden_size', 64),
            'classes': model_kwargs.get('classes', 24),
        }
        
        # ëª¨ë¸ë³„ íŠ¹ìˆ˜ íŒŒë¼ë¯¸í„° ì¶”ê°€
        for param_name, default_value in default_params.items():
            # ì‚¬ìš©ìžê°€ ëª…ì‹œì ìœ¼ë¡œ ì œê³µí•œ ê°’ì´ ìžˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
            user_value = model_kwargs.get(param_name)
            final_params[param_name] = user_value if user_value is not None else default_value
        
        # MSCSGRUëŠ” from_checkpoint ì‚¬ìš©
        if self.model_type == 'MSCSGRU':
            model = model_class.from_checkpoint(
                str(self.model_path),
                **final_params
            )
        else:
            # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ ì§ì ‘ ë¡œë“œ
            model = model_class(**final_params)
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            if self.model_path.exists():
                state_dict = self._load_checkpoint_state_dict(self.model_path)
                try:
                    model.load_state_dict(state_dict, strict=False)
                except Exception as e:
                    warnings.warn(
                        f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘ ì¼ë¶€ íŒŒë¼ë¯¸í„° ë¶ˆì¼ì¹˜: {e}\n"
                        f"ëª¨ë¸ì´ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤."
                    )
            else:
                warnings.warn(
                    f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}\n"
                    f"ëª¨ë¸ì´ ì´ˆê¸°í™”ëœ ê°€ì¤‘ì¹˜ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤."
                )
            
            model.eval()
        
        return model
    
    def _init_preprocessor(
        self,
        scaler_path: Optional[str],
        target_timesteps: int,
        input_size: int
    ) -> InferencePreprocessor:
        """
        ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (ê°œì„ ëœ ë²„ì „ - ëª…í™•í•œ ê²½ê³ )
        
        Args:
            scaler_path: Scaler íŒŒì¼ ê²½ë¡œ
            target_timesteps: íƒ€ê²Ÿ íƒ€ìž„ìŠ¤í…
            input_size: ìž…ë ¥ ì±„ë„ ìˆ˜
            
        Returns:
            preprocessor: ì „ì²˜ë¦¬ê¸° ì¸ìŠ¤í„´ìŠ¤
        """
        # ìŠ¤ì¼€ì¼ëŸ¬ ê²½ë¡œ ê²°ì •
        if scaler_path is None:
            scaler_path = str(self.model_path.parent / 'scaler.pkl')
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì‹œë„
        try:
            preprocessor = InferencePreprocessor.load_scaler(
                scaler_path,
                target_timesteps=target_timesteps,
                n_channels=input_size
            )
            print(f"  âœ… Scaler ë¡œë“œ ì„±ê³µ: {scaler_path}")
        except FileNotFoundError:
            warnings.warn(
                f"âš ï¸  Scaler íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scaler_path}\n"
                f"   ì •ê·œí™” ì—†ì´ ì¶”ë¡ ì„ ì§„í–‰í•©ë‹ˆë‹¤. ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n"
                f"   í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ scaler.pkl íŒŒì¼ì„ ì œê³µí•˜ëŠ” ê²ƒì„ ê¶Œìž¥í•©ë‹ˆë‹¤."
            )
            preprocessor = InferencePreprocessor(
                target_timesteps=target_timesteps,
                n_channels=input_size,
                scaler=None
            )
        
        return preprocessor
    
    def predict_single(
        self,
        raw_data: Union[np.ndarray, List[List[float]]],
        top_k: int = 5,
        return_all_info: bool = True
    ) -> Dict:
        """
        ë‹¨ì¼ ìƒ˜í”Œ ì˜ˆì¸¡
        
        Args:
            raw_data: ì›ì‹œ ì„¼ì„œ ë°ì´í„° (timesteps, channels)
            top_k: ìƒìœ„ Kê°œ í´ëž˜ìŠ¤ ë°˜í™˜
            return_all_info: Trueì´ë©´ ëª¨ë“  ì •ë³´ ë°˜í™˜, Falseì´ë©´ ìµœìƒìœ„ ì˜ˆì¸¡ë§Œ
        
        Returns:
            result: ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
                - predicted_class: ì˜ˆì¸¡ëœ í´ëž˜ìŠ¤ëª…
                - predicted_class_idx: ì˜ˆì¸¡ëœ í´ëž˜ìŠ¤ ì¸ë±ìŠ¤
                - confidence: ì˜ˆì¸¡ í™•ë¥ 
                - top_k_predictions: ìƒìœ„ Kê°œ ì˜ˆì¸¡ ë¦¬ìŠ¤íŠ¸
        """
        # ì „ì²˜ë¦¬
        x = self.preprocessor.preprocess_single(raw_data, normalize=True)
        
        # ë‹¨ì¼ ìƒ˜í”Œì€ latency ìµœì†Œí™”ë¥¼ ìœ„í•´ ì§€ì •ëœ ë””ë°”ì´ìŠ¤ì—ì„œ ì²˜ë¦¬
        run_device = torch.device(self.single_predict_device)
        x = x.to(run_device)
        
        # ì¶”ë¡  (í•„ìš” ì‹œ ìž„ì‹œë¡œ ëª¨ë¸ì„ í•´ë‹¹ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™)
        original_device = next(self.model.parameters()).device
        if original_device != run_device:
            self.model.to(run_device)
        
        logits = self.model.predict(x)
        
        if original_device != run_device:
            self.model.to(original_device)
        
        # í›„ì²˜ë¦¬
        if return_all_info:
            result = self.postprocessor.format_single_prediction(logits, top_k=top_k)
        else:
            predicted_class, confidence = self.postprocessor.logits_to_class(logits)
            result = {
                'predicted_class': self.postprocessor.class_names[predicted_class.item()],
                'confidence': float(confidence.item())
            }
        
        return result
    
    def predict_batch(
        self,
        raw_data_list: List[Union[np.ndarray, List[List[float]]]],
        top_k: int = 5
    ) -> List[Dict]:
        """
        ë°°ì¹˜ ì˜ˆì¸¡
        
        Args:
            raw_data_list: ì›ì‹œ ì„¼ì„œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
            top_k: ìƒìœ„ Kê°œ í´ëž˜ìŠ¤ ë°˜í™˜
        
        Returns:
            results: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ì „ì²˜ë¦¬
        x = self.preprocessor.preprocess_batch(raw_data_list, normalize=True)
        x = x.to(self.device)
        
        # ì¶”ë¡ 
        logits = self.model.predict(x)
        
        # í›„ì²˜ë¦¬
        results = self.postprocessor.format_batch_predictions(logits, top_k=top_k)
        
        return results
    
    def predict_with_details(
        self,
        raw_data: Union[np.ndarray, List[List[float]]]
    ) -> Dict:
        """
        ìƒì„¸ ì •ë³´ë¥¼ í¬í•¨í•œ ì˜ˆì¸¡
        
        Args:
            raw_data: ì›ì‹œ ì„¼ì„œ ë°ì´í„° (timesteps, channels)
        
        Returns:
            result: ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼
                - predicted_class: ì˜ˆì¸¡ í´ëž˜ìŠ¤
                - confidence: ì˜ˆì¸¡ í™•ë¥ 
                - top_k_predictions: ìƒìœ„ Kê°œ ì˜ˆì¸¡
                - all_class_probabilities: ëª¨ë“  í´ëž˜ìŠ¤ì˜ í™•ë¥ 
                - input_shape: ìž…ë ¥ ë°ì´í„° shape
        """
        # ìž…ë ¥ ì •ë³´
        if isinstance(raw_data, list):
            raw_data = np.array(raw_data)
        input_shape = raw_data.shape
        
        # ì „ì²˜ë¦¬
        x = self.preprocessor.preprocess_single(raw_data, normalize=True)
        x = x.to(self.device)
        
        # ì¶”ë¡ 
        logits = self.model.predict(x)
        
        # í›„ì²˜ë¦¬
        result = self.postprocessor.format_single_prediction(logits, top_k=5)
        
        # ëª¨ë“  í´ëž˜ìŠ¤ì˜ í™•ë¥  ì¶”ê°€
        all_probs = self.postprocessor.get_class_probabilities(logits)
        result['all_class_probabilities'] = all_probs
        result['input_shape'] = input_shape
        
        return result
    
    def get_model_info(self) -> Dict:
        """
        ëª¨ë¸ ì •ë³´ ë°˜í™˜
        
        Returns:
            info: ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        info = self.model.get_model_info()
        info.update({
            'device': str(self.device),
            'target_timesteps': self.target_timesteps,
            'model_path': str(self.model_path),
            'class_names': self.postprocessor.class_names
        })
        return info
    
    def print_prediction(self, prediction: Dict):
        """
        ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
        
        Args:
            prediction: predict_single ë˜ëŠ” predict_with_detailsì˜ ë°˜í™˜ê°’
        """
        self.postprocessor.print_prediction(prediction)


# íŽ¸ì˜ í•¨ìˆ˜
def load_inference_engine(
    model_path: str,
    model_type: str = 'MS3DGRU',
    device: Optional[str] = None,
    **kwargs
) -> SignGloveInference:
    """
    ì¶”ë¡  ì—”ì§„ ë¡œë”© íŽ¸ì˜ í•¨ìˆ˜
    
    Args:
        model_path: ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        model_type: ëª¨ë¸ íƒ€ìž…
        device: ë””ë°”ì´ìŠ¤
        **kwargs: SignGloveInference ì´ˆê¸°í™” ì¸ìž
    
    Returns:
        engine: ì¶”ë¡  ì—”ì§„
        
    Example:
        >>> engine = load_inference_engine(
        ...     'best_model.ckpt',
        ...     model_type='MS3DGRU',
        ...     device='cpu'
        ... )
    """
    return SignGloveInference(
        model_path=model_path,
        model_type=model_type,
        device=device,
        **kwargs
    )
