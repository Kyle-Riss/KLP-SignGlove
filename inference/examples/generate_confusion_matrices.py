#!/usr/bin/env python3
"""
4ê°œ ëª¨ë¸ì— ëŒ€í•œ í˜¼ë™ í–‰ë ¬ ìƒì„±: GRU, StackedGRU, MS3DGRU, MS3DStackedGRU
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics import (
    confusion_matrix, accuracy_score, f1_score,
    precision_score, recall_score, classification_report
)
from tqdm import tqdm
import torch

from src.misc.DynamicDataModule import DynamicDataModule
from inference import SignGloveInference

# í•œê¸€ í°íŠ¸ ì„¤ì •
def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    import matplotlib.font_manager as fm
    available_fonts = [f.name for f in fm.fontManager.ttflist if 'nanum' in f.name.lower()]
    if available_fonts:
        preferred_fonts = ['NanumGothic', 'NanumBarunGothic', 'NanumSquare']
        for font in preferred_fonts:
            if font in available_fonts:
                plt.rcParams['font.family'] = font
                print(f"âœ… Korean font setup complete: {font}")
                return True
        plt.rcParams['font.family'] = available_fonts[0]
        print(f"âœ… Korean font setup complete: {available_fonts[0]}")
        return True
    print("âš ï¸  Korean font not found. Using English labels.")
    plt.rcParams['font.family'] = 'DejaVu Sans'
    return False

# í•œê¸€ í´ë˜ìŠ¤ëª…
CLASS_NAMES = [
    'ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…',
    'ã…', 'ã…‘', 'ã…“', 'ã…•', 'ã…—', 'ã…›', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£'
]

def generate_confusion_matrix_for_model(model_name, config, test_loader, output_dir):
    """ë‹¨ì¼ ëª¨ë¸ì— ëŒ€í•œ í˜¼ë™ í–‰ë ¬ ìƒì„±"""
    
    print(f"\nğŸ¤– ëª¨ë¸: {model_name} í˜¼ë™ í–‰ë ¬ ìƒì„± ì¤‘...")
    
    try:
        # ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
        init_params = {
            'model_path': config['path'],
            'model_type': config['type'],
            'input_size': 8,
            'hidden_size': config.get('hidden_size', 64),
            'classes': 24,
            'device': 'cpu',
            'dropout': config.get('dropout', 0.2),
            'scaler_path': config.get('scaler_path', None)  # Scaler ê²½ë¡œ ì¶”ê°€
        }
        if 'cnn_filters' in config:
            init_params['cnn_filters'] = config['cnn_filters']
        # GRU ëª¨ë¸ì€ layers=1ë¡œ ì„¤ì • (ì²´í¬í¬ì¸íŠ¸ì™€ ì¼ì¹˜)
        if config['type'] == 'GRU':
            init_params['layers'] = config.get('layers', 1)
        
        engine = SignGloveInference(**init_params)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì—ì„œ ì˜ˆì¸¡ ìˆ˜í–‰
        all_predictions = []
        all_labels = []
        
        print(f"  ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì˜ˆì¸¡ ì¤‘...")
        for batch in tqdm(test_loader, desc=f"  {model_name}"):
            measurements = batch['measurement'].numpy()  # (batch_size, timesteps, channels)
            labels = batch['label'].numpy()  # (batch_size,)
            
            # ë°°ì¹˜ë³„ë¡œ ì˜ˆì¸¡
            for i in range(len(measurements)):
                sample = measurements[i]  # (timesteps, channels)
                label = labels[i]
                
                # ì˜ˆì¸¡ ìˆ˜í–‰ (DynamicDataModuleì˜ ë°ì´í„°ëŠ” ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ normalize=False)
                result = engine.predict_single(sample, return_all_info=True, normalize=False)
                predicted_class_idx = result['predicted_class_idx']  # ì¸ë±ìŠ¤ ì‚¬ìš©
                
                all_predictions.append(predicted_class_idx)
                all_labels.append(int(label))  # ëª…ì‹œì ìœ¼ë¡œ intë¡œ ë³€í™˜
        
        all_predictions = np.array(all_predictions, dtype=np.int32)
        all_labels = np.array(all_labels, dtype=np.int32)
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        accuracy = accuracy_score(all_labels, all_predictions)
        f1 = f1_score(all_labels, all_predictions, average='weighted')
        precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)
        recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)
        
        print(f"  âœ… ì™„ë£Œ!")
        print(f"     Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"     F1-Score: {f1:.4f}")
        print(f"     Precision: {precision:.4f}")
        print(f"     Recall: {recall:.4f}")
        
        # í˜¼ë™ í–‰ë ¬ ìƒì„±
        cm = confusion_matrix(all_labels, all_predictions, labels=range(24))
        
        # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        fig, ax = plt.subplots(figsize=(14, 12))
        
        # ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬ (í¼ì„¼íŠ¸)
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        cm_normalized = np.nan_to_num(cm_normalized)  # NaN ì²˜ë¦¬
        
        # íˆíŠ¸ë§µ ìƒì„±
        sns.heatmap(
            cm_normalized,
            annot=True,
            fmt='.2f',
            cmap='Blues',
            xticklabels=CLASS_NAMES,
            yticklabels=CLASS_NAMES,
            ax=ax,
            cbar_kws={'label': 'Normalized Count'}
        )
        
        ax.set_xlabel('Predicted Label', fontsize=14)
        ax.set_ylabel('True Label', fontsize=14)
        ax.set_title(
            f'Confusion Matrix - {model_name}\n'
            f'Accuracy: {accuracy*100:.2f}% | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}',
            fontsize=16,
            fontweight='bold'
        )
        
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        # ì €ì¥
        output_file = output_dir / f'confusion_matrix_{model_name.lower()}.png'
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_file}")
        
        # ì •ê·œí™”ë˜ì§€ ì•Šì€ í˜¼ë™ í–‰ë ¬ë„ ì €ì¥ (ì›ë³¸ ìˆ«ì)
        fig, ax = plt.subplots(figsize=(14, 12))
        sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=CLASS_NAMES,
            yticklabels=CLASS_NAMES,
            ax=ax,
            cbar_kws={'label': 'Count'}
        )
        ax.set_xlabel('Predicted Label', fontsize=14)
        ax.set_ylabel('True Label', fontsize=14)
        ax.set_title(
            f'Confusion Matrix (Raw Counts) - {model_name}\n'
            f'Total Samples: {len(all_labels)}',
            fontsize=16,
            fontweight='bold'
        )
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        output_file_raw = output_dir / f'confusion_matrix_{model_name.lower()}_raw.png'
        plt.savefig(output_file_raw, dpi=300, bbox_inches='tight')
        plt.close()
        
        return {
            'model': model_name,
            'accuracy': accuracy,
            'f1_score': f1,
            'precision': precision,
            'recall': recall,
            'confusion_matrix': cm,
            'confusion_matrix_normalized': cm_normalized,
            'total_samples': len(all_labels)
        }
        
    except Exception as e:
        print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None

def create_comparison_visualization(results, output_dir):
    """ëª¨ë“  ëª¨ë¸ì˜ í˜¼ë™ í–‰ë ¬ì„ ë¹„êµí•˜ëŠ” ì‹œê°í™”"""
    
    if not results or len([r for r in results if r is not None]) == 0:
        print("âš ï¸  ë¹„êµ ì‹œê°í™”ë¥¼ ìƒì„±í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    valid_results = [r for r in results if r is not None]
    
    # 1. ì„±ëŠ¥ ì§€í‘œ ë¹„êµ (Bar Plot)
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    models = [r['model'] for r in valid_results]
    accuracies = [r['accuracy'] * 100 for r in valid_results]
    f1_scores = [r['f1_score'] for r in valid_results]
    precisions = [r['precision'] for r in valid_results]
    recalls = [r['recall'] for r in valid_results]
    
    # Accuracy
    axes[0, 0].bar(models, accuracies, color='skyblue')
    axes[0, 0].set_title('Accuracy Comparison (%)', fontsize=14, fontweight='bold')
    axes[0, 0].set_ylabel('Accuracy (%)', fontsize=12)
    axes[0, 0].set_ylim([min(accuracies) - 2, 100])
    for i, v in enumerate(accuracies):
        axes[0, 0].text(i, v + 0.5, f'{v:.2f}%', ha='center', va='bottom')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # F1-Score
    axes[0, 1].bar(models, f1_scores, color='lightgreen')
    axes[0, 1].set_title('F1-Score Comparison', fontsize=14, fontweight='bold')
    axes[0, 1].set_ylabel('F1-Score', fontsize=12)
    axes[0, 1].set_ylim([min(f1_scores) - 0.02, 1.0])
    for i, v in enumerate(f1_scores):
        axes[0, 1].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # Precision
    axes[1, 0].bar(models, precisions, color='lightcoral')
    axes[1, 0].set_title('Precision Comparison', fontsize=14, fontweight='bold')
    axes[1, 0].set_ylabel('Precision', fontsize=12)
    axes[1, 0].set_ylim([min(precisions) - 0.02, 1.0])
    for i, v in enumerate(precisions):
        axes[1, 0].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # Recall
    axes[1, 1].bar(models, recalls, color='gold')
    axes[1, 1].set_title('Recall Comparison', fontsize=14, fontweight='bold')
    axes[1, 1].set_ylabel('Recall', fontsize=12)
    axes[1, 1].set_ylim([min(recalls) - 0.02, 1.0])
    for i, v in enumerate(recalls):
        axes[1, 1].text(i, v + 0.01, f'{v:.4f}', ha='center', va='bottom')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    comparison_file = output_dir / 'confusion_matrix_comparison.png'
    plt.savefig(comparison_file, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ë¹„êµ ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {comparison_file}")
    
    # 2. ëª¨ë“  ëª¨ë¸ì˜ í˜¼ë™ í–‰ë ¬ì„ í•œ ë²ˆì— ë³´ê¸° (2x2 ê·¸ë¦¬ë“œ)
    fig, axes = plt.subplots(2, 2, figsize=(20, 20))
    axes = axes.flatten()
    
    for idx, result in enumerate(valid_results):
        if idx >= 4:
            break
        ax = axes[idx]
        
        cm_norm = result['confusion_matrix_normalized']
        sns.heatmap(
            cm_norm,
            annot=True,
            fmt='.2f',
            cmap='Blues',
            xticklabels=CLASS_NAMES,
            yticklabels=CLASS_NAMES,
            ax=ax,
            cbar_kws={'label': 'Normalized Count'}
        )
        ax.set_title(
            f"{result['model']}\nAcc: {result['accuracy']*100:.2f}% | F1: {result['f1_score']:.4f}",
            fontsize=12,
            fontweight='bold'
        )
        ax.set_xlabel('Predicted', fontsize=10)
        ax.set_ylabel('True', fontsize=10)
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
    
    plt.tight_layout()
    grid_file = output_dir / 'confusion_matrix_grid_all_models.png'
    plt.savefig(grid_file, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… ê·¸ë¦¬ë“œ ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {grid_file}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    setup_korean_font()
    plt.rcParams['axes.unicode_minus'] = False
    
    output_dir = Path('visualizations/confusion_matrices')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("ğŸ“Š 4ê°œ ëª¨ë¸ í˜¼ë™ í–‰ë ¬ ìƒì„±: GRU, StackedGRU, MS3DGRU, MS3DStackedGRU")
    print("=" * 80)
    print()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
    print("ğŸ“Œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    data_dir = "/home/billy/25-1kp/SignGlove-DataAnalysis/unified/unified"
    datamodule = DynamicDataModule(
        data_dir=data_dir,
        time_steps=87,
        n_channels=8,
        batch_size=32,
        seed=42,
        use_test_split=True
    )
    datamodule.setup('test')
    test_loader = datamodule.test_dataloader()
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
    print(f"   í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(datamodule.test_dataset)}")
    print()
    
    # Scaler íŒŒì¼ ê²½ë¡œ (ëª¨ë“  ëª¨ë¸ì— ê³µí†µ)
    scaler_path = 'archive/checkpoints_backup/checkpoints_backup/scaler.pkl'
    
    # ëª¨ë¸ ì„¤ì • (ì›ë³¸ ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ì‚¬ìš©)
    models_config = {
        'GRU': {
            'path': 'checkpoints/best_model_epoch=epoch=92_val/loss=val/loss=0.04.ckpt',  # ì›ë³¸ ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ì‚¬ìš©
            'type': 'GRU',
            'hidden_size': 64,
            'layers': 1,  # ì²´í¬í¬ì¸íŠ¸ëŠ” layers=1ë¡œ í•™ìŠµë¨
            'dropout': 0.2,
            'scaler_path': scaler_path
        },
        'StackedGRU': {
            'path': 'checkpoints/best_model_epoch=epoch=68_val/loss=val/loss=0.19.ckpt',  # ìƒˆë¡œ ì¬í›ˆë ¨ëœ StackedGRU ì²´í¬í¬ì¸íŠ¸ (ìµœê³  ì„±ëŠ¥: 94.30%)
            'type': 'StackedGRU',
            'hidden_size': 64,
            'dropout': 0.2,
            'scaler_path': scaler_path
        },
        'MS3DGRU': {
            'path': 'best_model/ms3dgru_best.ckpt',
            'type': 'MS3DGRU',
            'cnn_filters': 32,
            'dropout': 0.1,
            'scaler_path': scaler_path
        },
        'MS3DStackedGRU': {
            'path': 'checkpoints/best_model_epoch=epoch=82_val/loss=val/loss=0.05.ckpt',  # ì›ë³¸ ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ì‚¬ìš©
            'type': 'MS3DStackedGRU',
            'cnn_filters': 32,
            'dropout': 0.05,
            'scaler_path': scaler_path
        }
    }
    
    # ê° ëª¨ë¸ì— ëŒ€í•´ í˜¼ë™ í–‰ë ¬ ìƒì„±
    all_results = []
    for model_name, config in models_config.items():
        result = generate_confusion_matrix_for_model(model_name, config, test_loader, output_dir)
        all_results.append(result)
    
    # ë¹„êµ ì‹œê°í™” ìƒì„±
    print("\nğŸ¨ ë¹„êµ ì‹œê°í™” ìƒì„± ì¤‘...")
    create_comparison_visualization(all_results, output_dir)
    
    # ìš”ì•½ ì •ë³´ ì €ì¥
    summary_path = output_dir / 'confusion_matrix_summary.txt'
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("ğŸ“Š í˜¼ë™ í–‰ë ¬ ê²°ê³¼ ìš”ì•½\n")
        f.write("=" * 80 + "\n\n")
        
        for result in all_results:
            if result is None:
                continue
            f.write(f"ëª¨ë¸: {result['model']}\n")
            f.write(f"  Accuracy: {result['accuracy']:.4f} ({result['accuracy']*100:.2f}%)\n")
            f.write(f"  F1-Score: {result['f1_score']:.4f}\n")
            f.write(f"  Precision: {result['precision']:.4f}\n")
            f.write(f"  Recall: {result['recall']:.4f}\n")
            f.write(f"  Total Samples: {result['total_samples']}\n")
            f.write("\n")
        
        f.write("=" * 80 + "\n")
        f.write("âœ… ì™„ë£Œ!\n")
        f.write("=" * 80 + "\n")
    
    print(f"âœ… ìš”ì•½ ì •ë³´ ì €ì¥ ì™„ë£Œ: {summary_path}")
    
    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  í˜¼ë™ í–‰ë ¬ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print("=" * 80)

if __name__ == "__main__":
    main()

