"""
ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
"""

import sys
sys.path.append('.')

import torch
import numpy as np
from pathlib import Path
from sklearn.metrics import (
    accuracy_score, f1_score, confusion_matrix,
    classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

from src.misc.DynamicDataModule import DynamicDataModule
from inference import SignGloveInference

# í•œê¸€ í´ë˜ìŠ¤ëª…
CLASS_NAMES = [
    'ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…',
    'ã…', 'ã…‘', 'ã…“', 'ã…•', 'ã…—', 'ã…›', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£'
]

def test_model_performance(
    model_path: str,
    model_type: str = 'MS3DGRU',
    data_dir: str = '/home/billy/25-1kp/SignGlove_HW/datasets/unified',
    batch_size: int = 32,
    device: str = 'cpu'
):
    """
    ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    
    Args:
        model_path: ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        model_type: ëª¨ë¸ íƒ€ì…
        data_dir: ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬
        batch_size: ë°°ì¹˜ í¬ê¸°
        device: ë””ë°”ì´ìŠ¤
    """
    print('=' * 80)
    print(f'ğŸ§ª ì‹¤ì „ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸: {model_type}')
    print('=' * 80)
    print()
    
    # 1. ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
    print('ğŸ“Œ ë‹¨ê³„ 1: ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”')
    print('-' * 80)
    try:
        engine = SignGloveInference(
            model_path=model_path,
            model_type=model_type,
            device=device
        )
        print(f'âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_type}')
        print(f'   íŒŒë¼ë¯¸í„° ìˆ˜: {engine.model.count_parameters():,}')
    except Exception as e:
        print(f'âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}')
        return
    
    print()
    
    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
    print('ğŸ“Œ ë‹¨ê³„ 2: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ')
    print('-' * 80)
    try:
        datamodule = DynamicDataModule(
            data_dir=data_dir,
            batch_size=batch_size,
            test_size=0.2,
            val_size=0.2,
            seed=42
        )
        datamodule.setup('test')
        test_loader = datamodule.test_dataloader()
        
        print(f'âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ')
        print(f'   ë°ì´í„° ë””ë ‰í† ë¦¬: {data_dir}')
        print(f'   ë°°ì¹˜ í¬ê¸°: {batch_size}')
        print(f'   í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ìˆ˜: {len(test_loader)}')
    except Exception as e:
        print(f'âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}')
        import traceback
        traceback.print_exc()
        return
    
    print()
    
    # 3. ì˜ˆì¸¡ ìˆ˜í–‰
    print('ğŸ“Œ ë‹¨ê³„ 3: ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...')
    print('-' * 80)
    
    all_predictions = []
    all_labels = []
    all_probs = []
    
    engine.model.eval()
    engine.model.to(device)
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(test_loader, desc='ì˜ˆì¸¡ ì§„í–‰')):
            x = batch['measurement']
            y = batch['label']
            x_padding = batch.get('measurement_padding', None)
            
            # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            x = x.to(device)
            y = y.to(device)
            
            # ì˜ˆì¸¡
            logits = engine.model.predict(x)
            probs = torch.softmax(logits, dim=-1)
            preds = torch.argmax(logits, dim=-1)
            
            all_predictions.extend(preds.cpu().numpy())
            all_labels.extend(y.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)
    
    print(f'âœ… ì˜ˆì¸¡ ì™„ë£Œ!')
    print(f'   ì´ ìƒ˜í”Œ ìˆ˜: {len(all_labels)}')
    print()
    
    # 4. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    print('ğŸ“Œ ë‹¨ê³„ 4: ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°')
    print('-' * 80)
    
    accuracy = accuracy_score(all_labels, all_predictions)
    f1_macro = f1_score(all_labels, all_predictions, average='macro')
    f1_weighted = f1_score(all_labels, all_predictions, average='weighted')
    f1_per_class = f1_score(all_labels, all_predictions, average=None)
    
    print(f'âœ… ì„±ëŠ¥ ì§€í‘œ:')
    print(f'   ì •í™•ë„ (Accuracy): {accuracy * 100:.2f}%')
    print(f'   F1-Score (Macro): {f1_macro:.4f}')
    print(f'   F1-Score (Weighted): {f1_weighted:.4f}')
    print()
    
    # í´ë˜ìŠ¤ë³„ ì •í™•ë„
    cm = confusion_matrix(all_labels, all_predictions, labels=range(24))
    class_accuracies = cm.diagonal() / cm.sum(axis=1)
    
    print('ğŸ“Š í´ë˜ìŠ¤ë³„ ì •í™•ë„ (ìƒìœ„ 10ê°œ):')
    class_acc_dict = {CLASS_NAMES[i]: acc for i, acc in enumerate(class_accuracies)}
    sorted_classes = sorted(class_acc_dict.items(), key=lambda x: x[1], reverse=True)[:10]
    for class_name, acc in sorted_classes:
        print(f'   {class_name}: {acc * 100:.2f}%')
    print()
    
    # ì˜¤ë¶„ë¥˜ ë¶„ì„
    print('ğŸ“Š ì˜¤ë¶„ë¥˜ ë¶„ì„:')
    wrong_predictions = all_predictions != all_labels
    num_wrong = wrong_predictions.sum()
    print(f'   ì˜ëª» ì˜ˆì¸¡: {num_wrong}ê°œ ({num_wrong / len(all_labels) * 100:.2f}%)')
    print(f'   ì •í™• ì˜ˆì¸¡: {len(all_labels) - num_wrong}ê°œ ({(len(all_labels) - num_wrong) / len(all_labels) * 100:.2f}%)')
    print()
    
    # 5. Confusion Matrix ìƒì„±
    print('ğŸ“Œ ë‹¨ê³„ 5: Confusion Matrix ìƒì„±')
    print('-' * 80)
    
    fig, axes = plt.subplots(1, 2, figsize=(20, 8))
    
    # ì •ê·œí™”ëœ Confusion Matrix
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,
                ax=axes[0], cbar_kws={'label': 'Count'})
    axes[0].set_title('Confusion Matrix (Count)', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Predicted', fontsize=12)
    axes[0].set_ylabel('True', fontsize=12)
    
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,
                ax=axes[1], cbar_kws={'label': 'Normalized'})
    axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Predicted', fontsize=12)
    axes[1].set_ylabel('True', fontsize=12)
    
    plt.tight_layout()
    
    output_dir = Path('inference/performance_visualizations')
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / f'real_test_confusion_matrix_{model_type.lower()}.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f'âœ… ì €ì¥: {output_file}')
    print()
    
    # 6. ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ë¶„ì„
    print('ğŸ“Œ ë‹¨ê³„ 6: ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ìƒì„¸ ë¶„ì„')
    print('-' * 80)
    
    wrong_indices = np.where(wrong_predictions)[0]
    
    if len(wrong_indices) > 0:
        print(f'ğŸ“‹ ì˜¤ë¶„ë¥˜ ìƒ˜í”Œ ìƒì„¸ (ìµœëŒ€ 10ê°œ):')
        for i, idx in enumerate(wrong_indices[:10]):
            true_label = all_labels[idx]
            pred_label = all_predictions[idx]
            true_prob = all_probs[idx][true_label]
            pred_prob = all_probs[idx][pred_label]
            
            print(f'   ìƒ˜í”Œ {i+1}:')
            print(f'     ì •ë‹µ: {CLASS_NAMES[true_label]} (í™•ë¥ : {true_prob:.4f})')
            print(f'     ì˜ˆì¸¡: {CLASS_NAMES[pred_label]} (í™•ë¥ : {pred_prob:.4f})')
            print(f'     ì°¨ì´: {pred_prob - true_prob:.4f}')
    else:
        print('   âœ… ëª¨ë“  ìƒ˜í”Œ ì •í™•íˆ ì˜ˆì¸¡!')
    
    print()
    
    # 7. í´ë˜ìŠ¤ë³„ ìƒì„¸ ë¦¬í¬íŠ¸
    print('ğŸ“Œ ë‹¨ê³„ 7: í´ë˜ìŠ¤ë³„ ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±')
    print('-' * 80)
    
    report = classification_report(
        all_labels, all_predictions,
        target_names=CLASS_NAMES,
        output_dict=True,
        zero_division=0
    )
    
    # ë¦¬í¬íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    report_file = output_dir / f'real_test_report_{model_type.lower()}.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f'ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì„±ëŠ¥ ë¦¬í¬íŠ¸\n')
        f.write(f'ëª¨ë¸: {model_type}\n')
        f.write(f'ì´ ìƒ˜í”Œ ìˆ˜: {len(all_labels)}\n')
        f.write(f'ì •í™•ë„: {accuracy * 100:.2f}%\n')
        f.write(f'F1-Score (Macro): {f1_macro:.4f}\n')
        f.write(f'F1-Score (Weighted): {f1_weighted:.4f}\n')
        f.write('\n')
        f.write(classification_report(all_labels, all_predictions, target_names=CLASS_NAMES))
    
    print(f'âœ… ë¦¬í¬íŠ¸ ì €ì¥: {report_file}')
    print()
    
    # 8. í´ë˜ìŠ¤ë³„ ì •í™•ë„ ì‹œê°í™”
    print('ğŸ“Œ ë‹¨ê³„ 8: í´ë˜ìŠ¤ë³„ ì •í™•ë„ ì‹œê°í™”')
    print('-' * 80)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))
    
    # í´ë˜ìŠ¤ë³„ ì •í™•ë„
    sorted_indices = np.argsort(class_accuracies)
    colors = ['red' if acc < 0.95 else 'green' if acc >= 0.98 else 'orange' 
              for acc in class_accuracies[sorted_indices]]
    
    bars = ax1.barh(range(len(CLASS_NAMES)), class_accuracies[sorted_indices], color=colors, alpha=0.7)
    ax1.set_yticks(range(len(CLASS_NAMES)))
    ax1.set_yticklabels([CLASS_NAMES[i] for i in sorted_indices], fontsize=10)
    ax1.set_xlabel('Accuracy', fontsize=12)
    ax1.set_title('Class-wise Accuracy', fontsize=14, fontweight='bold')
    ax1.axvline(x=0.95, color='orange', linestyle='--', alpha=0.5, label='95% threshold')
    ax1.axvline(x=0.98, color='green', linestyle='--', alpha=0.5, label='98% threshold')
    ax1.legend()
    ax1.grid(axis='x', alpha=0.3)
    
    # ê°’ í‘œì‹œ
    for i, (idx, acc) in enumerate(zip(sorted_indices, class_accuracies[sorted_indices])):
        ax1.text(acc + 0.01, i, f'{acc*100:.1f}%', va='center', fontsize=8)
    
    # F1-Score per class
    ax2.barh(range(len(CLASS_NAMES)), f1_per_class[sorted_indices], color=colors, alpha=0.7)
    ax2.set_yticks(range(len(CLASS_NAMES)))
    ax2.set_yticklabels([CLASS_NAMES[i] for i in sorted_indices], fontsize=10)
    ax2.set_xlabel('F1-Score', fontsize=12)
    ax2.set_title('Class-wise F1-Score', fontsize=14, fontweight='bold')
    ax2.axvline(x=0.95, color='orange', linestyle='--', alpha=0.5, label='95% threshold')
    ax2.axvline(x=0.98, color='green', linestyle='--', alpha=0.5, label='98% threshold')
    ax2.legend()
    ax2.grid(axis='x', alpha=0.3)
    
    # ê°’ í‘œì‹œ
    for i, (idx, f1) in enumerate(zip(sorted_indices, f1_per_class[sorted_indices])):
        ax2.text(f1 + 0.01, i, f'{f1:.3f}', va='center', fontsize=8)
    
    plt.tight_layout()
    
    accuracy_file = output_dir / f'real_test_class_accuracy_{model_type.lower()}.png'
    plt.savefig(accuracy_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f'âœ… ì €ì¥: {accuracy_file}')
    print()
    
    # 9. ìµœì¢… ìš”ì•½
    print('=' * 80)
    print('ğŸ“Š ìµœì¢… ì„±ëŠ¥ ìš”ì•½')
    print('=' * 80)
    print(f'ëª¨ë¸: {model_type}')
    print(f'í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(all_labels):,}')
    print(f'ì •í™•ë„: {accuracy * 100:.2f}%')
    print(f'F1-Score (Macro): {f1_macro:.4f}')
    print(f'F1-Score (Weighted): {f1_weighted:.4f}')
    print(f'ì˜¤ë¶„ë¥˜: {num_wrong}ê°œ ({num_wrong / len(all_labels) * 100:.2f}%)')
    print()
    print(f'âœ… 95% ì´ìƒ ì •í™•ë„ í´ë˜ìŠ¤: {(class_accuracies >= 0.95).sum()}/24')
    print(f'âœ… 98% ì´ìƒ ì •í™•ë„ í´ë˜ìŠ¤: {(class_accuracies >= 0.98).sum()}/24')
    print()
    print('ìƒì„±ëœ íŒŒì¼:')
    print(f'  â€¢ {output_file}')
    print(f'  â€¢ {accuracy_file}')
    print(f'  â€¢ {report_file}')
    print()
    print('=' * 80)


if __name__ == '__main__':
    # MS3DGRU ëª¨ë¸ í…ŒìŠ¤íŠ¸
    test_model_performance(
        model_path='best_model/ms3dgru_best.ckpt',
        model_type='MS3DGRU',
        data_dir='/home/billy/25-1kp/SignGlove_HW/datasets/unified',
        batch_size=32,
        device='cpu'
    )

