#!/usr/bin/env python3
"""
í†µê³„ ê²€ì¦ ê²°ê³¼ ë¹„êµí‘œ ìƒì„±
training_results í´ë”ì™€ ìœ ì‚¬í•œ í˜•ì‹ì˜ ë¹„êµí‘œ ìƒì„±
"""
import json
import re
from pathlib import Path
import numpy as np

def extract_dataset_statistics():
    """ë¡œê·¸ íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ë³„ í†µê³„ ì¶”ì¶œ"""
    log_dir = Path('lightning_logs')
    if not log_dir.exists():
        return None
    
    # ëª¨ë¸ ë§¤í•‘
    model_mapping = {
        'ms3dgru': 'MS3DGRU',
        'gru': 'GRU',
        'stackedgru': 'StackedGRU',
        'ms3dstackedgru': 'MS3DStackedGRU'
    }
    
    # ë°ì´í„°ì…‹ ë§¤í•‘
    dataset_mapping = {
        'unified': 'Unified',
        'yubeen': 'Yubeen',
        'jaeyeon': 'Jaeyeon'
    }
    
    results = {}
    
    # multi_seed_*.log íŒŒì¼ ì°¾ê¸°
    for log_file in log_dir.glob('multi_seed_*.log'):
        try:
            filename = log_file.stem.replace('multi_seed_', '')
            
            # ëª¨ë¸ëª… ì¶”ì¶œ
            model_key = None
            for key in model_mapping.keys():
                if filename.startswith(key):
                    model_key = key
                    break
            
            if model_key is None:
                continue
            
            model_name = model_mapping[model_key]
            
            # ë°ì´í„°ì…‹ ì¶”ì¶œ
            dataset_key = None
            for key in dataset_mapping.keys():
                if key in filename:
                    dataset_key = key
                    break
            
            if dataset_key is None:
                continue
            
            dataset_name = dataset_mapping[dataset_key]
            
            # ì‹œë“œì™€ ì‹¤í–‰ ë²ˆí˜¸ ì¶”ì¶œ
            seed_match = re.search(r'seed(\d+)_run(\d+)', filename)
            if not seed_match:
                continue
            
            # ë¡œê·¸ íŒŒì¼ì—ì„œ test/accuracy ì¶”ì¶œ
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()
                acc_match = None
                # ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„
                acc_match = re.search(r'test/accuracy\s*[â”‚|\|]\s*([0-9.]+)', content)
                if not acc_match:
                    lines = content.split('\n')
                    for line in reversed(lines):
                        if 'test/accuracy' in line.lower():
                            acc_match = re.search(r'([0-9]+\.[0-9]+)', line)
                            if acc_match:
                                break
                
                if acc_match:
                    acc_value = float(acc_match.group(1)) * 100
                    
                    # ê²°ê³¼ ì €ì¥
                    key = f"{model_name}_{dataset_name}"
                    if key not in results:
                        results[key] = {
                            'model': model_name,
                            'dataset': dataset_name,
                            'accuracies': []
                        }
                    results[key]['accuracies'].append(acc_value)
        except Exception as e:
            continue
    
    # í†µê³„ ê³„ì‚°
    table_data = []
    summary_stats = {}
    
    for key, data in results.items():
        if len(data['accuracies']) >= 3:
            accuracies = data['accuracies']
            mean_acc = np.mean(accuracies)
            std_acc = np.std(accuracies)
            min_acc = np.min(accuracies)
            max_acc = np.max(accuracies)
            
            table_data.append({
                'Model': data['model'],
                'Dataset': data['dataset'],
                'Mean (%)': round(mean_acc, 2),
                'Std (%)': round(std_acc, 2),
                'Min (%)': round(min_acc, 2),
                'Max (%)': round(max_acc, 2),
                'Runs': len(accuracies)
            })
            
            # ëª¨ë¸ë³„ ìš”ì•½ í†µê³„
            if data['model'] not in summary_stats:
                summary_stats[data['model']] = {
                    'accuracies': [],
                    'datasets': []
                }
            summary_stats[data['model']]['accuracies'].extend(accuracies)
            summary_stats[data['model']]['datasets'].append(data['dataset'])
    
    # ëª¨ë¸ëª…ìœ¼ë¡œ ì •ë ¬
    model_order = ['MS3DGRU', 'GRU', 'StackedGRU', 'MS3DStackedGRU']
    dataset_order = ['Unified', 'Yubeen', 'Jaeyeon']
    
    table_data.sort(key=lambda x: (
        model_order.index(x['Model']) if x['Model'] in model_order else 999,
        dataset_order.index(x['Dataset']) if x['Dataset'] in dataset_order else 999
    ))
    
    return table_data, summary_stats

def create_comparison_table(output_dir='visualizations/statistical_validation'):
    """ë¹„êµí‘œ ìƒì„±"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ“Š Extracting dataset statistics from log files...")
    table_data, summary_stats = extract_dataset_statistics()
    
    if not table_data:
        print("âš ï¸  No data found. Please check log files.")
        return
    
    # 1. í…ìŠ¤íŠ¸ í˜•ì‹ ë¹„êµí‘œ
    txt_file = output_path / 'statistical_comparison_table.txt'
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("Statistical Validation Results Summary (5 Runs per Model-Dataset)\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"{'Model':<20} {'Dataset':<15} {'Mean (%)':<12} {'Std (%)':<12} {'Min (%)':<12} {'Max (%)':<12} {'Runs':<8}\n")
        f.write("-" * 80 + "\n")
        
        for row in table_data:
            f.write(f"{row['Model']:<20} {row['Dataset']:<15} {row['Mean (%)']:<12.2f} "
                   f"{row['Std (%)']:<12.2f} {row['Min (%)']:<12.2f} {row['Max (%)']:<12.2f} "
                   f"{row['Runs']:<8}\n")
        
        f.write("\n" + "=" * 80 + "\n")
        f.write("Model Summary (across all datasets)\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"{'Model':<20} {'Mean (%)':<12} {'Std (%)':<12} {'Min (%)':<12} {'Max (%)':<12} {'Total Runs':<12}\n")
        f.write("-" * 80 + "\n")
        
        for model in ['MS3DGRU', 'GRU', 'StackedGRU', 'MS3DStackedGRU']:
            if model in summary_stats:
                accuracies = summary_stats[model]['accuracies']
                f.write(f"{model:<20} {np.mean(accuracies):<12.2f} {np.std(accuracies):<12.2f} "
                       f"{np.min(accuracies):<12.2f} {np.max(accuracies):<12.2f} {len(accuracies):<12}\n")
    
    print(f"âœ… Text table saved: {txt_file}")
    
    # 2. JSON í˜•ì‹ ë°ì´í„°
    json_file = output_path / 'statistical_comparison_table.json'
    json_data = {
        'dataset_results': table_data,
        'model_summary': {}
    }
    
    for model in ['MS3DGRU', 'GRU', 'StackedGRU', 'MS3DStackedGRU']:
        if model in summary_stats:
            accuracies = summary_stats[model]['accuracies']
            json_data['model_summary'][model] = {
                'Mean (%)': round(np.mean(accuracies), 2),
                'Std (%)': round(np.std(accuracies), 2),
                'Min (%)': round(np.min(accuracies), 2),
                'Max (%)': round(np.max(accuracies), 2),
                'Total Runs': len(accuracies),
                'Datasets': list(set(summary_stats[model]['datasets']))
            }
    
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… JSON data saved: {json_file}")
    
    # 3. ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 80)
    print("Dataset-wise Results")
    print("=" * 80)
    for row in table_data:
        print(f"{row['Model']:<20} {row['Dataset']:<15} Mean: {row['Mean (%)']:.2f}% "
              f"(Std: {row['Std (%)']:.2f}%, Range: {row['Min (%)']:.2f}%-{row['Max (%)']:.2f}%)")
    
    print("\n" + "=" * 80)
    print("Model Summary (across all datasets)")
    print("=" * 80)
    for model in ['MS3DGRU', 'GRU', 'StackedGRU', 'MS3DStackedGRU']:
        if model in summary_stats:
            accuracies = summary_stats[model]['accuracies']
            print(f"{model:<20} Mean: {np.mean(accuracies):.2f}% "
                  f"(Std: {np.std(accuracies):.2f}%, Range: {np.min(accuracies):.2f}%-{np.max(accuracies):.2f}%, "
                  f"Runs: {len(accuracies)})")

if __name__ == '__main__':
    print("=" * 80)
    print("Statistical Validation Comparison Table Generator")
    print("=" * 80)
    print()
    create_comparison_table()
    print("\nâœ… Complete!")







