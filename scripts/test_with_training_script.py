"""
í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‹¤ì œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
PyTorch Lightningì˜ test_stepì„ ì§ì ‘ ì‹¤í–‰
"""

import sys
sys.path.append('.')

import torch
import numpy as np
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import pytorch_lightning as L
from pytorch_lightning.callbacks import ModelCheckpoint

from src.misc.DynamicDataModule import DynamicDataModule
from src.models.MultiScale3DGRUModels import MS3DGRU

def test_model_with_training_setup():
    """í›ˆë ¨ ì‹œì™€ ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸"""
    
    print('=' * 80)
    print('ğŸ§ª í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ ë°©ì‹ìœ¼ë¡œ ì‹¤ì œ í…ŒìŠ¤íŠ¸')
    print('=' * 80)
    print()
    
    # 1. ë°ì´í„° ëª¨ë“ˆ ì„¤ì •
    print('ğŸ“Œ 1ë‹¨ê³„: ë°ì´í„° ëª¨ë“ˆ ì„¤ì •')
    print('-' * 80)
    datamodule = DynamicDataModule(
        data_dir='/home/billy/25-1kp/SignGlove_HW/datasets/unified',
        batch_size=32,
        test_size=0.2,
        val_size=0.2,
        seed=42
    )
    datamodule.setup('test')
    print(f'âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ')
    print(f'   í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(datamodule.test_dataset)}')
    print()
    
    # 2. ëª¨ë¸ ë¡œë“œ (ì²´í¬í¬ì¸íŠ¸ì—ì„œ)
    print('ğŸ“Œ 2ë‹¨ê³„: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ')
    print('-' * 80)
    
    checkpoint_path = 'best_model/ms3dgru_best.ckpt'
    
    # ëª¨ë¸ ìƒì„±
    model = MS3DGRU(
        learning_rate=0.001,
        input_size=8,
        hidden_size=64,
        classes=24,
        cnn_filters=32,
        dropout=0.1
    )
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    if 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
        # 'model.' ì ‘ë‘ì‚¬ ì œê±°
        cleaned_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('model.'):
                cleaned_state_dict[key[6:]] = value
            else:
                cleaned_state_dict[key] = value
        model.load_state_dict(cleaned_state_dict, strict=False)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    print(f'âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ')
    print(f'   íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')
    print()
    
    # 3. Trainerë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print('ğŸ“Œ 3ë‹¨ê³„: PyTorch Lightningìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰')
    print('-' * 80)
    
    trainer = L.Trainer(
        accelerator='cpu',
        devices=1,
        logger=False,
        enable_progress_bar=True,
    )
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_results = trainer.test(model, datamodule=datamodule, verbose=False)
    
    print(f'âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!')
    print()
    
    # 4. ìƒì„¸ ê²°ê³¼ ìˆ˜ì§‘
    print('ğŸ“Œ 4ë‹¨ê³„: ìƒì„¸ ê²°ê³¼ ìˆ˜ì§‘')
    print('-' * 80)
    
    all_predictions = []
    all_labels = []
    all_probs = []
    
    model.eval()
    model.to('cpu')
    
    test_loader = datamodule.test_dataloader()
    
    with torch.no_grad():
        for batch in test_loader:
            x = batch['measurement']
            y = batch['label']
            x_padding = batch.get('measurement_padding', None)
            
            # Forward pass (í›ˆë ¨ ì‹œì™€ ë™ì¼)
            logits, loss = model(x, x_padding, y)
            
            probs = torch.softmax(logits, dim=-1)
            preds = torch.argmax(logits, dim=-1)
            
            all_predictions.extend(preds.cpu().numpy())
            all_labels.extend(y.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)
    
    # 5. ì„±ëŠ¥ ê³„ì‚°
    accuracy = accuracy_score(all_labels, all_predictions)
    f1_macro = f1_score(all_labels, all_predictions, average='macro')
    f1_weighted = f1_score(all_labels, all_predictions, average='weighted')
    
    CLASS_NAMES = [
        'ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…',
        'ã…', 'ã…‘', 'ã…“', 'ã…•', 'ã…—', 'ã…›', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£'
    ]
    
    cm = confusion_matrix(all_labels, all_predictions, labels=range(24))
    class_accuracies = cm.diagonal() / cm.sum(axis=1)
    
    print('=' * 80)
    print('ğŸ“Š ì‹¤ì „ ì„±ëŠ¥ ê²°ê³¼')
    print('=' * 80)
    print(f'í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(all_labels):,}')
    print(f'ì •í™•ë„ (Accuracy): {accuracy * 100:.2f}%')
    print(f'F1-Score (Macro): {f1_macro:.4f}')
    print(f'F1-Score (Weighted): {f1_weighted:.4f}')
    print()
    
    wrong_count = (all_predictions != all_labels).sum()
    print(f'ì •í™• ì˜ˆì¸¡: {len(all_labels) - wrong_count:,}ê°œ ({(len(all_labels) - wrong_count) / len(all_labels) * 100:.2f}%)')
    print(f'ì˜¤ë¶„ë¥˜: {wrong_count:,}ê°œ ({wrong_count / len(all_labels) * 100:.2f}%)')
    print()
    
    print('ğŸ“Š í´ë˜ìŠ¤ë³„ ì •í™•ë„:')
    print('-' * 80)
    for i, (class_name, acc) in enumerate(zip(CLASS_NAMES, class_accuracies)):
        status = 'âœ…' if acc >= 0.98 else 'âš ï¸' if acc >= 0.95 else 'âŒ'
        correct = cm[i, i]
        total = cm.sum(axis=1)[i]
        print(f'{status} {class_name}: {acc * 100:.2f}% ({correct}/{total})')
    
    print()
    print('=' * 80)
    print('âœ… ì‹¤ì „ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!')
    print('=' * 80)
    
    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'confusion_matrix': cm,
        'class_accuracies': class_accuracies,
        'predictions': all_predictions,
        'labels': all_labels
    }


if __name__ == '__main__':
    try:
        results = test_model_with_training_setup()
    except Exception as e:
        print(f'âŒ ì˜¤ë¥˜ ë°œìƒ: {e}')
        import traceback
        traceback.print_exc()

