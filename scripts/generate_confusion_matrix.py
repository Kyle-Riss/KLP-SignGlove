"""
ì˜¬ë°”ë¥¸ MS3DGRU ì²´í¬í¬ì¸íŠ¸ë¡œ Confusion Matrix ìƒì„±
"""

import sys
sys.path.append('.')

import torch
import numpy as np
from pathlib import Path
from sklearn.metrics import (
    accuracy_score, f1_score, confusion_matrix,
    classification_report
)
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from tqdm import tqdm

from src.misc.DynamicDataModule import DynamicDataModule
from src.models.MultiScale3DGRUModels import MS3DGRU

# í•œê¸€ í°íŠ¸ ì„¤ì •
def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    # ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ ì°¾ê¸°
    korean_fonts = [
        'NanumGothic',
        'NanumBarunGothic',
        'Malgun Gothic',
        'AppleGothic',
        'Noto Sans CJK KR',
        'DejaVu Sans'  # fallback
    ]
    
    # í°íŠ¸ ê²½ë¡œ ì§ì ‘ ì§€ì • (Linux)
    font_paths = [
        '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',
        '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf',
        '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
    ]
    
    # í°íŠ¸ ì°¾ê¸°
    font_found = False
    for font_name in korean_fonts:
        try:
            plt.rcParams['font.family'] = font_name
            # í…ŒìŠ¤íŠ¸
            fig, ax = plt.subplots(figsize=(1, 1))
            ax.text(0.5, 0.5, 'í•œê¸€', fontsize=12)
            plt.close(fig)
            font_found = True
            print(f'âœ… í•œê¸€ í°íŠ¸ ì„¤ì •: {font_name}')
            break
        except:
            continue
    
    # í°íŠ¸ ê²½ë¡œë¡œ ì§ì ‘ ì„¤ì •
    if not font_found:
        for font_path in font_paths:
            if Path(font_path).exists():
                try:
                    font_prop = fm.FontProperties(fname=font_path)
                    plt.rcParams['font.family'] = font_prop.get_name()
                    font_found = True
                    print(f'âœ… í•œê¸€ í°íŠ¸ ì„¤ì •: {font_path}')
                    break
                except:
                    continue
    
    if not font_found:
        # ë§ˆì§€ë§‰ fallback: í°íŠ¸ ì—†ì´ë„ ì‘ë™í•˜ë„ë¡
        plt.rcParams['font.family'] = 'DejaVu Sans'
        print('âš ï¸  í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ ì‚¬ìš© (í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŒ)')
    
    # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams['axes.unicode_minus'] = False

# í°íŠ¸ ì„¤ì • ì‹¤í–‰
setup_korean_font()

# í•œê¸€ í´ë˜ìŠ¤ëª…
CLASS_NAMES = [
    'ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…',
    'ã…', 'ã…‘', 'ã…“', 'ã…•', 'ã…—', 'ã…›', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£'
]

def generate_confusion_matrix():
    """Confusion Matrix ìƒì„±"""
    
    print('=' * 80)
    print('ğŸ“Š MS3DGRU Confusion Matrix ìƒì„±')
    print('=' * 80)
    print()
    
    # 1. ë°ì´í„° ëª¨ë“ˆ ì„¤ì •
    print('ğŸ“Œ 1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ')
    print('-' * 80)
    datamodule = DynamicDataModule(
        data_dir='/home/billy/25-1kp/SignGlove_HW/datasets/unified',
        batch_size=32,
        test_size=0.2,
        val_size=0.2,
        seed=42
    )
    datamodule.setup('test')
    test_loader = datamodule.test_dataloader()
    print(f'âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ')
    print(f'   í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(datamodule.test_dataset):,}')
    print()
    
    # 2. ëª¨ë¸ ë¡œë“œ
    print('ğŸ“Œ 2ë‹¨ê³„: ëª¨ë¸ ë¡œë“œ')
    print('-' * 80)
    checkpoint_path = 'inference/best_models/ms3dgru_best.ckpt'
    
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì¶œ (model_config ë˜ëŠ” hyper_parameters ì‚¬ìš©)
    if 'model_config' in checkpoint:
        hyper_params = checkpoint['model_config']
    elif 'hyper_parameters' in checkpoint:
        hyper_params = checkpoint['hyper_parameters']
    else:
        hyper_params = {}
    
    model = MS3DGRU(
        learning_rate=hyper_params.get('learning_rate', 0.001),
        input_size=hyper_params.get('input_size', 8),
        hidden_size=hyper_params.get('hidden_size', 64),
        classes=hyper_params.get('classes', 24),
        cnn_filters=hyper_params.get('cnn_filters', 32),
        dropout=hyper_params.get('dropout', 0.1)
    )
    
    # State dict ë¡œë“œ (model_config í˜•íƒœëŠ” ì§ì ‘ ë¡œë“œ ê°€ëŠ¥)
    if 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
        # model_config í˜•íƒœëŠ” ì´ë¯¸ cleaní•œ ìƒíƒœ
        # model. ì ‘ë‘ì‚¬ê°€ ìˆì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ í™•ì¸
        cleaned_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('model.'):
                cleaned_state_dict[key[6:]] = value
            else:
                cleaned_state_dict[key] = value
        
        missing_keys, unexpected_keys = model.load_state_dict(cleaned_state_dict, strict=False)
        if missing_keys:
            print(f'âš ï¸  ëˆ„ë½ëœ í‚¤: {len(missing_keys)}ê°œ')
            if missing_keys:
                print(f'   ì˜ˆì‹œ: {list(missing_keys)[:3]}')
        if unexpected_keys:
            print(f'âš ï¸  ì˜ˆìƒì¹˜ ëª»í•œ í‚¤: {len(unexpected_keys)}ê°œ')
    
    model.eval()
    print(f'âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ')
    print(f'   ëª¨ë¸ íƒ€ì…: {checkpoint.get("model_type", "N/A")}')
    if 'model_info' in checkpoint:
        mi = checkpoint['model_info']
        if 'performance' in mi:
            perf = mi['performance']
            print(f'   ì˜ˆìƒ ì„±ëŠ¥: {perf.get("test_accuracy", 0)*100:.2f}%')
    print()
    
    # 3. ì˜ˆì¸¡ ìˆ˜í–‰
    print('ğŸ“Œ 3ë‹¨ê³„: ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...')
    print('-' * 80)
    
    all_predictions = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc='ì˜ˆì¸¡ ì§„í–‰'):
            x = batch['measurement']
            y = batch['label']
            x_padding = batch.get('measurement_padding', None)
            
            # Forward pass
            logits, loss = model(x, x_padding, y)
            
            probs = torch.softmax(logits, dim=-1)
            preds = torch.argmax(logits, dim=-1)
            
            all_predictions.extend(preds.cpu().numpy())
            all_labels.extend(y.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)
    
    print(f'âœ… ì˜ˆì¸¡ ì™„ë£Œ! ì´ {len(all_labels):,}ê°œ ìƒ˜í”Œ')
    print()
    
    # 4. ì„±ëŠ¥ ê³„ì‚°
    print('ğŸ“Œ 4ë‹¨ê³„: ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°')
    print('-' * 80)
    
    accuracy = accuracy_score(all_labels, all_predictions)
    f1_macro = f1_score(all_labels, all_predictions, average='macro')
    f1_weighted = f1_score(all_labels, all_predictions, average='weighted')
    f1_per_class = f1_score(all_labels, all_predictions, average=None)
    
    print(f'âœ… ì„±ëŠ¥ ì§€í‘œ:')
    print(f'   ì •í™•ë„ (Accuracy): {accuracy * 100:.2f}%')
    print(f'   F1-Score (Macro): {f1_macro:.4f}')
    print(f'   F1-Score (Weighted): {f1_weighted:.4f}')
    print()
    
    # 5. Confusion Matrix ìƒì„±
    print('ğŸ“Œ 5ë‹¨ê³„: Confusion Matrix ìƒì„±')
    print('-' * 80)
    
    cm = confusion_matrix(all_labels, all_predictions, labels=range(24))
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    class_accuracies = cm.diagonal() / cm.sum(axis=1)
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(24, 10))
    
    # Count Confusion Matrix
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,
                ax=axes[0], cbar_kws={'label': 'Count'}, annot_kws={'size': 8})
    axes[0].set_title('Confusion Matrix (Count)', fontsize=16, fontweight='bold', pad=20)
    axes[0].set_xlabel('Predicted Class', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('True Class', fontsize=12, fontweight='bold')
    
    # Normalized Confusion Matrix
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,
                ax=axes[1], cbar_kws={'label': 'Normalized'}, annot_kws={'size': 8})
    axes[1].set_title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold', pad=20)
    axes[1].set_xlabel('Predicted Class', fontsize=12, fontweight='bold')
    axes[1].set_ylabel('True Class', fontsize=12, fontweight='bold')
    
    plt.tight_layout()
    
    output_dir = Path('inference/performance_visualizations')
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / 'real_test_confusion_matrix_ms3dgru_final.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f'âœ… ì €ì¥: {output_file}')
    print()
    
    # 6. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì‹œê°í™”
    print('ğŸ“Œ 6ë‹¨ê³„: í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì‹œê°í™”')
    print('-' * 80)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
    
    # í´ë˜ìŠ¤ë³„ ì •í™•ë„
    sorted_indices = np.argsort(class_accuracies)
    colors = ['#e74c3c' if acc < 0.95 else '#f39c12' if acc < 0.98 else '#2ecc71' 
              for acc in class_accuracies[sorted_indices]]
    
    bars = ax1.barh(range(len(CLASS_NAMES)), class_accuracies[sorted_indices] * 100, 
                    color=colors, alpha=0.8, edgecolor='black', linewidth=1)
    ax1.set_yticks(range(len(CLASS_NAMES)))
    ax1.set_yticklabels([CLASS_NAMES[i] for i in sorted_indices], fontsize=11)
    ax1.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')
    ax1.set_title('Class-wise Accuracy', fontsize=14, fontweight='bold', pad=20)
    ax1.axvline(x=98, color='green', linestyle='--', alpha=0.5, label='98% threshold')
    ax1.axvline(x=95, color='orange', linestyle='--', alpha=0.5, label='95% threshold')
    ax1.set_xlim([90, 100])
    ax1.legend(fontsize=10)
    ax1.grid(axis='x', alpha=0.3)
    
    # ê°’ í‘œì‹œ
    for i, (idx, acc) in enumerate(zip(sorted_indices, class_accuracies[sorted_indices])):
        ax1.text(acc * 100 + 0.3, i, f'{acc*100:.1f}%', va='center', fontsize=9)
    
    # F1-Score per class
    bars2 = ax2.barh(range(len(CLASS_NAMES)), f1_per_class[sorted_indices] * 100, 
                     color=colors, alpha=0.8, edgecolor='black', linewidth=1)
    ax2.set_yticks(range(len(CLASS_NAMES)))
    ax2.set_yticklabels([CLASS_NAMES[i] for i in sorted_indices], fontsize=11)
    ax2.set_xlabel('F1-Score (%)', fontsize=12, fontweight='bold')
    ax2.set_title('Class-wise F1-Score', fontsize=14, fontweight='bold', pad=20)
    ax2.axvline(x=98, color='green', linestyle='--', alpha=0.5, label='98% threshold')
    ax2.axvline(x=95, color='orange', linestyle='--', alpha=0.5, label='95% threshold')
    ax2.set_xlim([90, 100])
    ax2.legend(fontsize=10)
    ax2.grid(axis='x', alpha=0.3)
    
    # ê°’ í‘œì‹œ
    for i, (idx, f1) in enumerate(zip(sorted_indices, f1_per_class[sorted_indices])):
        ax2.text(f1 * 100 + 0.3, i, f'{f1*100:.1f}%', va='center', fontsize=9)
    
    plt.tight_layout()
    
    accuracy_file = output_dir / 'real_test_class_accuracy_ms3dgru_final.png'
    plt.savefig(accuracy_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f'âœ… ì €ì¥: {accuracy_file}')
    print()
    
    # 7. ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±
    print('ğŸ“Œ 7ë‹¨ê³„: ìƒì„¸ ë¦¬í¬íŠ¸ ìƒì„±')
    print('-' * 80)
    
    report = classification_report(
        all_labels, all_predictions,
        target_names=CLASS_NAMES,
        output_dict=True,
        zero_division=0
    )
    
    report_file = output_dir / 'real_test_report_ms3dgru_final.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f'MS3DGRU ì‹¤ì „ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸\n')
        f.write(f'=' * 60 + '\n\n')
        f.write(f'ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}\n')
        f.write(f'í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(all_labels):,}\n')
        f.write(f'ì •í™•ë„: {accuracy * 100:.2f}%\n')
        f.write(f'F1-Score (Macro): {f1_macro:.4f}\n')
        f.write(f'F1-Score (Weighted): {f1_weighted:.4f}\n')
        f.write(f'\ní´ë˜ìŠ¤ë³„ ìƒì„¸ ì„±ëŠ¥:\n')
        f.write('-' * 60 + '\n')
        
        for i, class_name in enumerate(CLASS_NAMES):
            class_report = report.get(class_name, {})
            precision = class_report.get('precision', 0)
            recall = class_report.get('recall', 0)
            f1 = class_report.get('f1-score', 0)
            support = class_report.get('support', 0)
            
            correct = cm[i, i]
            total = cm.sum(axis=1)[i]
            acc = class_accuracies[i]
            
            f.write(f'{class_name:3s}: Acc={acc*100:6.2f}%, '
                   f'Prec={precision*100:6.2f}%, Rec={recall*100:6.2f}%, '
                   f'F1={f1*100:6.2f}%, Correct={correct:2d}/{total:2d}\n')
        
        f.write('\n' + '=' * 60 + '\n')
        f.write(classification_report(all_labels, all_predictions, target_names=CLASS_NAMES))
    
    print(f'âœ… ì €ì¥: {report_file}')
    print()
    
    # 8. ìµœì¢… ìš”ì•½
    print('=' * 80)
    print('ğŸ“Š ìµœì¢… ì„±ëŠ¥ ìš”ì•½')
    print('=' * 80)
    print(f'ëª¨ë¸: MS3DGRU')
    print(f'ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}')
    print(f'í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(all_labels):,}')
    print(f'ì •í™•ë„: {accuracy * 100:.2f}%')
    print(f'F1-Score (Macro): {f1_macro:.4f}')
    print(f'F1-Score (Weighted): {f1_weighted:.4f}')
    print()
    
    wrong_count = (all_predictions != all_labels).sum()
    print(f'âœ… ì •í™• ì˜ˆì¸¡: {len(all_labels) - wrong_count:,}ê°œ ({(len(all_labels) - wrong_count) / len(all_labels) * 100:.2f}%)')
    print(f'âŒ ì˜¤ë¶„ë¥˜: {wrong_count:,}ê°œ ({wrong_count / len(all_labels) * 100:.2f}%)')
    print()
    
    print('ğŸ“Š í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:')
    print('-' * 80)
    for i, (class_name, acc) in enumerate(zip(CLASS_NAMES, class_accuracies)):
        status = 'âœ…' if acc >= 0.98 else 'âš ï¸' if acc >= 0.95 else 'âŒ'
        correct = cm[i, i]
        total = cm.sum(axis=1)[i]
        print(f'{status} {class_name}: {acc * 100:.2f}% ({correct}/{total})')
    
    print()
    print('ìƒì„±ëœ íŒŒì¼:')
    print(f'  â€¢ {output_file}')
    print(f'  â€¢ {accuracy_file}')
    print(f'  â€¢ {report_file}')
    print()
    print('=' * 80)


if __name__ == '__main__':
    try:
        generate_confusion_matrix()
    except Exception as e:
        print(f'âŒ ì˜¤ë¥˜ ë°œìƒ: {e}')
        import traceback
        traceback.print_exc()

