"""
ì¶”ë¡  ì—”ì§„ì„ ì‚¬ìš©í•œ ì‹¤ì œ í…ŒìŠ¤íŠ¸
"""

import sys
sys.path.append('.')

import torch
import numpy as np
from pathlib import Path
from sklearn.metrics import (
    accuracy_score, f1_score, confusion_matrix,
    classification_report
)
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

from src.misc.DynamicDataModule import DynamicDataModule
from inference import SignGloveInference

# í•œê¸€ í´ë˜ìŠ¤ëª…
CLASS_NAMES = [
    'ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…',
    'ã…', 'ã…‘', 'ã…“', 'ã…•', 'ã…—', 'ã…›', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£'
]

def test_with_inference_engine():
    """ì¶”ë¡  ì—”ì§„ì„ ì‚¬ìš©í•œ ì‹¤ì œ í…ŒìŠ¤íŠ¸"""
    
    print('=' * 80)
    print('ğŸ§ª ì¶”ë¡  ì—”ì§„ì„ ì‚¬ìš©í•œ MS3DGRU ì„±ëŠ¥ í…ŒìŠ¤íŠ¸')
    print('=' * 80)
    print()
    
    # 1. ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
    print('ğŸ“Œ 1ë‹¨ê³„: ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”')
    print('-' * 80)
    try:
        engine = SignGloveInference(
            model_path='inference/best_models/ms3dgru_best.ckpt',
            model_type='MS3DGRU',
            device='cpu'
        )
        print(f'âœ… ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ')
        print(f'   ëª¨ë¸ íŒŒë¼ë¯¸í„°: {engine.model.count_parameters():,}')
    except Exception as e:
        print(f'âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}')
        import traceback
        traceback.print_exc()
        return
    
    print()
    
    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ
    print('ğŸ“Œ 2ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ')
    print('-' * 80)
    datamodule = DynamicDataModule(
        data_dir='/home/billy/25-1kp/SignGlove_HW/datasets/unified',
        batch_size=32,
        test_size=0.2,
        val_size=0.2,
        seed=42
    )
    datamodule.setup('test')
    test_loader = datamodule.test_dataloader()
    print(f'âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ')
    print(f'   í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(datamodule.test_dataset):,}')
    print()
    
    # 3. ì˜ˆì¸¡ ìˆ˜í–‰
    print('ğŸ“Œ 3ë‹¨ê³„: ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...')
    print('-' * 80)
    
    all_predictions = []
    all_labels = []
    
    engine.model.eval()
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc='ì˜ˆì¸¡ ì§„í–‰'):
            x = batch['measurement']
            y = batch['label']
            x_padding = batch.get('measurement_padding', None)
            
            # ë°ì´í„°ë¥¼ numpyë¡œ ë³€í™˜ (ì¶”ë¡  ì—”ì§„ ì…ë ¥ í˜•ì‹)
            batch_size = x.shape[0]
            for i in range(batch_size):
                sample = x[i].cpu().numpy()  # (time, channels)
                label = y[i].item()
                
                # ì¶”ë¡  ì—”ì§„ìœ¼ë¡œ ì˜ˆì¸¡
                result = engine.predict_single(sample, top_k=1, return_all_info=False)
                pred_class = result['predicted_class']
                pred_idx = CLASS_NAMES.index(pred_class)
                
                all_predictions.append(pred_idx)
                all_labels.append(label)
    
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    
    print(f'âœ… ì˜ˆì¸¡ ì™„ë£Œ! ì´ {len(all_labels):,}ê°œ ìƒ˜í”Œ')
    print()
    
    # 4. ì„±ëŠ¥ ê³„ì‚°
    print('ğŸ“Œ 4ë‹¨ê³„: ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°')
    print('-' * 80)
    
    accuracy = accuracy_score(all_labels, all_predictions)
    f1_macro = f1_score(all_labels, all_predictions, average='macro')
    f1_weighted = f1_score(all_labels, all_predictions, average='weighted')
    f1_per_class = f1_score(all_labels, all_predictions, average=None)
    
    print(f'âœ… ì„±ëŠ¥ ì§€í‘œ:')
    print(f'   ì •í™•ë„ (Accuracy): {accuracy * 100:.2f}%')
    print(f'   F1-Score (Macro): {f1_macro:.4f}')
    print(f'   F1-Score (Weighted): {f1_weighted:.4f}')
    print()
    
    # 5. Confusion Matrix
    print('ğŸ“Œ 5ë‹¨ê³„: Confusion Matrix ìƒì„±')
    print('-' * 80)
    
    cm = confusion_matrix(all_labels, all_predictions, labels=range(24))
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    class_accuracies = cm.diagonal() / cm.sum(axis=1)
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(24, 10))
    
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,
                ax=axes[0], cbar_kws={'label': 'Count'}, annot_kws={'size': 8})
    axes[0].set_title('Confusion Matrix (Count)', fontsize=16, fontweight='bold', pad=20)
    axes[0].set_xlabel('Predicted Class', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('True Class', fontsize=12, fontweight='bold')
    
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,
                ax=axes[1], cbar_kws={'label': 'Normalized'}, annot_kws={'size': 8})
    axes[1].set_title('Confusion Matrix (Normalized)', fontsize=16, fontweight='bold', pad=20)
    axes[1].set_xlabel('Predicted Class', fontsize=12, fontweight='bold')
    axes[1].set_ylabel('True Class', fontsize=12, fontweight='bold')
    
    plt.tight_layout()
    
    output_dir = Path('inference/performance_visualizations')
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / 'inference_engine_confusion_matrix.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f'âœ… ì €ì¥: {output_file}')
    print()
    
    # 6. í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
    print('ğŸ“Œ 6ë‹¨ê³„: í´ë˜ìŠ¤ë³„ ì„±ëŠ¥')
    print('-' * 80)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
    
    sorted_indices = np.argsort(class_accuracies)
    colors = ['#e74c3c' if acc < 0.95 else '#f39c12' if acc < 0.98 else '#2ecc71' 
              for acc in class_accuracies[sorted_indices]]
    
    bars = ax1.barh(range(len(CLASS_NAMES)), class_accuracies[sorted_indices] * 100, 
                    color=colors, alpha=0.8, edgecolor='black', linewidth=1)
    ax1.set_yticks(range(len(CLASS_NAMES)))
    ax1.set_yticklabels([CLASS_NAMES[i] for i in sorted_indices], fontsize=11)
    ax1.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')
    ax1.set_title('Class-wise Accuracy', fontsize=14, fontweight='bold', pad=20)
    ax1.axvline(x=98, color='green', linestyle='--', alpha=0.5, label='98% threshold')
    ax1.axvline(x=95, color='orange', linestyle='--', alpha=0.5, label='95% threshold')
    ax1.set_xlim([90, 100])
    ax1.legend(fontsize=10)
    ax1.grid(axis='x', alpha=0.3)
    
    for i, (idx, acc) in enumerate(zip(sorted_indices, class_accuracies[sorted_indices])):
        ax1.text(acc * 100 + 0.3, i, f'{acc*100:.1f}%', va='center', fontsize=9)
    
    bars2 = ax2.barh(range(len(CLASS_NAMES)), f1_per_class[sorted_indices] * 100, 
                     color=colors, alpha=0.8, edgecolor='black', linewidth=1)
    ax2.set_yticks(range(len(CLASS_NAMES)))
    ax2.set_yticklabels([CLASS_NAMES[i] for i in sorted_indices], fontsize=11)
    ax2.set_xlabel('F1-Score (%)', fontsize=12, fontweight='bold')
    ax2.set_title('Class-wise F1-Score', fontsize=14, fontweight='bold', pad=20)
    ax2.axvline(x=98, color='green', linestyle='--', alpha=0.5, label='98% threshold')
    ax2.axvline(x=95, color='orange', linestyle='--', alpha=0.5, label='95% threshold')
    ax2.set_xlim([90, 100])
    ax2.legend(fontsize=10)
    ax2.grid(axis='x', alpha=0.3)
    
    for i, (idx, f1) in enumerate(zip(sorted_indices, f1_per_class[sorted_indices])):
        ax2.text(f1 * 100 + 0.3, i, f'{f1*100:.1f}%', va='center', fontsize=9)
    
    plt.tight_layout()
    
    accuracy_file = output_dir / 'inference_engine_class_accuracy.png'
    plt.savefig(accuracy_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f'âœ… ì €ì¥: {accuracy_file}')
    print()
    
    # 7. ìµœì¢… ìš”ì•½
    print('=' * 80)
    print('ğŸ“Š ìµœì¢… ì„±ëŠ¥ ìš”ì•½')
    print('=' * 80)
    print(f'ëª¨ë¸: MS3DGRU (ì¶”ë¡  ì—”ì§„ ì‚¬ìš©)')
    print(f'ì²´í¬í¬ì¸íŠ¸: inference/best_models/ms3dgru_best.ckpt')
    print(f'í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(all_labels):,}')
    print(f'ì •í™•ë„: {accuracy * 100:.2f}%')
    print(f'F1-Score (Macro): {f1_macro:.4f}')
    print(f'F1-Score (Weighted): {f1_weighted:.4f}')
    print()
    
    wrong_count = (all_predictions != all_labels).sum()
    print(f'âœ… ì •í™• ì˜ˆì¸¡: {len(all_labels) - wrong_count:,}ê°œ ({(len(all_labels) - wrong_count) / len(all_labels) * 100:.2f}%)')
    print(f'âŒ ì˜¤ë¶„ë¥˜: {wrong_count:,}ê°œ ({wrong_count / len(all_labels) * 100:.2f}%)')
    print()
    
    print('ğŸ“Š í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:')
    print('-' * 80)
    for i, (class_name, acc) in enumerate(zip(CLASS_NAMES, class_accuracies)):
        status = 'âœ…' if acc >= 0.98 else 'âš ï¸' if acc >= 0.95 else 'âŒ'
        correct = cm[i, i]
        total = cm.sum(axis=1)[i]
        print(f'{status} {class_name}: {acc * 100:.2f}% ({correct}/{total})')
    
    print()
    print('ìƒì„±ëœ íŒŒì¼:')
    print(f'  â€¢ {output_file}')
    print(f'  â€¢ {accuracy_file}')
    print()
    print('=' * 80)


if __name__ == '__main__':
    try:
        test_with_inference_engine()
    except Exception as e:
        print(f'âŒ ì˜¤ë¥˜ ë°œìƒ: {e}')
        import traceback
        traceback.print_exc()

