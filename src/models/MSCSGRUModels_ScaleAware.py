"""
MS-CSGRU Scale-Aware Models
Multi-Scale CNN + Scale-Aware GRU í†µí•© ëª¨ë¸

íŠ¹ì§•:
1. ê° CNN íƒ€ì›Œì˜ íŠ¹ì§•ì— ë…ë¦½ì ì¸ GRU ê°€ì¤‘ì¹˜ í• ë‹¹
2. Hard í•¨ìˆ˜ ì˜µì…˜ìœ¼ë¡œ ì„ë² ë””ë“œ ìµœì í™” ì§€ì›
3. íŒ¨ë”© ì¸ì‹ íŠ¹ì§• ì¶”ì¶œ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple
from torch import Tensor

from src.models.LightningModel import LitModel
from src.models.ScaleAwareGRU import ScaleAwareGRU


class MSCSGRU_ScaleAware(LitModel):
    """
    MS-CSGRU with Scale-Aware GRU
    
    ì•„í‚¤í…ì²˜:
        Multi-Scale CNN (3 towers) 
        â†’ Scale-Aware GRU Layer 1 
        â†’ Scale-Aware GRU Layer 2
        â†’ Padding-Aware Feature Extraction
        â†’ Classifier
    
    ê°œì„ ì :
        1. ê° CNN ìŠ¤ì¼€ì¼(k=3,5,7)ì— ë…ë¦½ì ì¸ GRU ê°€ì¤‘ì¹˜
        2. ìŠ¤ì¼€ì¼ë³„ ì¤‘ìš”ë„ í•™ìŠµ ê°€ëŠ¥
        3. í•´ì„ ê°€ëŠ¥ì„± í–¥ìƒ
    """
    
    def __init__(
        self,
        learning_rate,
        input_size=8,
        hidden_size=64,
        classes=24,
        cnn_filters=32,
        dropout=0.3,
        use_hard_functions=False,  # Hard í•¨ìˆ˜ ì‚¬ìš© ì—¬ë¶€
        **kwargs
    ):
        super().__init__()
        
        self.lr = learning_rate
        self.classes = classes
        self.use_hard = use_hard_functions
        
        # Multi-Scale CNN: 3ê°œ íƒ€ì›Œ ë³‘ë ¬ ì²˜ë¦¬
        self.tower1 = nn.Sequential(
            nn.Conv1d(input_size, cnn_filters, 3, padding=1),
            nn.BatchNorm1d(cnn_filters), 
            nn.ReLU()
        )
        self.tower2 = nn.Sequential(
            nn.Conv1d(input_size, cnn_filters, 5, padding=2),
            nn.BatchNorm1d(cnn_filters), 
            nn.ReLU()
        )
        self.tower3 = nn.Sequential(
            nn.Conv1d(input_size, cnn_filters, 7, padding=3),
            nn.BatchNorm1d(cnn_filters), 
            nn.ReLU()
        )
        
        # CNN í›„ì²˜ë¦¬
        self.cnn_post = nn.Sequential(
            nn.BatchNorm1d(cnn_filters * 3),
            nn.ReLU(),
            nn.MaxPool1d(2, 2),
            nn.Dropout(dropout)
        )
        
        # Scale-Aware Stacked GRU
        self.gru1 = ScaleAwareGRU(
            scale_sizes=(cnn_filters, cnn_filters, cnn_filters),  # (32, 32, 32)
            hidden_size=hidden_size,
            use_hard_functions=use_hard_functions
        )
        self.dropout1 = nn.Dropout(dropout)
        
        self.gru2 = ScaleAwareGRU(
            scale_sizes=(hidden_size, hidden_size, hidden_size),  # GRU1 ì¶œë ¥ì„ 3ë“±ë¶„
            hidden_size=hidden_size,
            use_hard_functions=use_hard_functions
        )
        
        # ë¶„ë¥˜ê¸°
        self.output_layers = nn.Sequential(
            nn.Linear(hidden_size, 2*hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(2*hidden_size, classes)
        )
    
    def forward(
        self, 
        x: Tensor, 
        x_padding: Tensor, 
        y_targets: Tensor
    ) -> Tuple[Tensor, Tensor]:
        """
        Forward pass
        
        Args:
            x: ì…ë ¥ ì„¼ì„œ ë°ì´í„° (batch, time, channels)
            x_padding: íŒ¨ë”© ë§ˆìŠ¤í¬ (batch, time)
            y_targets: íƒ€ê²Ÿ ë ˆì´ë¸” (batch,)
        
        Returns:
            logits: í´ë˜ìŠ¤ ë¡œì§“ (batch, classes)
            loss: Cross-entropy loss
        """
        
        # Multi-Scale CNN
        x_conv = x.transpose(1, 2)  # (batch, channels, time)
        t1 = self.tower1(x_conv)
        t2 = self.tower2(x_conv) 
        t3 = self.tower3(x_conv)
        conv_out = torch.cat([t1, t2, t3], dim=1)
        conv_out = self.cnn_post(conv_out)
        
        # Scale-Aware GRU Layer 1
        conv_out = conv_out.transpose(1, 2)  # (batch, time, channels)
        gru1_out, _ = self.gru1(conv_out)
        gru1_out = self.dropout1(gru1_out)
        
        # Scale-Aware GRU Layer 2
        # GRU1 ì¶œë ¥(64 channels)ì„ ë³µì œí•˜ì—¬ 3ê°œ ìŠ¤ì¼€ì¼ë¡œ ì‚¬ìš©
        # ê° ìŠ¤ì¼€ì¼ì´ ë…ë¦½ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•˜ë„ë¡ í•¨
        gru1_expanded = gru1_out.repeat(1, 1, 3)  # (batch, time, 192)
        gru2_out, _ = self.gru2(gru1_expanded)
        
        # íŒ¨ë”© ì¸ì‹ íŠ¹ì§• ì¶”ì¶œ
        if x_padding is not None:
            # MaxPoolë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì ˆë°˜ì´ ë˜ì—ˆìœ¼ë¯€ë¡œ ì¡°ì •
            valid_lengths = (x_padding == 0).sum(dim=1) - 1
            valid_lengths = (valid_lengths / 2).long()  # MaxPool(2) ë°˜ì˜
            valid_lengths = valid_lengths.clamp(min=0, max=gru2_out.size(1)-1)
            
            batch_size = gru2_out.size(0)
            final_features = gru2_out[torch.arange(batch_size), valid_lengths]
        else:
            final_features = gru2_out[:, -1, :]
        
        # ë¶„ë¥˜
        logits = self.output_layers(final_features)
        loss = F.cross_entropy(logits, y_targets)
        
        return logits, loss
    
    def get_scale_importance(self) -> dict:
        """
        ê° ìŠ¤ì¼€ì¼ì˜ ì¤‘ìš”ë„ ë¶„ì„
        
        Returns:
            dict: GRU ë ˆì´ì–´ë³„ ìŠ¤ì¼€ì¼ ê°€ì¤‘ì¹˜
        """
        return {
            'gru_layer1': self.gru1.get_gate_weights(),
            'gru_layer2': self.gru2.get_gate_weights()
        }


class MSCSGRU_ScaleHard(MSCSGRU_ScaleAware):
    """
    MS-CSGRU with Scale-Aware + Hard Functions
    
    MSCSGRU_ScaleAwareì˜ Hard í•¨ìˆ˜ ë²„ì „
    ì„ë² ë””ë“œ ì‹œìŠ¤í…œ ìµœì í™”ë¥¼ ìœ„í•´ ëª¨ë“  í™œì„±í™” í•¨ìˆ˜ë¥¼ Hard ë²„ì „ìœ¼ë¡œ ì‚¬ìš©
    """
    
    def __init__(
        self,
        learning_rate,
        input_size=8,
        hidden_size=64,
        classes=24,
        cnn_filters=32,
        dropout=0.3,
        **kwargs
    ):
        # Hard í•¨ìˆ˜ ê°•ì œ í™œì„±í™”
        super().__init__(
            learning_rate=learning_rate,
            input_size=input_size,
            hidden_size=hidden_size,
            classes=classes,
            cnn_filters=cnn_filters,
            dropout=dropout,
            use_hard_functions=True,  # ê°•ì œë¡œ True
            **kwargs
        )


class MSCGRU_ScaleAware(LitModel):
    """
    MS-CGRU with Scale-Aware GRU (Single GRU)
    
    Stacked GRU ëŒ€ì‹  ë‹¨ì¼ GRU ì‚¬ìš©
    ë” ë¹ ë¥¸ í•™ìŠµê³¼ ì¶”ë¡ ì„ ìœ„í•œ ê²½ëŸ‰ ë²„ì „
    """
    
    def __init__(
        self,
        learning_rate,
        input_size=8,
        hidden_size=64,
        classes=24,
        cnn_filters=32,
        dropout=0.3,
        use_hard_functions=False,
        **kwargs
    ):
        super().__init__()
        
        self.lr = learning_rate
        self.classes = classes
        
        # Multi-Scale CNN
        self.tower1 = nn.Sequential(
            nn.Conv1d(input_size, cnn_filters, 3, padding=1),
            nn.BatchNorm1d(cnn_filters), nn.ReLU()
        )
        self.tower2 = nn.Sequential(
            nn.Conv1d(input_size, cnn_filters, 5, padding=2),
            nn.BatchNorm1d(cnn_filters), nn.ReLU()
        )
        self.tower3 = nn.Sequential(
            nn.Conv1d(input_size, cnn_filters, 7, padding=3),
            nn.BatchNorm1d(cnn_filters), nn.ReLU()
        )
        
        # CNN í›„ì²˜ë¦¬
        self.cnn_post = nn.Sequential(
            nn.BatchNorm1d(cnn_filters * 3),
            nn.ReLU(),
            nn.MaxPool1d(2, 2),
            nn.Dropout(dropout)
        )
        
        # Single Scale-Aware GRU
        self.gru = ScaleAwareGRU(
            scale_sizes=(cnn_filters, cnn_filters, cnn_filters),
            hidden_size=hidden_size,
            use_hard_functions=use_hard_functions
        )
        self.dropout = nn.Dropout(dropout)
        
        # ë¶„ë¥˜ê¸°
        self.output_layers = nn.Sequential(
            nn.Linear(hidden_size, 2*hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(2*hidden_size, classes)
        )
    
    def forward(
        self, 
        x: Tensor, 
        x_padding: Tensor, 
        y_targets: Tensor
    ) -> Tuple[Tensor, Tensor]:
        # Multi-Scale CNN
        x_conv = x.transpose(1, 2)
        t1 = self.tower1(x_conv)
        t2 = self.tower2(x_conv) 
        t3 = self.tower3(x_conv)
        conv_out = torch.cat([t1, t2, t3], dim=1)
        conv_out = self.cnn_post(conv_out)
        
        # Single GRU
        conv_out = conv_out.transpose(1, 2)
        gru_out, _ = self.gru(conv_out)
        
        # íŒ¨ë”© ì¸ì‹ íŠ¹ì§• ì¶”ì¶œ
        if x_padding is not None:
            valid_lengths = (x_padding == 0).sum(dim=1) - 1
            valid_lengths = (valid_lengths / 2).long()
            valid_lengths = valid_lengths.clamp(min=0, max=gru_out.size(1)-1)
            batch_size = gru_out.size(0)
            final_features = gru_out[torch.arange(batch_size), valid_lengths]
        else:
            final_features = gru_out[:, -1, :]
        
        final_features = self.dropout(final_features)
        
        # ë¶„ë¥˜
        logits = self.output_layers(final_features)
        loss = F.cross_entropy(logits, y_targets)
        
        return logits, loss


# Test the models
if __name__ == "__main__":
    print("ğŸ§ª Scale-Aware ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    batch_size, time_steps, input_channels = 4, 87, 8
    num_classes = 24
    
    x = torch.randn(batch_size, time_steps, input_channels)
    x_padding = torch.zeros(batch_size, time_steps)
    x_padding[:, 80:] = 1.0  # 80ë²ˆì§¸ ì´í›„ëŠ” íŒ¨ë”©
    y_targets = torch.randint(0, num_classes, (batch_size,))
    
    print(f"\nì…ë ¥ ë°ì´í„°:")
    print(f"  x: {x.shape}")
    print(f"  x_padding: {x_padding.shape}")
    print(f"  y_targets: {y_targets.shape}")
    
    models_to_test = [
        ('MSCSGRU_ScaleAware (Sigmoid/Tanh)', 
         MSCSGRU_ScaleAware(learning_rate=1e-3, input_size=8, classes=24, use_hard_functions=False)),
        
        ('MSCSGRU_ScaleHard (HardSigmoid/HardTanh)', 
         MSCSGRU_ScaleHard(learning_rate=1e-3, input_size=8, classes=24)),
        
        ('MSCGRU_ScaleAware (Single GRU)', 
         MSCGRU_ScaleAware(learning_rate=1e-3, input_size=8, classes=24, use_hard_functions=False)),
    ]
    
    print("\n" + "="*80)
    print("ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*80)
    
    for name, model in models_to_test:
        print(f"\nğŸ“Š {name}")
        print("-"*80)
        
        try:
            model.eval()
            with torch.no_grad():
                logits, loss = model(x, x_padding, y_targets)
            
            # íŒŒë¼ë¯¸í„° ìˆ˜
            num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            print(f"âœ… ì¶œë ¥ shape: {logits.shape}")
            print(f"âœ… ì†ì‹¤: {loss.item():.4f}")
            print(f"âœ… íŒŒë¼ë¯¸í„° ìˆ˜: {num_params:,}")
            
            # ìŠ¤ì¼€ì¼ ì¤‘ìš”ë„ ë¶„ì„ (MSCSGRUë§Œ)
            if hasattr(model, 'get_scale_importance'):
                importance = model.get_scale_importance()
                print(f"\nğŸ“ˆ ìŠ¤ì¼€ì¼ ì¤‘ìš”ë„ (GRU Layer 1 - Update Gate):")
                weights = importance['gru_layer1']['update_gate']
                print(f"  Scale 3 (kernel=3): {weights['scale_3']:.4f}")
                print(f"  Scale 5 (kernel=5): {weights['scale_5']:.4f}")
                print(f"  Scale 7 (kernel=7): {weights['scale_7']:.4f}")
                
                # ê°€ì¥ ì¤‘ìš”í•œ ìŠ¤ì¼€ì¼
                max_scale = max(weights.items(), key=lambda x: x[1])
                print(f"  â†’ ê°€ì¥ ì¤‘ìš”í•œ ìŠ¤ì¼€ì¼: {max_scale[0]} (ê°€ì¤‘ì¹˜: {max_scale[1]:.4f})")
            
        except Exception as e:
            print(f"âŒ ì—ëŸ¬: {str(e)}")
    
    # ì†ë„ ë¹„êµ
    print("\n" + "="*80)
    print("ì¶”ë¡  ì†ë„ ë¹„êµ")
    print("="*80)
    
    import time
    
    model_normal = MSCSGRU_ScaleAware(learning_rate=1e-3, input_size=8, classes=24, use_hard_functions=False)
    model_hard = MSCSGRU_ScaleHard(learning_rate=1e-3, input_size=8, classes=24)
    
    model_normal.eval()
    model_hard.eval()
    
    # Warmup
    for _ in range(10):
        with torch.no_grad():
            _ = model_normal(x, x_padding, y_targets)
            _ = model_hard(x, x_padding, y_targets)
    
    # ì¼ë°˜ ë²„ì „
    start = time.time()
    for _ in range(100):
        with torch.no_grad():
            _ = model_normal(x, x_padding, y_targets)
    time_normal = (time.time() - start) / 100 * 1000
    
    # Hard ë²„ì „
    start = time.time()
    for _ in range(100):
        with torch.no_grad():
            _ = model_hard(x, x_padding, y_targets)
    time_hard = (time.time() - start) / 100 * 1000
    
    print(f"\nSigmoid/Tanh:      {time_normal:.2f}ms")
    print(f"HardSigmoid/Tanh:  {time_hard:.2f}ms")
    print(f"ì†ë„ í–¥ìƒ:         {(time_normal - time_hard) / time_normal * 100:.1f}%")
    
    print("\n" + "="*80)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80)
    
    print("\nğŸ“ ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµ")
    print("  2. ê¸°ì¡´ MSCSGRUì™€ ì„±ëŠ¥ ë¹„êµ")
    print("  3. ìŠ¤ì¼€ì¼ ì¤‘ìš”ë„ ë¶„ì„")
    print("  4. ì¶”ë¡  ìµœì í™” (ONNX ë³€í™˜)")

