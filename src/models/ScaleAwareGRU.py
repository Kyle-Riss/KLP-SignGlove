"""
Scale-Aware GRU Implementation
ê° Multi-Scale CNN íƒ€ì›Œì˜ íŠ¹ì§•ì— ë…ë¦½ì ì¸ ê°€ì¤‘ì¹˜ë¥¼ í• ë‹¹í•˜ëŠ” GRU ì…€
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional


class ScaleAwareGRUCell(nn.Module):
    """
    Scale-Aware GRU Cell
    
    ê¸°ì¡´ GRUì™€ì˜ ì°¨ì´ì :
    - ì…ë ¥ì„ 3ê°œ ìŠ¤ì¼€ì¼(t3, t5, t7)ë¡œ ë¶„ë¦¬
    - ê° ìŠ¤ì¼€ì¼ì— ë…ë¦½ì ì¸ ê°€ì¤‘ì¹˜ í–‰ë ¬ í• ë‹¹
    - Update/Reset ê²Œì´íŠ¸ê°€ ìŠ¤ì¼€ì¼ë³„ ì¤‘ìš”ë„ë¥¼ í•™ìŠµ
    
    ìˆ˜ì‹:
        z_t = sigmoid(W_z3*t3 + W_z5*t5 + W_z7*t7 + U_z*h_{t-1} + b_z)
        r_t = sigmoid(W_r3*t3 + W_r5*t5 + W_r7*t7 + U_r*h_{t-1} + b_r)
        h_tilde = tanh(W_h3*t3 + W_h5*t5 + W_h7*t7 + U_h*(r_t âŠ™ h_{t-1}) + b_h)
        h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ h_tilde
    """
    
    def __init__(
        self,
        scale_sizes: Tuple[int, int, int] = (32, 32, 32),  # (t3, t5, t7)
        hidden_size: int = 64,
        use_hard_functions: bool = False
    ):
        super().__init__()
        
        self.scale_sizes = scale_sizes
        self.hidden_size = hidden_size
        self.use_hard = use_hard_functions
        
        # Update gate (z_t) - 3ê°œ ìŠ¤ì¼€ì¼ë³„ ê°€ì¤‘ì¹˜
        self.W_z3 = nn.Linear(scale_sizes[0], hidden_size, bias=False)
        self.W_z5 = nn.Linear(scale_sizes[1], hidden_size, bias=False)
        self.W_z7 = nn.Linear(scale_sizes[2], hidden_size, bias=False)
        self.U_z = nn.Linear(hidden_size, hidden_size, bias=True)
        
        # Reset gate (r_t) - 3ê°œ ìŠ¤ì¼€ì¼ë³„ ê°€ì¤‘ì¹˜
        self.W_r3 = nn.Linear(scale_sizes[0], hidden_size, bias=False)
        self.W_r5 = nn.Linear(scale_sizes[1], hidden_size, bias=False)
        self.W_r7 = nn.Linear(scale_sizes[2], hidden_size, bias=False)
        self.U_r = nn.Linear(hidden_size, hidden_size, bias=True)
        
        # Hidden state candidate (h_tilde) - 3ê°œ ìŠ¤ì¼€ì¼ë³„ ê°€ì¤‘ì¹˜
        self.W_h3 = nn.Linear(scale_sizes[0], hidden_size, bias=False)
        self.W_h5 = nn.Linear(scale_sizes[1], hidden_size, bias=False)
        self.W_h7 = nn.Linear(scale_sizes[2], hidden_size, bias=False)
        self.U_h = nn.Linear(hidden_size, hidden_size, bias=True)
        
        self._init_weights()
    
    def _init_weights(self):
        """Xavier ì´ˆê¸°í™”"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.zeros_(param)
    
    def forward(
        self, 
        t3: torch.Tensor,  # (batch, 32)
        t5: torch.Tensor,  # (batch, 32)
        t7: torch.Tensor,  # (batch, 32)
        h_prev: torch.Tensor  # (batch, hidden_size)
    ) -> torch.Tensor:
        """
        Forward pass
        
        Args:
            t3: Tower 1 ì¶œë ¥ (kernel=3)
            t5: Tower 2 ì¶œë ¥ (kernel=5)
            t7: Tower 3 ì¶œë ¥ (kernel=7)
            h_prev: ì´ì „ ì€ë‹‰ ìƒíƒœ
            
        Returns:
            h_t: í˜„ì¬ ì€ë‹‰ ìƒíƒœ (batch, hidden_size)
        """
        
        # Update gate (z_t)
        z_input = self.W_z3(t3) + self.W_z5(t5) + self.W_z7(t7) + self.U_z(h_prev)
        z_t = F.hardsigmoid(z_input) if self.use_hard else torch.sigmoid(z_input)
        
        # Reset gate (r_t)
        r_input = self.W_r3(t3) + self.W_r5(t5) + self.W_r7(t7) + self.U_r(h_prev)
        r_t = F.hardsigmoid(r_input) if self.use_hard else torch.sigmoid(r_input)
        
        # Hidden state candidate (h_tilde)
        h_input = self.W_h3(t3) + self.W_h5(t5) + self.W_h7(t7) + self.U_h(r_t * h_prev)
        h_tilde = F.hardtanh(h_input) if self.use_hard else torch.tanh(h_input)
        
        # Final hidden state
        h_t = (1 - z_t) * h_prev + z_t * h_tilde
        
        return h_t
    
    def get_gate_weights(self) -> dict:
        """
        ê° ìŠ¤ì¼€ì¼ì˜ ê°€ì¤‘ì¹˜ í¬ê¸° ë°˜í™˜ (í•´ì„ ê°€ëŠ¥ì„±)
        
        Returns:
            dict: ê° ê²Œì´íŠ¸ì˜ ìŠ¤ì¼€ì¼ë³„ ê°€ì¤‘ì¹˜ norm
        """
        return {
            'update_gate': {
                'scale_3': self.W_z3.weight.norm().item(),
                'scale_5': self.W_z5.weight.norm().item(),
                'scale_7': self.W_z7.weight.norm().item(),
            },
            'reset_gate': {
                'scale_3': self.W_r3.weight.norm().item(),
                'scale_5': self.W_r5.weight.norm().item(),
                'scale_7': self.W_r7.weight.norm().item(),
            },
            'hidden_gate': {
                'scale_3': self.W_h3.weight.norm().item(),
                'scale_5': self.W_h5.weight.norm().item(),
                'scale_7': self.W_h7.weight.norm().item(),
            }
        }


class ScaleAwareGRU(nn.Module):
    """
    Scale-Aware GRU Layer
    
    ScaleAwareGRUCellì„ ì‹œí€€ìŠ¤ ì „ì²´ì— ì ìš©
    """
    
    def __init__(
        self,
        scale_sizes: Tuple[int, int, int] = (32, 32, 32),
        hidden_size: int = 64,
        use_hard_functions: bool = False
    ):
        super().__init__()
        
        self.scale_sizes = scale_sizes
        self.hidden_size = hidden_size
        
        self.cell = ScaleAwareGRUCell(
            scale_sizes=scale_sizes,
            hidden_size=hidden_size,
            use_hard_functions=use_hard_functions
        )
    
    def forward(
        self,
        x: torch.Tensor,  # (batch, seq_len, sum(scale_sizes))
        h0: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass for entire sequence
        
        Args:
            x: ì…ë ¥ ì‹œí€€ìŠ¤ (batch, seq_len, 96)
               - 96 = 32 (t3) + 32 (t5) + 32 (t7)
            h0: ì´ˆê¸° ì€ë‹‰ ìƒíƒœ (batch, hidden_size)
        
        Returns:
            outputs: ëª¨ë“  íƒ€ì„ìŠ¤í…ì˜ ì€ë‹‰ ìƒíƒœ (batch, seq_len, hidden_size)
            h_n: ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ (batch, hidden_size)
        """
        batch_size, seq_len, _ = x.shape
        
        # ì´ˆê¸° ì€ë‹‰ ìƒíƒœ
        if h0 is None:
            h_t = torch.zeros(batch_size, self.hidden_size, device=x.device)
        else:
            h_t = h0
        
        outputs = []
        
        # ê° íƒ€ì„ìŠ¤í…ë§ˆë‹¤ GRU ì…€ ì‹¤í–‰
        for t in range(seq_len):
            # ì…ë ¥ì„ 3ê°œ ìŠ¤ì¼€ì¼ë¡œ ë¶„ë¦¬
            x_t = x[:, t, :]  # (batch, 96)
            t3 = x_t[:, :self.scale_sizes[0]]  # (batch, 32)
            t5 = x_t[:, self.scale_sizes[0]:self.scale_sizes[0]+self.scale_sizes[1]]  # (batch, 32)
            t7 = x_t[:, self.scale_sizes[0]+self.scale_sizes[1]:]  # (batch, 32)
            
            # GRU ì…€ ì‹¤í–‰
            h_t = self.cell(t3, t5, t7, h_t)
            outputs.append(h_t.unsqueeze(1))
        
        # ëª¨ë“  íƒ€ì„ìŠ¤í… ê²°í•©
        outputs = torch.cat(outputs, dim=1)  # (batch, seq_len, hidden_size)
        
        return outputs, h_t
    
    def get_gate_weights(self) -> dict:
        """ê° ìŠ¤ì¼€ì¼ì˜ ê°€ì¤‘ì¹˜ í¬ê¸° ë°˜í™˜"""
        return self.cell.get_gate_weights()


# Test code
if __name__ == "__main__":
    print("ğŸ§ª Scale-Aware GRU í…ŒìŠ¤íŠ¸...")
    
    # íŒŒë¼ë¯¸í„°
    batch_size = 4
    seq_len = 43
    scale_sizes = (32, 32, 32)
    hidden_size = 64
    
    # ì…ë ¥ ë°ì´í„°
    x = torch.randn(batch_size, seq_len, sum(scale_sizes))
    
    print(f"\nì…ë ¥ shape: {x.shape}")
    print(f"  - batch_size: {batch_size}")
    print(f"  - seq_len: {seq_len}")
    print(f"  - input_size: {sum(scale_sizes)} (32+32+32)")
    
    # 1. ì¼ë°˜ Sigmoid/Tanh ë²„ì „
    print("\n" + "="*70)
    print("1ï¸âƒ£ Scale-Aware GRU (Sigmoid/Tanh)")
    print("="*70)
    
    gru_normal = ScaleAwareGRU(
        scale_sizes=scale_sizes,
        hidden_size=hidden_size,
        use_hard_functions=False
    )
    
    outputs_normal, h_n_normal = gru_normal(x)
    print(f"âœ… ì¶œë ¥ shape: {outputs_normal.shape}")
    print(f"âœ… ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ: {h_n_normal.shape}")
    
    # ê°€ì¤‘ì¹˜ ë¶„ì„
    weights = gru_normal.get_gate_weights()
    print(f"\nğŸ“Š Update Gate ê°€ì¤‘ì¹˜ í¬ê¸°:")
    print(f"  - Scale 3 (kernel=3): {weights['update_gate']['scale_3']:.4f}")
    print(f"  - Scale 5 (kernel=5): {weights['update_gate']['scale_5']:.4f}")
    print(f"  - Scale 7 (kernel=7): {weights['update_gate']['scale_7']:.4f}")
    
    # 2. Hard í•¨ìˆ˜ ë²„ì „
    print("\n" + "="*70)
    print("2ï¸âƒ£ Scale-Aware GRU (HardSigmoid/HardTanh)")
    print("="*70)
    
    gru_hard = ScaleAwareGRU(
        scale_sizes=scale_sizes,
        hidden_size=hidden_size,
        use_hard_functions=True
    )
    
    outputs_hard, h_n_hard = gru_hard(x)
    print(f"âœ… ì¶œë ¥ shape: {outputs_hard.shape}")
    print(f"âœ… ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ: {h_n_hard.shape}")
    
    # 3. íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
    print("\n" + "="*70)
    print("3ï¸âƒ£ íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ")
    print("="*70)
    
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    # ê¸°ì¡´ GRU (ë¹„êµìš©)
    gru_standard = nn.GRU(sum(scale_sizes), hidden_size, 1, batch_first=True)
    
    print(f"ê¸°ì¡´ GRU:          {count_parameters(gru_standard):,} íŒŒë¼ë¯¸í„°")
    print(f"Scale-Aware GRU:   {count_parameters(gru_normal):,} íŒŒë¼ë¯¸í„°")
    print(f"ì¦ê°€ìœ¨:            {count_parameters(gru_normal) / count_parameters(gru_standard):.2f}x")
    
    # 4. ì†ë„ í…ŒìŠ¤íŠ¸
    print("\n" + "="*70)
    print("4ï¸âƒ£ ì¶”ë¡  ì†ë„ ë¹„êµ")
    print("="*70)
    
    import time
    
    # Warmup
    for _ in range(10):
        _ = gru_normal(x)
        _ = gru_hard(x)
    
    # ì¼ë°˜ ë²„ì „
    start = time.time()
    for _ in range(100):
        _ = gru_normal(x)
    time_normal = (time.time() - start) / 100 * 1000
    
    # Hard ë²„ì „
    start = time.time()
    for _ in range(100):
        _ = gru_hard(x)
    time_hard = (time.time() - start) / 100 * 1000
    
    print(f"Sigmoid/Tanh:      {time_normal:.2f}ms")
    print(f"HardSigmoid/Tanh:  {time_hard:.2f}ms")
    print(f"ì†ë„ í–¥ìƒ:         {(time_normal - time_hard) / time_normal * 100:.1f}%")
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")

