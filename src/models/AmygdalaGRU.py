"""
Amygdala-Boosted GRU (A-GRU)
í¸ë„ì²´ì—ì„œ ì˜ê°ì„ ë°›ì€ ì¤‘ìš”ë„ ê¸°ë°˜ ê¸°ì–µ ê°•í™” ë©”ì»¤ë‹ˆì¦˜

í•µì‹¬ ì•„ì´ë””ì–´:
- A-Net: í˜„ì¬ ì…ë ¥ì˜ ë¶„ë¥˜ì  ì¤‘ìš”ë„ ê³„ì‚° (í¸ë„ì²´ ì—­í• )
- ì…ë ¥ ì¦í­: ì¤‘ìš”í•œ ì…ë ¥ì„ ê°•í™”í•˜ì—¬ GRUì— ì „ë‹¬
- ê°•í™”ëœ ê¸°ì–µ: ì¤‘ìš”í•œ ì •ë³´ê°€ ë” ì˜ ì €ì¥ë˜ë„ë¡ ìœ ë„
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional


class AmygdalaNet(nn.Module):
    """
    A-Net: í¸ë„ì²´ ë„¤íŠ¸ì›Œí¬
    ì…ë ¥ X_tì™€ ì´ì „ ì€ë‹‰ ìƒíƒœ h_{t-1}ì„ ë³´ê³  ì¤‘ìš”ë„ e_t ê³„ì‚°
    
    ìˆ˜ì‹:
        e_t = Ïƒ(W_A [X_t âŠ• h_{t-1}] + b_A)
        X'_t = X_t âŠ™ (1 + Î³Â·e_t)
    """
    
    def __init__(self, input_size: int, hidden_size: int, gamma: float = 1.0):
        super().__init__()
        self.gamma = gamma
        
        # ì…ë ¥ê³¼ ì€ë‹‰ ìƒíƒœë¥¼ ì—°ê²°í•´ì„œ ì¤‘ìš”ë„ ê³„ì‚°
        self.importance_net = nn.Sequential(
            nn.Linear(input_size + hidden_size, (input_size + hidden_size) // 2),
            nn.Tanh(),
            nn.Linear((input_size + hidden_size) // 2, input_size),
            nn.Sigmoid()  # 0~1 ë²”ìœ„ì˜ ì¤‘ìš”ë„
        )
    
    def forward(self, x_t: torch.Tensor, h_prev: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x_t: (batch, input_size) - í˜„ì¬ ì…ë ¥
            h_prev: (batch, hidden_size) - ì´ì „ ì€ë‹‰ ìƒíƒœ
        
        Returns:
            x_boosted: (batch, input_size) - ì¦í­ëœ ì…ë ¥
            importance: (batch, input_size) - ì¤‘ìš”ë„ ì ìˆ˜
        """
        # [X_t âŠ• h_{t-1}] ì—°ê²°
        combined = torch.cat([x_t, h_prev], dim=1)
        
        # e_t = Ïƒ(W_A [X_t âŠ• h_{t-1}] + b_A)
        importance = self.importance_net(combined)
        
        # X'_t = X_t âŠ™ (1 + Î³Â·e_t)
        x_boosted = x_t * (1.0 + self.gamma * importance)
        
        return x_boosted, importance


class AGRUCell(nn.Module):
    """
    A-GRU Cell: Amygdala-Boosted GRU Cell
    A-Netìœ¼ë¡œ ì¦í­ëœ ì…ë ¥ì„ ì‚¬ìš©í•˜ëŠ” GRU ì…€
    
    ìˆ˜ì‹:
        r_t = Ïƒ(W_Xr X'_t + W_Hr h_{t-1} + b_r)
        z_t = Ïƒ(W_Xz X'_t + W_Hz h_{t-1} + b_z)
        hÌƒ_t = tanh(W_R (r_t âŠ™ h_{t-1}) + W_X X'_t + b_h)
        h_t = z_t âŠ™ hÌƒ_t + (1 - z_t) âŠ™ h_{t-1}
    """
    
    def __init__(self, input_size: int, hidden_size: int, gamma: float = 1.0):
        super().__init__()
        self.hidden_size = hidden_size
        
        # A-Net: í¸ë„ì²´ ë„¤íŠ¸ì›Œí¬
        self.a_net = AmygdalaNet(input_size, hidden_size, gamma)
        
        # GRU ê²Œì´íŠ¸ë“¤ (í‘œì¤€ GRUì™€ ë™ì¼í•˜ì§€ë§Œ X'_t ì‚¬ìš©)
        # Reset gate
        self.W_xr = nn.Linear(input_size, hidden_size, bias=True)
        self.W_hr = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # Update gate
        self.W_xz = nn.Linear(input_size, hidden_size, bias=True)
        self.W_hz = nn.Linear(hidden_size, hidden_size, bias=False)
        
        # Candidate hidden state
        self.W_x = nn.Linear(input_size, hidden_size, bias=True)
        self.W_r = nn.Linear(hidden_size, hidden_size, bias=False)
    
    def forward(self, x_t: torch.Tensor, h_prev: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x_t: (batch, input_size)
            h_prev: (batch, hidden_size)
        
        Returns:
            h_t: (batch, hidden_size) - ìƒˆë¡œìš´ ì€ë‹‰ ìƒíƒœ
            importance: (batch, input_size) - A-Net ì¤‘ìš”ë„ (ë¶„ì„ìš©)
        """
        # A-Net: X'_t = X_t âŠ™ (1 + Î³Â·e_t)
        x_boosted, importance = self.a_net(x_t, h_prev)
        
        # Reset gate: r_t = Ïƒ(W_Xr X'_t + W_Hr h_{t-1})
        r_t = torch.sigmoid(self.W_xr(x_boosted) + self.W_hr(h_prev))
        
        # Update gate: z_t = Ïƒ(W_Xz X'_t + W_Hz h_{t-1})
        z_t = torch.sigmoid(self.W_xz(x_boosted) + self.W_hz(h_prev))
        
        # Candidate: hÌƒ_t = tanh(W_R (r_t âŠ™ h_{t-1}) + W_X X'_t)
        h_tilde = torch.tanh(self.W_r(r_t * h_prev) + self.W_x(x_boosted))
        
        # Final: h_t = z_t âŠ™ hÌƒ_t + (1 - z_t) âŠ™ h_{t-1}
        h_t = z_t * h_tilde + (1 - z_t) * h_prev
        
        return h_t, importance


class AGRU(nn.Module):
    """
    A-GRU Layer: Amygdala-Boosted GRU
    ì „ì²´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ A-GRU Cell ì ìš©
    """
    
    def __init__(self, input_size: int, hidden_size: int, gamma: float = 1.0):
        super().__init__()
        self.cell = AGRUCell(input_size, hidden_size, gamma)
        self.hidden_size = hidden_size
    
    def forward(
        self, 
        x: torch.Tensor, 
        h0: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Args:
            x: (batch, seq_len, input_size)
            h0: (batch, hidden_size) - ì´ˆê¸° ì€ë‹‰ ìƒíƒœ
        
        Returns:
            outputs: (batch, seq_len, hidden_size) - ëª¨ë“  íƒ€ì„ìŠ¤í…ì˜ ì€ë‹‰ ìƒíƒœ
            h_n: (batch, hidden_size) - ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ
            importances: (batch, seq_len, input_size) - ê° íƒ€ì„ìŠ¤í…ì˜ ì¤‘ìš”ë„
        """
        batch_size, seq_len, _ = x.size()
        
        # ì´ˆê¸° ì€ë‹‰ ìƒíƒœ
        if h0 is None:
            h_t = torch.zeros(batch_size, self.hidden_size, device=x.device)
        else:
            h_t = h0
        
        outputs = []
        importances = []
        
        for t in range(seq_len):
            h_t, importance = self.cell(x[:, t], h_t)
            outputs.append(h_t.unsqueeze(1))
            importances.append(importance.unsqueeze(1))
        
        outputs = torch.cat(outputs, dim=1)
        importances = torch.cat(importances, dim=1)
        
        return outputs, h_t, importances


class StackedAGRU(nn.Module):
    """
    Stacked A-GRU: ë‹¤ì¸µ A-GRU
    """
    
    def __init__(
        self, 
        input_size: int, 
        hidden_size: int, 
        num_layers: int = 2,
        gamma: float = 1.0,
        dropout: float = 0.0
    ):
        super().__init__()
        self.num_layers = num_layers
        
        self.agru_layers = nn.ModuleList()
        self.dropout_layers = nn.ModuleList()
        
        for i in range(num_layers):
            layer_input_size = input_size if i == 0 else hidden_size
            self.agru_layers.append(AGRU(layer_input_size, hidden_size, gamma))
            if i < num_layers - 1:
                self.dropout_layers.append(nn.Dropout(dropout))
    
    def forward(
        self, 
        x: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor, list]:
        """
        Args:
            x: (batch, seq_len, input_size)
        
        Returns:
            outputs: (batch, seq_len, hidden_size) - ë§ˆì§€ë§‰ ë ˆì´ì–´ ì¶œë ¥
            h_n: (batch, hidden_size) - ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ
            all_importances: list of (batch, seq_len, input_size) - ê° ë ˆì´ì–´ì˜ ì¤‘ìš”ë„
        """
        all_importances = []
        
        current_input = x
        for i, agru in enumerate(self.agru_layers):
            outputs, h_n, importances = agru(current_input)
            all_importances.append(importances)
            
            if i < self.num_layers - 1:
                current_input = self.dropout_layers[i](outputs)
            else:
                current_input = outputs
        
        return outputs, h_n, all_importances


if __name__ == "__main__":
    print("ğŸ§ª A-GRU í…ŒìŠ¤íŠ¸...")
    
    batch_size, seq_len, input_size = 4, 43, 96
    hidden_size = 64
    
    # ë”ë¯¸ ë°ì´í„°
    x = torch.randn(batch_size, seq_len, input_size)
    
    # 1. A-GRU Cell í…ŒìŠ¤íŠ¸
    print("\n1ï¸âƒ£ A-GRU Cell")
    cell = AGRUCell(input_size, hidden_size, gamma=1.0)
    h_prev = torch.zeros(batch_size, hidden_size)
    h_t, importance = cell(x[:, 0], h_prev)
    print(f"   h_t: {h_t.shape}, importance: {importance.shape}")
    print(f"   Importance range: [{importance.min():.3f}, {importance.max():.3f}]")
    
    # 2. A-GRU Layer í…ŒìŠ¤íŠ¸
    print("\n2ï¸âƒ£ A-GRU Layer")
    agru = AGRU(input_size, hidden_size, gamma=1.0)
    outputs, h_n, importances = agru(x)
    print(f"   outputs: {outputs.shape}")
    print(f"   h_n: {h_n.shape}")
    print(f"   importances: {importances.shape}")
    
    # 3. Stacked A-GRU í…ŒìŠ¤íŠ¸
    print("\n3ï¸âƒ£ Stacked A-GRU (2 layers)")
    stacked_agru = StackedAGRU(input_size, hidden_size, num_layers=2, gamma=1.0, dropout=0.3)
    outputs, h_n, all_importances = stacked_agru(x)
    print(f"   outputs: {outputs.shape}")
    print(f"   h_n: {h_n.shape}")
    print(f"   importances (layer 1): {all_importances[0].shape}")
    print(f"   importances (layer 2): {all_importances[1].shape}")
    
    # 4. íŒŒë¼ë¯¸í„° ë¹„êµ
    print("\n4ï¸âƒ£ íŒŒë¼ë¯¸í„° ë¹„êµ")
    
    # í‘œì¤€ GRU
    standard_gru = nn.GRU(input_size, hidden_size, num_layers=2, batch_first=True)
    gru_params = sum(p.numel() for p in standard_gru.parameters())
    
    # A-GRU
    agru_params = sum(p.numel() for p in stacked_agru.parameters())
    
    # A-Netë§Œì˜ ì¶”ê°€ íŒŒë¼ë¯¸í„°
    a_net_params = sum(p.numel() for layer in stacked_agru.agru_layers 
                       for p in layer.cell.a_net.parameters())
    
    print(f"   Standard GRU: {gru_params:,} params")
    print(f"   A-GRU:        {agru_params:,} params")
    print(f"   A-Net only:   {a_net_params:,} params (+{a_net_params/gru_params*100:.1f}%)")
    print(f"   Overhead:     +{(agru_params - gru_params):,} params")
    
    print("\nâœ… A-GRU í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")





