import torch
import torch.nn as nn
import torch.nn.functional as F
from src.models.generalModels import *
from src.models.LightningModel import LitModel

from typing import Tuple
from torch import Tensor


class LSTM(LitModel):
    """
    SignSpeak ë…¼ë¬¸ ê¸°ë°˜ ê¸°ë³¸ LSTM ëª¨ë¸
    """
    
    def __init__(
        self,
        learning_rate,
        input_size=8,
        hidden_size=64,
        classes=24,
        batch_first=True,
        layers=2,
        dense_layer=(False, 64),
        dropout=0.2,
    ):
        super().__init__()

        # set hyperparameters
        self.lr = learning_rate
        self.dense = dense_layer
        self.classes = classes

        # if dense before RNN
        if self.dense:
            self.RNN = nn.Sequential(
                nn.Linear(input_size, 2 * hidden_size),
                nn.LSTM(
                    2 * hidden_size,
                    hidden_size,
                    num_layers=layers,
                    batch_first=batch_first,
                ),
            )
        else:
            self.RNN = nn.LSTM(
                input_size, hidden_size, num_layers=layers, batch_first=batch_first
            )

        self.output_layers = outputRNN(
            hidden_size=hidden_size, transformed_size=2*hidden_size, output_size=self.classes, dropout=dropout
        )

    def forward(
        self, x: Tensor, x_padding: Tensor, y_targets: Tensor
    ) -> Tuple[Tensor, Tensor]:
        outputs, (hidden, cell) = self.RNN(
            x
        )  # outputs: all timesteps (batch, seq, features), hidden/cell: last hidden state
        
        # ë‹¨ìˆœí™”ëœ íŒ¨ë”© ì²˜ë¦¬ - ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ì‚¬ìš©
        final_output = outputs[:, -1, :]  # (batch, features)
        
        logits = self.output_layers(
            final_output
        )  # output of last cell into dense layer
        loss = F.cross_entropy(logits, y_targets)  # cross entropy loss
        return logits, loss


class StackedLSTM(LitModel):
    """
    SignSpeak ë…¼ë¬¸ ê¸°ë°˜ Stacked LSTM ëª¨ë¸
    """
    
    def __init__(
        self,
        learning_rate,
        input_size=8,
        hidden_size=64,
        classes=24,
        batch_first=True,
        layers=2,
        dense_layer=(False, 64),
        dropout=0.2,
    ):
        super().__init__()

        # set hyperparameters
        self.lr = learning_rate
        self.dense = dense_layer
        self.classes = classes

        # Stacked LSTM with dropout between layers
        self.RNN = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers=layers, 
            batch_first=batch_first,
            dropout=dropout if layers > 1 else 0
        )

        self.output_layers = outputRNN(
            hidden_size=hidden_size, 
            transformed_size=2*hidden_size, 
            output_size=self.classes, 
            dropout=dropout
        )

    def forward(
        self, x: Tensor, x_padding: Tensor, y_targets: Tensor
    ) -> Tuple[Tensor, Tensor]:
        outputs, (hidden, cell) = self.RNN(x)
        
        # ë‹¨ìˆœí™”ëœ íŒ¨ë”© ì²˜ë¦¬ - ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ì‚¬ìš©
        final_output = outputs[:, -1, :]  # (batch, features)
        
        logits = self.output_layers(
            final_output
        )
        loss = F.cross_entropy(logits, y_targets)
        return logits, loss


# Test the models
if __name__ == "__main__":
    print("ğŸ§ª LSTM ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    print("âœ… LSTM ëª¨ë¸ë“¤ êµ¬í˜„ ì™„ë£Œ!")
    print("ğŸ“ ê¸°ë³¸ LSTM, StackedLSTM ëª¨ë¸")
    print("ğŸ”§ DynamicDataModuleê³¼ í˜¸í™˜ë˜ëŠ” êµ¬ì¡°")
    print("ğŸ¯ SignSpeak ë…¼ë¬¸ì˜ LSTM ì•„í‚¤í…ì²˜ êµ¬í˜„")
