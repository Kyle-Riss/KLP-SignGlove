import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple
from torch import Tensor

from src.models.LightningModel import LitModel


class MS3DGRU(LitModel):
    """
    Multi-Scale 3D CNN + GRU ëª¨ë¸
    ì‹œê°„-ê³µê°„ íŠ¹ì„± í•™ìŠµì„ ìœ„í•œ Multi-Scale 3D CNN ì ìš©
    """
    
    def __init__(
        self,
        learning_rate,
        input_size=8,
        hidden_size=64,
        classes=24,
        cnn_filters=32,
        dropout=0.2,
        **kwargs
    ):
        super().__init__()
        
        self.lr = learning_rate
        self.classes = classes
        
        # Multi-Scale 3D CNN: 3ê°œ íƒ€ì›Œ ë³‘ë ¬ ì²˜ë¦¬
        # Tower 1: ì‘ì€ ì»¤ë„ (3x3x3) - ì„¸ë°€í•œ íŠ¹ì„±
        self.tower1 = nn.Sequential(
            nn.Conv3d(1, cnn_filters, (3, 3, 3), padding=(1, 1, 1)),
            nn.BatchNorm3d(cnn_filters),
            nn.ReLU()
        )
        
        # Tower 2: ì¤‘ê°„ ì»¤ë„ (5x5x5) - ì¤‘ê°„ íŠ¹ì„±
        self.tower2 = nn.Sequential(
            nn.Conv3d(1, cnn_filters, (5, 5, 5), padding=(2, 2, 2)),
            nn.BatchNorm3d(cnn_filters),
            nn.ReLU()
        )
        
        # Tower 3: í° ì»¤ë„ (7x7x7) - ê±°ì‹œì  íŠ¹ì„±
        self.tower3 = nn.Sequential(
            nn.Conv3d(1, cnn_filters, (7, 7, 7), padding=(3, 3, 3)),
            nn.BatchNorm3d(cnn_filters),
            nn.ReLU()
        )
        
        # CNN í›„ì²˜ë¦¬ - ì‹œê°„ê³¼ ê³µê°„ ì°¨ì› ëª¨ë‘ pooling
        self.cnn_post = nn.Sequential(
            nn.BatchNorm3d(cnn_filters * 3),
            nn.ReLU(),
            nn.MaxPool3d((2, 4, 2)),  # ì‹œê°„, ë†’ì´, ë„ˆë¹„ ëª¨ë‘ pooling
            nn.Dropout3d(dropout)
        )
        
        # GRU - ê³µê°„ ì°¨ì›ì„ flattení•œ í›„ GRU ì…ë ¥
        self.gru = nn.GRU(cnn_filters * 3 * 1 * 1, hidden_size, 1, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        
        # ë¶„ë¥˜ê¸°
        self.output_layers = nn.Sequential(
            nn.Linear(hidden_size, 2*hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(2*hidden_size, classes)
        )
    
    def forward(self, x: Tensor, x_padding: Tensor, y_targets: Tensor) -> Tuple[Tensor, Tensor]:
        batch_size, time_steps, input_channels = x.shape
        
        # 3D í…ì„œë¡œ ë³€í™˜: (batch, 1, time, 4, 2) - ì„¼ì„œë¥¼ 4x2 ê³µê°„ìœ¼ë¡œ ì¬ë°°ì—´
        # 8ê°œ ì„¼ì„œë¥¼ 4x2ë¡œ ì¬ë°°ì—´í•˜ì—¬ ê³µê°„ì  êµ¬ì¡° ìƒì„±
        x_3d = x.view(batch_size, time_steps, 4, 2)  # (batch, time, 4, 2) -> í…ì„œ shape ë³€ê²½í•˜ëŠ” í•¨ìˆ˜ view()
        x_3d = x_3d.unsqueeze(1)  # (batch, 1, time, 4, 2) ì±„ë„ ì°¨ì› ì¶”ê°€ -> ì°¨ì› ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜ unsqueeze()
        x_3d = x_3d.transpose(1, 2)  # (batch, time, 1, 4, 2) ì‹œê°„, ì±„ë„ ì°¨ì› êµí™˜ -> 1ì€ ì±„ë„ ì°¨ì› -> ì±„ë„ ì°¨ì› ë¨¼ì € ì²˜ë¦¬í•˜ê³  ì‹œê°„ ì°¨ì› ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œ -> ì°¨ì›êµí™˜ 
        x_3d = x_3d.contiguous().view(batch_size, 1, time_steps, 4, 2) #3D í…ì„œ í˜•íƒœ -> ë©”ëª¨ë¦¬ ì—°ì†ì„± ë³´ì¥ -> ìµœì¢… shape ë³€ê²½
        
        # Multi-Scale 3D CNN
        t1 = self.tower1(x_3d) # (batch, filters, time, 4, 2) -> 3*3*3 ì»¤ë„
        t2 = self.tower2(x_3d) # 5*5*5 ì»¤ë„
        t3 = self.tower3(x_3d) # 7*7*7 ì»¤ë„
        
        conv_out = torch.cat([t1, t2, t3], dim=1)  # (batch, filters*3, time, 4, 2) 3ê°œ íƒ€ì›Œ ì¶œë ¥ ê²°í•©
        conv_out = self.cnn_post(conv_out)  # (batch, filters*3, time/2, 2, 1) MaxPool3dë¡œ í›„ì²˜ë¦¬
        
        # 3D â†’ 1D ë³€í™˜: (batch, time, features) 
        # conv_out shape: (batch, filters*3, time/2, 1, 1)
        # ê³µê°„ ì°¨ì›ì„ flatten: (batch, time/2, filters*3*1*1)
        conv_out = conv_out.permute(0, 2, 1, 3, 4)  # (batch, time/2, filters*3, 1, 1) 3D â†’ 1D ë³€í™˜
        conv_out = conv_out.contiguous().view(batch_size, conv_out.size(1), -1)  # (batch, time/2, filters*3*1*1) ê³µê°„ ì°¨ì›ì„ flatten -> (batch, time/2, features)) ìµœì¢…ê²°ê³¼ê°€ GRUì— ì…ë ¥ ê°€ëŠ¥í•œ í˜•íƒœ
        
        # GRU
        gru_out, _ = self.gru(conv_out) #ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬
        
        # íŒ¨ë”© ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë§ˆì§€ë§‰ ìœ íš¨í•œ íƒ€ì„ìŠ¤í… ì„ íƒ
        if x_padding is not None: #íŒ¨ë”© ê³ ë ¤í•´ì„œ ì‹¤ì œ ë°ì´í„°ì—ì„œ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ì„ íƒ -> íŒ¨ë”© ìˆìœ¼ë©´ ìœ íš¨í•œ íƒ€ì„ìŠ¤í…ìœ¼ë¡œ ê³„ì‚°í•˜ê³ , íŒ¨ë”©ì´ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ì‚¬ìš©
            valid_lengths = (x_padding == 0).sum(dim=1) - 1 # ê° ë°°ì¹˜ì˜ ìœ íš¨í•œ íƒ€ì„ìŠ¤í… ìˆ˜ ê³„ì‚°
            valid_lengths = valid_lengths.clamp(min=0, max=gru_out.size(1)-1) #í…ì„œì˜ ê°’ì„ ì§€ì •ëœ ë²”ìœ„ë¡œ ì œí•œí•˜ëŠ” í•¨ìˆ˜
            batch_size = gru_out.size(0)
            final_features = gru_out[torch.arange(batch_size), valid_lengths] 
        else:
            final_features = gru_out[:, -1, :]
        
        final_features = self.dropout(final_features) #ìµœì¢… íŠ¹ì§•ë²¡í„° (batch, hidden_size)
        
        # ë¶„ë¥˜
        logits = self.output_layers(final_features) #ë¶„ë¥˜ ë ˆì´ì–´ ì•„ê¹Œ (batch, classes=24)
        loss = F.cross_entropy(logits, y_targets) #ì†ì‹¤ ê³„ì‚°
        
        return logits, loss


class MS3DStackedGRU(LitModel):
    """
    Multi-Scale 3D CNN + Stacked GRU ëª¨ë¸ (CNN íŠ¹ì§• ì¶”ì¶œ ê°œì„ )
    ì‹œê°„-ê³µê°„ íŠ¹ì„± í•™ìŠµ + ë‹¤ì¸µ GRU + ê°œì„ ëœ CNN êµ¬ì¡°
    """
    
    def __init__(
        self,
        learning_rate,
        input_size=8,
        hidden_size=64,
        classes=24,
        cnn_filters=32,
        gru_layers=2,
        dropout=0.2,
        **kwargs
    ):
        super().__init__()
        
        self.lr = learning_rate
        self.classes = classes
        
        # ê°œì„ ëœ Multi-Scale 3D CNN: ë” ë‹¤ì–‘í•œ ì»¤ë„ í¬ê¸°ì™€ ê°œì„ ëœ êµ¬ì¡°
        self.tower1 = nn.Sequential(
            nn.Conv3d(1, cnn_filters, (3, 3, 3), padding=(1, 1, 1)),
            nn.BatchNorm3d(cnn_filters),
            nn.ReLU(),
            nn.Conv3d(cnn_filters, cnn_filters, (3, 3, 3), padding=(1, 1, 1)),  # ì¶”ê°€ Conv ë ˆì´ì–´
            nn.BatchNorm3d(cnn_filters),
            nn.ReLU()
        )
        
        self.tower2 = nn.Sequential(
            nn.Conv3d(1, cnn_filters, (5, 5, 5), padding=(2, 2, 2)),
            nn.BatchNorm3d(cnn_filters),
            nn.ReLU(),
            nn.Conv3d(cnn_filters, cnn_filters, (3, 3, 3), padding=(1, 1, 1)),  # ì¶”ê°€ Conv ë ˆì´ì–´
            nn.BatchNorm3d(cnn_filters),
            nn.ReLU()
        )
        
        self.tower3 = nn.Sequential(
            nn.Conv3d(1, cnn_filters, (7, 7, 7), padding=(3, 3, 3)),
            nn.BatchNorm3d(cnn_filters),
            nn.ReLU(),
            nn.Conv3d(cnn_filters, cnn_filters, (3, 3, 3), padding=(1, 1, 1)),  # ì¶”ê°€ Conv ë ˆì´ì–´
            nn.BatchNorm3d(cnn_filters),
            nn.ReLU()
        )
        
        # ê°œì„ ëœ CNN í›„ì²˜ë¦¬ - ì‹œê°„ ì°¨ì› ë³´ì¡´í•˜ëŠ” pooling
        self.cnn_post = nn.Sequential(
            nn.BatchNorm3d(cnn_filters * 3),
            nn.ReLU(),
            nn.MaxPool3d((1, 4, 2)),  # ì‹œê°„ ì°¨ì› ë³´ì¡´, ê³µê°„ ì°¨ì›ë§Œ pooling
            nn.Dropout3d(dropout)
        )
        
        # Stacked GRU - ê°œì„ ëœ ì…ë ¥ ì°¨ì›
        self.gru1 = nn.GRU(cnn_filters * 3 * 1 * 1, hidden_size, 1, batch_first=True)
        self.gru2 = nn.GRU(hidden_size, hidden_size, 1, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        
        # ë¶„ë¥˜ê¸°
        self.output_layers = nn.Sequential(
            nn.Linear(hidden_size, 2*hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(2*hidden_size, classes)
        )
    
    def forward(self, x: Tensor, x_padding: Tensor, y_targets: Tensor) -> Tuple[Tensor, Tensor]:
        batch_size, time_steps, input_channels = x.shape
        
        # 3D í…ì„œë¡œ ë³€í™˜: (batch, 1, time, 4, 2)
        x_3d = x.view(batch_size, time_steps, 4, 2)
        x_3d = x_3d.unsqueeze(1)
        x_3d = x_3d.transpose(1, 2)
        x_3d = x_3d.contiguous().view(batch_size, 1, time_steps, 4, 2)
        
        # Multi-Scale 3D CNN
        t1 = self.tower1(x_3d)
        t2 = self.tower2(x_3d)
        t3 = self.tower3(x_3d)
        
        conv_out = torch.cat([t1, t2, t3], dim=1)
        conv_out = self.cnn_post(conv_out)
        
        # 3D â†’ 1D ë³€í™˜
        # conv_out shape: (batch, filters*3, time/2, 1, 1)
        # ê³µê°„ ì°¨ì›ì„ flatten: (batch, time/2, filters*3*1*1)
        conv_out = conv_out.permute(0, 2, 1, 3, 4)  # (batch, time/2, filters*3, 1, 1)
        conv_out = conv_out.contiguous().view(batch_size, conv_out.size(1), -1)  # (batch, time/2, filters*3*1*1)
        
        # Stacked GRU
        gru1_out, _ = self.gru1(conv_out)
        gru1_out = self.dropout(gru1_out)
        gru2_out, _ = self.gru2(gru1_out)
        
        # íŒ¨ë”© ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë§ˆì§€ë§‰ ìœ íš¨í•œ íƒ€ì„ìŠ¤í… ì„ íƒ
        if x_padding is not None:
            valid_lengths = (x_padding == 0).sum(dim=1) - 1
            valid_lengths = valid_lengths.clamp(min=0, max=gru2_out.size(1)-1)
            batch_size = gru2_out.size(0)
            final_features = gru2_out[torch.arange(batch_size), valid_lengths]
        else:
            final_features = gru2_out[:, -1, :]
        
        final_features = self.dropout(final_features)
        
        # ë¶„ë¥˜
        logits = self.output_layers(final_features)
        loss = F.cross_entropy(logits, y_targets)
        
        return logits, loss


class SensorAware3DGRU(LitModel):
    """
    ì„¼ì„œ ê·¸ë£¹ë³„ 3D CNN + GRU ëª¨ë¸
    Yaw/Pitch/Rollê³¼ Flex 1-5ë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬
    """
    
    def __init__(
        self,
        learning_rate,
        input_size=8,
        hidden_size=64,
        classes=24,
        cnn_filters=32,
        dropout=0.2,
        **kwargs
    ):
        super().__init__()
        
        self.lr = learning_rate
        self.classes = classes
        
        # Yaw/Pitch/Roll ì„¼ì„œìš© 3D CNN (3ê°œ ì„¼ì„œ)
        self.orientation_cnn = nn.Sequential(
            nn.Conv3d(1, cnn_filters, (3, 3, 3), padding=(1, 1, 1)),
            nn.BatchNorm3d(cnn_filters),
            nn.ReLU(),
            nn.MaxPool3d((2, 3, 1)),  # ì‹œê°„, ë†’ì´, ë„ˆë¹„ ëª¨ë‘ pooling
            nn.Dropout3d(dropout)
        )
        
        # Flex 1-5 ì„¼ì„œìš© 3D CNN (5ê°œ ì„¼ì„œ)
        self.flex_cnn = nn.Sequential(
            nn.Conv3d(1, cnn_filters, (3, 3, 3), padding=(1, 1, 1)),
            nn.BatchNorm3d(cnn_filters),
            nn.ReLU(),
            nn.MaxPool3d((2, 5, 1)),  # ì‹œê°„, ë†’ì´, ë„ˆë¹„ ëª¨ë‘ pooling
            nn.Dropout3d(dropout)
        )
        
        # ê²°í•©ëœ íŠ¹ì§•ì„ ìœ„í•œ GRU - ê³µê°„ ì°¨ì›ì„ flattení•œ í›„ GRU ì…ë ¥
        self.gru = nn.GRU(cnn_filters * 2 * 1 * 1, hidden_size, 1, batch_first=True)
        self.dropout = nn.Dropout(dropout)
        
        # ë¶„ë¥˜ê¸°
        self.output_layers = nn.Sequential(
            nn.Linear(hidden_size, 2*hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(2*hidden_size, classes)
        )
    
    def forward(self, x: Tensor, x_padding: Tensor, y_targets: Tensor) -> Tuple[Tensor, Tensor]:
        batch_size, time_steps, input_channels = x.shape
        
        # ì„¼ì„œ ë¶„ë¦¬: Yaw/Pitch/Roll (0:3), Flex 1-5 (3:8)
        orientation_data = x[:, :, :3]  # (batch, time, 3)
        flex_data = x[:, :, 3:]  # (batch, time, 5)
        
        # Orientation ë°ì´í„° 3D ë³€í™˜
        ori_3d = orientation_data.view(batch_size, time_steps, 3, 1, 1)
        ori_3d = ori_3d.unsqueeze(1)  # (batch, 1, time, 3, 1)
        ori_3d = ori_3d.transpose(1, 2)  # (batch, time, 1, 3, 1)
        ori_3d = ori_3d.contiguous().view(batch_size, 1, time_steps, 3, 1)
        
        # Flex ë°ì´í„° 3D ë³€í™˜
        flex_3d = flex_data.view(batch_size, time_steps, 5, 1, 1)
        flex_3d = flex_3d.unsqueeze(1)  # (batch, 1, time, 5, 1)
        flex_3d = flex_3d.transpose(1, 2)  # (batch, time, 1, 5, 1)
        flex_3d = flex_3d.contiguous().view(batch_size, 1, time_steps, 5, 1)
        
        # ê° ì„¼ì„œ ê·¸ë£¹ë³„ 3D CNN ì²˜ë¦¬
        ori_features = self.orientation_cnn(ori_3d)  # (batch, filters, time/2, 1, 1)
        flex_features = self.flex_cnn(flex_3d)  # (batch, filters, time/2, 1, 1)
        
        # íŠ¹ì§• ê²°í•©
        # ori_features shape: (batch, filters, time/2, 1, 1)
        # flex_features shape: (batch, filters, time/2, 1, 1)
        # ê³µê°„ ì°¨ì›ì„ flatten: (batch, time/2, filters*1*1)
        ori_features = ori_features.permute(0, 2, 1, 3, 4)  # (batch, time/2, filters, 1, 1)
        ori_features = ori_features.contiguous().view(batch_size, ori_features.size(1), -1)  # (batch, time/2, filters)
        
        flex_features = flex_features.permute(0, 2, 1, 3, 4)  # (batch, time/2, filters, 1, 1)
        flex_features = flex_features.contiguous().view(batch_size, flex_features.size(1), -1)  # (batch, time/2, filters)
        
        combined_features = torch.cat([ori_features, flex_features], dim=-1)  # (batch, time/2, filters*2)
        
        # GRU
        gru_out, _ = self.gru(combined_features)
        
        # íŒ¨ë”© ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë§ˆì§€ë§‰ ìœ íš¨í•œ íƒ€ì„ìŠ¤í… ì„ íƒ
        if x_padding is not None:
            valid_lengths = (x_padding == 0).sum(dim=1) - 1
            valid_lengths = valid_lengths.clamp(min=0, max=gru_out.size(1)-1)
            batch_size = gru_out.size(0)
            final_features = gru_out[torch.arange(batch_size), valid_lengths]
        else:
            final_features = gru_out[:, -1, :]
        
        final_features = self.dropout(final_features)
        
        # ë¶„ë¥˜
        logits = self.output_layers(final_features)
        loss = F.cross_entropy(logits, y_targets)
        
        return logits, loss


# Test the models
if __name__ == "__main__":
    print("ğŸ§ª Multi-Scale 3D CNN ëª¨ë¸ë“¤ í…ŒìŠ¤íŠ¸...")
    
    # Test data
    batch_size, time_steps, input_channels = 4, 87, 8
    num_classes = 24
    
    x = torch.randn(batch_size, time_steps, input_channels)
    x_mask = torch.ones(batch_size, time_steps)
    y_targets = torch.randint(0, num_classes, (batch_size,))
    
    # Test MS3DGRU
    print("\nğŸ“Š MS3DGRU (Multi-Scale 3D CNN + GRU) í…ŒìŠ¤íŠ¸:")
    model_ms3d = MS3DGRU(
        learning_rate=1e-3,
        input_size=input_channels,
        hidden_size=64,
        classes=num_classes
    )
    
    logits, loss = model_ms3d(x, x_mask, y_targets)
    print(f"ì¶œë ¥ shape: {logits.shape}")
    print(f"ì†ì‹¤: {loss.item():.4f}")
    
    # Test MS3DStackedGRU
    print("\nğŸ“Š MS3DStackedGRU (Multi-Scale 3D CNN + Stacked GRU) í…ŒìŠ¤íŠ¸:")
    model_ms3d_stacked = MS3DStackedGRU(
        learning_rate=1e-3,
        input_size=input_channels,
        hidden_size=64,
        classes=num_classes
    )
    
    logits, loss = model_ms3d_stacked(x, x_mask, y_targets)
    print(f"ì¶œë ¥ shape: {logits.shape}")
    print(f"ì†ì‹¤: {loss.item():.4f}")
    
    # Test SensorAware3DGRU
    print("\nğŸ“Š SensorAware3DGRU (ì„¼ì„œ ê·¸ë£¹ë³„ 3D CNN + GRU) í…ŒìŠ¤íŠ¸:")
    model_sensor_aware = SensorAware3DGRU(
        learning_rate=1e-3,
        input_size=input_channels,
        hidden_size=64,
        classes=num_classes
    )
    
    logits, loss = model_sensor_aware(x, x_mask, y_targets)
    print(f"ì¶œë ¥ shape: {logits.shape}")
    print(f"ì†ì‹¤: {loss.item():.4f}")
    
    print("\nâœ… ëª¨ë“  Multi-Scale 3D CNN ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
