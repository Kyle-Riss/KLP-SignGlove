import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
import torchmetrics
from torchmetrics import Accuracy, F1Score, Precision, Recall, ConfusionMatrix
import numpy as np
from typing import Dict, Any, Optional


class StackedGRU(nn.Module):
    """
    SignsSpeak ë…¼ë¬¸ ê¸°ë°˜ Stacked GRU ëª¨ë¸
    
    ì•„í‚¤í…ì²˜:
    - ì…ë ¥: (batch_size, time_steps, channels)
    - 2ê°œì˜ GRU ë ˆì´ì–´ (hidden_size=64)
    - Dropout (0.2)
    - Dense ë ˆì´ì–´ (128)
    - ì¶œë ¥: (batch_size, num_classes)
    """
    
    def __init__(
        self,
        input_size: int = 8,
        hidden_size: int = 64,
        num_layers: int = 2,
        num_classes: int = 24,
        dropout: float = 0.2,
        dense_size: int = 128,
        bidirectional: bool = False
    ):
        super(StackedGRU, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_classes = num_classes
        self.dropout = dropout
        self.dense_size = dense_size
        self.bidirectional = bidirectional
        
        # GRU layers
        self.gru = nn.GRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
            bidirectional=bidirectional
        )
        
        # Calculate GRU output size
        gru_output_size = hidden_size * (2 if bidirectional else 1)
        
        # Dropout layer
        self.dropout_layer = nn.Dropout(dropout)
        
        # Dense layers
        self.dense = nn.Linear(gru_output_size, dense_size)
        self.output_layer = nn.Linear(dense_size, num_classes)
        
        # Activation function
        self.relu = nn.ReLU()
        
    def forward(self, x):
        """
        Forward pass
        
        Args:
            x: Input tensor of shape (batch_size, time_steps, input_size)
            
        Returns:
            Output tensor of shape (batch_size, num_classes)
        """
        # GRU forward pass
        gru_out, _ = self.gru(x)  # (batch_size, time_steps, hidden_size)
        
        # Take the last timestep output
        last_output = gru_out[:, -1, :]  # (batch_size, hidden_size)
        
        # Apply dropout
        dropped = self.dropout_layer(last_output)
        
        # Dense layers
        dense_out = self.relu(self.dense(dropped))
        output = self.output_layer(dense_out)
        
        return output


class StackedGRULightning(pl.LightningModule):
    """
    PyTorch Lightning wrapper for StackedGRU model
    SignsSpeak ë…¼ë¬¸ ê¸°ë°˜ í›ˆë ¨ ì„¤ì •
    """
    
    def __init__(
        self,
        input_size: int = 8,
        hidden_size: int = 64,
        num_layers: int = 2,
        num_classes: int = 24,
        dropout: float = 0.2,
        dense_size: int = 128,
        bidirectional: bool = False,
        learning_rate: float = 1e-3,
        weight_decay: float = 1e-4,
        # AdamW hyperparameters (SignsSpeak ë…¼ë¬¸ ê¸°ë°˜)
        adamw_beta1: float = 0.9,
        adamw_beta2: float = 0.999,
        adamw_eps: float = 1e-8,
        # Learning rate scheduler hyperparameters
        lr_scheduler_factor: float = 0.5,
        lr_scheduler_patience: int = 10,
        lr_scheduler_min_lr: float = 1e-6,
        lr_scheduler_threshold: float = 1e-4,
        class_names: Optional[list] = None
    ):
        super().__init__()
        
        # Save hyperparameters
        self.save_hyperparameters()
        
        # Model
        self.model = StackedGRU(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            num_classes=num_classes,
            dropout=dropout,
            dense_size=dense_size,
            bidirectional=bidirectional
        )
        
        # Hyperparameters
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        self.adamw_beta1 = adamw_beta1
        self.adamw_beta2 = adamw_beta2
        self.adamw_eps = adamw_eps
        self.lr_scheduler_factor = lr_scheduler_factor
        self.lr_scheduler_patience = lr_scheduler_patience
        self.lr_scheduler_min_lr = lr_scheduler_min_lr
        self.lr_scheduler_threshold = lr_scheduler_threshold
        
        # Class names
        self.class_names = class_names or [f"class_{i}" for i in range(num_classes)]
        
        # Loss function
        self.criterion = nn.CrossEntropyLoss()
        
        # Metrics
        self.train_accuracy = Accuracy(task="multiclass", num_classes=num_classes)
        self.val_accuracy = Accuracy(task="multiclass", num_classes=num_classes)
        self.test_accuracy = Accuracy(task="multiclass", num_classes=num_classes)
        
        # F1-Score metrics (SignsSpeak ë…¼ë¬¸ì—ì„œ ì¤‘ìš”í•˜ê²Œ ì‚¬ìš©)
        self.train_f1_macro = F1Score(task="multiclass", num_classes=num_classes, average="macro")
        self.val_f1_macro = F1Score(task="multiclass", num_classes=num_classes, average="macro")
        self.test_f1_macro = F1Score(task="multiclass", num_classes=num_classes, average="macro")
        
        self.train_f1_weighted = F1Score(task="multiclass", num_classes=num_classes, average="weighted")
        self.val_f1_weighted = F1Score(task="multiclass", num_classes=num_classes, average="weighted")
        self.test_f1_weighted = F1Score(task="multiclass", num_classes=num_classes, average="weighted")
        
        self.train_f1_micro = F1Score(task="multiclass", num_classes=num_classes, average="micro")
        self.val_f1_micro = F1Score(task="multiclass", num_classes=num_classes, average="micro")
        self.test_f1_micro = F1Score(task="multiclass", num_classes=num_classes, average="micro")
        
        # Precision and Recall
        self.train_precision = Precision(task="multiclass", num_classes=num_classes, average="macro")
        self.val_precision = Precision(task="multiclass", num_classes=num_classes, average="macro")
        self.test_precision = Precision(task="multiclass", num_classes=num_classes, average="macro")
        
        self.train_recall = Recall(task="multiclass", num_classes=num_classes, average="macro")
        self.val_recall = Recall(task="multiclass", num_classes=num_classes, average="macro")
        self.test_recall = Recall(task="multiclass", num_classes=num_classes, average="macro")
        
        # Confusion Matrix
        self.train_confusion_matrix = ConfusionMatrix(task="multiclass", num_classes=num_classes)
        self.val_confusion_matrix = ConfusionMatrix(task="multiclass", num_classes=num_classes)
        self.test_confusion_matrix = ConfusionMatrix(task="multiclass", num_classes=num_classes)
        
        # Per-class F1-Score for detailed analysis
        self.test_f1_per_class = F1Score(task="multiclass", num_classes=num_classes, average="none")
        
    def forward(self, x):
        return self.model(x)
    
    def training_step(self, batch, batch_idx):
        x, y = batch["measurement"], batch["label"]
        logits = self(x)
        loss = self.criterion(logits, y)
        
        # Metrics
        preds = torch.argmax(logits, dim=1)
        self.train_accuracy(preds, y)
        self.train_f1_macro(preds, y)
        self.train_f1_weighted(preds, y)
        self.train_f1_micro(preds, y)
        self.train_precision(preds, y)
        self.train_recall(preds, y)
        self.train_confusion_matrix(preds, y)
        
        # Logging
        self.log("train/loss", loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train/accuracy", self.train_accuracy, on_step=False, on_epoch=True, prog_bar=True)
        self.log("train/f1_macro", self.train_f1_macro, on_step=False, on_epoch=True, prog_bar=True)
        self.log("train/f1_weighted", self.train_f1_weighted, on_step=False, on_epoch=True)
        self.log("train/f1_micro", self.train_f1_micro, on_step=False, on_epoch=True)
        self.log("train/precision", self.train_precision, on_step=False, on_epoch=True)
        self.log("train/recall", self.train_recall, on_step=False, on_epoch=True)
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        x, y = batch["measurement"], batch["label"]
        logits = self(x)
        loss = self.criterion(logits, y)
        
        # Metrics
        preds = torch.argmax(logits, dim=1)
        self.val_accuracy(preds, y)
        self.val_f1_macro(preds, y)
        self.val_f1_weighted(preds, y)
        self.val_f1_micro(preds, y)
        self.val_precision(preds, y)
        self.val_recall(preds, y)
        self.val_confusion_matrix(preds, y)
        
        # Logging
        self.log("val/loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log("val/accuracy", self.val_accuracy, on_step=False, on_epoch=True, prog_bar=True)
        self.log("val/f1_macro", self.val_f1_macro, on_step=False, on_epoch=True, prog_bar=True)
        self.log("val/f1_weighted", self.val_f1_weighted, on_step=False, on_epoch=True)
        self.log("val/f1_micro", self.val_f1_micro, on_step=False, on_epoch=True)
        self.log("val/precision", self.val_precision, on_step=False, on_epoch=True)
        self.log("val/recall", self.val_recall, on_step=False, on_epoch=True)
        
        return loss
    
    def test_step(self, batch, batch_idx):
        x, y = batch["measurement"], batch["label"]
        logits = self(x)
        loss = self.criterion(logits, y)
        
        # Metrics
        preds = torch.argmax(logits, dim=1)
        self.test_accuracy(preds, y)
        self.test_f1_macro(preds, y)
        self.test_f1_weighted(preds, y)
        self.test_f1_micro(preds, y)
        self.test_precision(preds, y)
        self.test_recall(preds, y)
        self.test_confusion_matrix(preds, y)
        self.test_f1_per_class(preds, y)
        
        # Logging
        self.log("test/loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log("test/accuracy", self.test_accuracy, on_step=False, on_epoch=True, prog_bar=True)
        self.log("test/f1_macro", self.test_f1_macro, on_step=False, on_epoch=True, prog_bar=True)
        self.log("test/f1_weighted", self.test_f1_weighted, on_step=False, on_epoch=True)
        self.log("test/f1_micro", self.test_f1_micro, on_step=False, on_epoch=True)
        self.log("test/precision", self.test_precision, on_step=False, on_epoch=True)
        self.log("test/recall", self.test_recall, on_step=False, on_epoch=True)
        
        return loss
    
    def configure_optimizers(self):
        """
        SignsSpeak ë…¼ë¬¸ ê¸°ë°˜ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        - AdamW ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©
        - Plateau learning rate decay ì ìš©
        """
        # AdamW ì˜µí‹°ë§ˆì´ì € (SignsSpeak ë…¼ë¬¸ê³¼ ë™ì¼)
        optimizer = AdamW(
            self.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay,
            betas=(self.adamw_beta1, self.adamw_beta2),
            eps=self.adamw_eps
        )
        
        # Plateau learning rate decay (SignsSpeak ë…¼ë¬¸ê³¼ ë™ì¼)
        scheduler = ReduceLROnPlateau(
            optimizer,
            mode="max",  # ì •í™•ë„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§
            factor=self.lr_scheduler_factor,  # í•™ìŠµë¥  ê°ì†Œ ë¹„ìœ¨
            patience=self.lr_scheduler_patience,  # ëŒ€ê¸° ì—í¬í¬ ìˆ˜
            verbose=True,
            min_lr=self.lr_scheduler_min_lr,  # ìµœì†Œ í•™ìŠµë¥ 
            threshold=self.lr_scheduler_threshold,  # ê°œì„  ì„ê³„ê°’
            threshold_mode='rel'  # ìƒëŒ€ì  ì„ê³„ê°’
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": "val/accuracy",  # ê²€ì¦ ì •í™•ë„ ëª¨ë‹ˆí„°ë§
                "interval": "epoch",
                "frequency": 1
            }
        }
    
    def get_detailed_evaluation_report(self) -> str:
        """ìƒì„¸í•œ í‰ê°€ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
        if not hasattr(self, 'test_accuracy') or self.test_accuracy.compute() is None:
            return "âŒ í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = self.test_accuracy.compute().item()
        f1_macro = self.test_f1_macro.compute().item()
        f1_weighted = self.test_f1_weighted.compute().item()
        f1_micro = self.test_f1_micro.compute().item()
        precision = self.test_precision.compute().item()
        recall = self.test_recall.compute().item()
        
        # í´ë˜ìŠ¤ë³„ F1-Score
        f1_per_class = self.test_f1_per_class.compute()
        
        # í˜¼ë™ í–‰ë ¬
        confusion_matrix = self.test_confusion_matrix.compute()
        
        report = f"""
ğŸ“Š SignsSpeak ë…¼ë¬¸ ê¸°ë°˜ ìƒì„¸ í‰ê°€ ë³´ê³ ì„œ
{'='*60}

ğŸ¯ ì „ì²´ ì„±ëŠ¥ ì§€í‘œ:
  â€¢ ì •í™•ë„ (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)
  â€¢ F1-Score (Macro): {f1_macro:.4f} ({f1_macro*100:.2f}%)
  â€¢ F1-Score (Weighted): {f1_weighted:.4f} ({f1_weighted*100:.2f}%)
  â€¢ F1-Score (Micro): {f1_micro:.4f} ({f1_micro*100:.2f}%)
  â€¢ Precision (Macro): {precision:.4f} ({precision*100:.2f}%)
  â€¢ Recall (Macro): {recall:.4f} ({recall*100:.2f}%)

ğŸ“ˆ F1-Score ë¶„ì„:
  â€¢ Macro F1: í´ë˜ìŠ¤ë³„ F1ì˜ í‰ê·  (ë¶ˆê· í˜• ë°ì´í„°ì— ì í•©)
  â€¢ Weighted F1: í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ì— ë”°ë¥¸ ê°€ì¤‘ í‰ê· 
  â€¢ Micro F1: ì „ì²´ TP, FP, FNìœ¼ë¡œ ê³„ì‚° (ì •í™•ë„ì™€ ë™ì¼)

ğŸ” í´ë˜ìŠ¤ë³„ F1-Score (ìƒìœ„ 10ê°œ):
"""
        
        # í´ë˜ìŠ¤ë³„ F1-Score ì •ë ¬
        class_f1_scores = []
        for i, f1_score in enumerate(f1_per_class):
            class_f1_scores.append((self.class_names[i], f1_score.item()))
        
        class_f1_scores.sort(key=lambda x: x[1], reverse=True)
        
        for i, (class_name, f1_score) in enumerate(class_f1_scores[:10]):
            report += f"  {i+1:2d}. {class_name}: {f1_score:.4f} ({f1_score*100:.2f}%)\n"
        
        if len(class_f1_scores) > 10:
            report += f"  ... (ì´ {len(class_f1_scores)}ê°œ í´ë˜ìŠ¤)\n"
        
        # ì„±ëŠ¥ ë¶„ì„
        avg_f1 = sum([score for _, score in class_f1_scores]) / len(class_f1_scores)
        min_f1 = min([score for _, score in class_f1_scores])
        max_f1 = max([score for _, score in class_f1_scores])
        
        report += f"""
ğŸ“Š ì„±ëŠ¥ ë¶„ì„:
  â€¢ í‰ê·  F1-Score: {avg_f1:.4f} ({avg_f1*100:.2f}%)
  â€¢ ìµœê³  F1-Score: {max_f1:.4f} ({max_f1*100:.2f}%)
  â€¢ ìµœì € F1-Score: {min_f1:.4f} ({min_f1*100:.2f}%)
  â€¢ F1-Score í‘œì¤€í¸ì°¨: {np.std([score for _, score in class_f1_scores]):.4f}

ğŸ¯ SignsSpeak ë…¼ë¬¸ ë¹„êµ:
  â€¢ ì •í™•ë„: {accuracy*100:.2f}% (ë…¼ë¬¸ ëŒ€ë¹„ ìš°ìˆ˜/ë³´í†µ/ê°œì„  í•„ìš”)
  â€¢ F1-Score: {f1_macro*100:.2f}% (ë¶ˆê· í˜• ë°ì´í„° ê³ ë ¤í•œ í‰ê°€)

ğŸ’¡ ê¶Œì¥ì‚¬í•­:
"""
        
        if f1_macro < 0.7:
            report += "  â€¢ F1-Scoreê°€ ë‚®ìŠµë‹ˆë‹¤. ë°ì´í„° ì¦ê°•ì´ë‚˜ ëª¨ë¸ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
        elif f1_macro < 0.85:
            report += "  â€¢ F1-Scoreê°€ ë³´í†µì…ë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.\n"
        else:
            report += "  â€¢ F1-Scoreê°€ ìš°ìˆ˜í•©ë‹ˆë‹¤! í˜„ì¬ ëª¨ë¸ì´ ì˜ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
        
        if max_f1 - min_f1 > 0.3:
            report += "  â€¢ í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ì°¨ì´ê°€ í½ë‹ˆë‹¤. ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.\n"
        
        report += f"{'='*60}"
        
        return report
    
    def save_evaluation_results(self, filepath: str = "evaluation_results.txt"):
        """í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        report = self.get_detailed_evaluation_report()
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"ğŸ“„ í‰ê°€ ê²°ê³¼ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# Test the model
if __name__ == "__main__":
    print("ğŸ§ª StackedGRU ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # Test model creation
    model = StackedGRU(
        input_size=8,
        hidden_size=64,
        num_layers=2,
        num_classes=24,
        dropout=0.2,
        dense_size=128
    )
    
    # Test forward pass
    batch_size = 16
    time_steps = 100
    input_size = 8
    
    x = torch.randn(batch_size, time_steps, input_size)
    output = model(x)
    
    print(f"âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    print(f"  ì…ë ¥ shape: {x.shape}")
    print(f"  ì¶œë ¥ shape: {output.shape}")
    print(f"  ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters()):,}")
    
    # Test Lightning wrapper
    lightning_model = StackedGRULightning(
        input_size=8,
        hidden_size=64,
        num_layers=2,
        num_classes=24,
        dropout=0.2,
        dense_size=128,
        learning_rate=1e-3,
        class_names=['ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…', 'ã…', 'ã…‘', 'ã…“', 'ã…•', 'ã…—', 'ã…›', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£']
    )
    
    print(f"âœ… Lightning ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    print(f"  í´ë˜ìŠ¤ ìˆ˜: {lightning_model.model.num_classes}")
    print(f"  íˆë“  í¬ê¸°: {lightning_model.model.hidden_size}")
    print(f"  GRU ë ˆì´ì–´ ìˆ˜: {lightning_model.model.num_layers}")
    print(f"  ë“œë¡­ì•„ì›ƒ: {lightning_model.model.dropout}")
    print(f"  Dense í¬ê¸°: {lightning_model.model.dense_size}")
    
    print("\nğŸ¯ SignsSpeak ë…¼ë¬¸ ê¸°ë°˜ Stacked GRU ëª¨ë¸ êµ¬í˜„ ì™„ë£Œ!")