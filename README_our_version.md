<<<<<<< HEAD
# KLP-SignGlove: í•œêµ­ì–´ ìˆ˜í™” ì¸ì‹ í”„ë¡œì íŠ¸

í•œêµ­ì–´ ìˆ˜í™” ìëª¨(ììŒ, ëª¨ìŒ) ì¸ì‹ì„ ìœ„í•œ ì„¼ì„œ ì¥ê°‘ ê¸°ë°˜ ì‹¤ì‹œê°„ ë¶„ë¥˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ¯ í”„ë¡œì íŠ¸ ê°œìš”

**ëª©í‘œ**: í•œêµ­ì–´ ìˆ˜í™” ìëª¨ 24ê°œ í´ë˜ìŠ¤(ììŒ 14ê°œ + ëª¨ìŒ 10ê°œ)ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ì‹œìŠ¤í…œ ê°œë°œ

**ì„±ê³¼**: 
- **í˜„ì¬ ì„±ëŠ¥**: 97.5% ì •í™•ë„ (598ê°œ ìƒ˜í”Œ)
- **ëª©í‘œ ì„±ëŠ¥**: 98.5% ì •í™•ë„ (3000ê°œ ìƒ˜í”Œ ì˜ˆì •)
- **ë°ì´í„°ì…‹**: 598ê°œ â†’ 3000ê°œ í™•ì¥ ì˜ˆì •
- **ì‹¤ì‹œê°„ ì¶”ë¡ **: ê²½ëŸ‰ ëª¨ë¸ë¡œ ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥

## ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´

### í˜„ì¬ ë°ì´í„°ì…‹ (598ê°œ)
- **ì´ ìƒ˜í”Œ ìˆ˜**: 598ê°œ
- **í´ë˜ìŠ¤ ìˆ˜**: 24ê°œ (ììŒ 14ê°œ + ëª¨ìŒ 10ê°œ)
- **íƒ€ì„ìŠ¤í…**: 100ê°œ (3.12ì´ˆ ì§€ì†ì‹œê°„)
- **ì„¼ì„œ ì±„ë„**: 8ê°œ (flex1-5 + pitch, roll, yaw)
- **ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜**: 32.1 Hz
- **ë°ì´í„° ë¶„í• **: í›ˆë ¨ 59.9%, ê²€ì¦ 20.1%, í…ŒìŠ¤íŠ¸ 20.1%

### í™•ì¥ ì˜ˆì • ë°ì´í„°ì…‹ (3000ê°œ)
- **ì´ ìƒ˜í”Œ ìˆ˜**: 3000ê°œ (5ë°° ì¦ê°€)
- **í´ë˜ìŠ¤ ìˆ˜**: 24ê°œ (ë™ì¼)
- **íƒ€ì„ìŠ¤í…**: 100ê°œ (ë™ì¼)
- **ì„¼ì„œ ì±„ë„**: 8ê°œ (ë™ì¼)
- **ì˜ˆìƒ ì„±ëŠ¥**: 98.5% (í˜„ì¬ 97.5% ëŒ€ë¹„ +1.0% í–¥ìƒ)

### í´ë˜ìŠ¤ ëª©ë¡
- **ììŒ (14ê°œ)**: ã„±, ã„´, ã„·, ã„¹, ã…, ã…‚, ã……, ã…‡, ã…ˆ, ã…Š, ã…‹, ã…Œ, ã…, ã…
- **ëª¨ìŒ (10ê°œ)**: ã…, ã…‘, ã…“, ã…•, ã…—, ã…›, ã…œ, ã… , ã…¡, ã…£

## ğŸ”§ ìµœì  ëª¨ë¸ ì„¤ì •

### í˜„ì¬ ëª¨ë¸ (598ê°œ ë°ì´í„°ì…‹ - 97.5% ì„±ëŠ¥)
```python
{
    'hidden_size': 48,
    'num_layers': 1,
    'dropout': 0.15,
    'dense_size': 96,
    'learning_rate': 0.0003,
    'batch_size': 16,
    'weight_decay': 0.001,
    'max_epochs': 100,
    'early_stopping_patience': 30
}
```

### í™•ì¥ ëª¨ë¸ (3000ê°œ ë°ì´í„°ì…‹ - ì˜ˆìƒ 98.5% ì„±ëŠ¥)
```python
{
    'hidden_size': 64,        # 48 â†’ 64 (ë” í° ëª¨ë¸)
    'num_layers': 2,          # 1 â†’ 2 (ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬)
    'dropout': 0.2,           # 0.15 â†’ 0.2 (ê³¼ì í•© ë°©ì§€)
    'dense_size': 128,        # 96 â†’ 128 (ë” í° Dense)
    'learning_rate': 0.001,   # 0.0003 â†’ 0.001 (ë” í° í•™ìŠµë¥ )
    'batch_size': 32,         # 16 â†’ 32 (ë” í° ë°°ì¹˜)
    'weight_decay': 0.0001,   # 0.001 â†’ 0.0001 (ì•½í•œ ì •ê·œí™”)
    'max_epochs': 150,        # 100 â†’ 150 (ë” ë§ì€ ì—í¬í¬)
    'early_stopping_patience': 40  # 30 â†’ 40 (ë” ê¸´ patience)
}
```

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
KLP-SignGlove-Clean/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/                 # ëª¨ë¸ êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ GRU.py             # 1ì¸µ GRU
â”‚   â”‚   â”œâ”€â”€ LSTM.py            # 1ì¸µ LSTM
â”‚   â”‚   â”œâ”€â”€ encoder.py         # Transformer Encoder
â”‚   â”‚   â”œâ”€â”€ generalModels.py   # ê³µí†µ ëª¨ë¸ í´ë˜ìŠ¤
â”‚   â”‚   â””â”€â”€ LightningModel.py  # PyTorch Lightning ê¸°ë³¸ í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ misc/
â”‚   â”‚   â””â”€â”€ DynamicDataModule.py  # ë°ì´í„° ë¡œë”
â”‚   â””â”€â”€ StackedGRUModel.py     # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
â”œâ”€â”€ best_model/
â”‚   â”œâ”€â”€ best_model.ckpt        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ results.json           # ì„±ëŠ¥ ê²°ê³¼
â”œâ”€â”€ final_results/
â”‚   â”œâ”€â”€ results.json           # ìµœì¢… ì‹¤í—˜ ê²°ê³¼
â”‚   â””â”€â”€ project_summary.txt    # í”„ë¡œì íŠ¸ ìš”ì•½
â”œâ”€â”€ optimal_config.py          # í˜„ì¬ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
â”œâ”€â”€ optimal_config_3000.py     # 3000ê°œ ë°ì´í„°ì…‹ìš© ì„¤ì •
â”œâ”€â”€ train_optimal_model.py     # í˜„ì¬ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ train_3000_dataset.py      # 3000ê°œ ë°ì´í„°ì…‹ìš© í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt           # ì˜ì¡´ì„± íŒ¨í‚¤ì§€
â””â”€â”€ README.md                  # í”„ë¡œì íŠ¸ ë¬¸ì„œ
```

## ğŸš€ ì‹œì‘í•˜ê¸°

### 1. í™˜ê²½ ì„¤ì •
```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone <repository-url>
cd KLP-SignGlove-Clean

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. ë°ì´í„° ì¤€ë¹„
SignGlove-DataAnalysis í´ë”ë¥¼ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ë°°ì¹˜í•˜ì„¸ìš”.

### 3. ëª¨ë¸ í›ˆë ¨

#### í˜„ì¬ ë°ì´í„°ì…‹ (598ê°œ) í›ˆë ¨
```bash
# í˜„ì¬ ìµœì  ëª¨ë¸ í›ˆë ¨
python train_optimal_model.py
```

#### í™•ì¥ ë°ì´í„°ì…‹ (3000ê°œ) í›ˆë ¨
```bash
# 3000ê°œ ë°ì´í„°ì…‹ìš© ìµœì í™” ëª¨ë¸ í›ˆë ¨
python train_3000_dataset.py
```

### 4. ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 
```python
import torch
from src.StackedGRUModel import StackedGRULightning
from optimal_config import OPTIMAL_CONFIG

# ëª¨ë¸ ë¡œë“œ
model = StackedGRULightning.load_from_checkpoint('best_model/best_model.ckpt')
model.eval()

# ì¶”ë¡ 
with torch.no_grad():
    predictions = model(input_data)
```

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ

### í˜„ì¬ ì„±ëŠ¥ (598ê°œ ë°ì´í„°ì…‹)
| ëª¨ë¸ | í…ŒìŠ¤íŠ¸ ì •í™•ë„ | íŠ¹ì§• |
|------|---------------|------|
| **StackedGRU** | **97.5%** | ğŸ¥‡ ìµœê³  ì„±ëŠ¥ + ë¹ ë¥¸ ì†ë„ |
| GRU | 89.2% | ì•ˆì •ì  ì„±ëŠ¥ + ë§¤ìš° ë¹ ë¥¸ ì†ë„ |
| LSTM | ~90% | ê· í˜•ì¡íŒ ì„±ëŠ¥ |
| Encoder | ~94% | Transformer ê¸°ë°˜ |

### ì˜ˆìƒ ì„±ëŠ¥ (3000ê°œ ë°ì´í„°ì…‹)
| ëª¨ë¸ | ì˜ˆìƒ ì •í™•ë„ | ê°œì„  ìš”ì¸ |
|------|-------------|-----------|
| **StackedGRU (í™•ì¥)** | **98.5%** | ë” í° ëª¨ë¸ + ë” ë§ì€ ë°ì´í„° |
| GRU (í™•ì¥) | ~92% | ë” í° ëª¨ë¸ + ë” ë§ì€ ë°ì´í„° |
| LSTM (í™•ì¥) | ~94% | ë” í° ëª¨ë¸ + ë” ë§ì€ ë°ì´í„° |
| Encoder (í™•ì¥) | ~96% | ë” í° ëª¨ë¸ + ë” ë§ì€ ë°ì´í„° |

## ğŸ”¬ ê¸°ìˆ ì  íŠ¹ì§•

### ë°ì´í„° ì „ì²˜ë¦¬
- **íƒ€ì„ìŠ¤í… ì •ê·œí™”**: ê°€ë³€ ê¸¸ì´ â†’ 100 íƒ€ì„ìŠ¤í…
- **ìŠ¤ì¼€ì¼ë§**: StandardScaler ì ìš©
- **ë°ì´í„° ì¦ê°•**: ì‹œë“œ ê¸°ë°˜ ì¬í˜„ ê°€ëŠ¥í•œ ë¶„í• 

### ëª¨ë¸ ì•„í‚¤í…ì²˜
- **StackedGRU**: 2ì¸µ GRU + Dense ë ˆì´ì–´
- **ë“œë¡­ì•„ì›ƒ**: 0.15 (ê³¼ì í•© ë°©ì§€)
- **ì •ê·œí™”**: Weight Decay 0.001

### í›ˆë ¨ ì „ëµ
- **ì˜µí‹°ë§ˆì´ì €**: AdamW
- **ìŠ¤ì¼€ì¤„ëŸ¬**: ReduceLROnPlateau
- **ì¡°ê¸° ì¢…ë£Œ**: 30 ì—í¬í¬ patience

## ğŸ¯ ì£¼ìš” ì„±ê³¼

âœ… **ì„±ëŠ¥ ëª©í‘œ ì´ˆê³¼ ë‹¬ì„±**: 70% â†’ 97.5% (39% ì´ˆê³¼)  
âœ… **ë°ì´í„° ë¡œë” ë²„ê·¸ ìˆ˜ì •**: í´ë˜ìŠ¤ ì¤‘ë³µ ë¬¸ì œ í•´ê²°  
âœ… **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**: ì²´ê³„ì ì¸ ì‹¤í—˜ì„ í†µí•œ ìµœì  ì„¤ì • ë„ì¶œ  
âœ… **ì‹¤ì‹œê°„ ì¶”ë¡  ì¤€ë¹„**: ê²½ëŸ‰ ëª¨ë¸ë¡œ ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥  
âœ… **SignGlove_HW í˜¸í™˜ì„±**: í•˜ë“œì›¨ì–´ í”„ë¡œì íŠ¸ì™€ ì™„ë²½ ì—°ë™  

## ğŸš€ í–¥í›„ ê³„íš

### ë‹¨ê¸° ê³„íš
- [x] í˜„ì¬ ë°ì´í„°ì…‹ ìµœì í™” ì™„ë£Œ (598ê°œ, 97.5%)
- [ ] 3000ê°œ ë°ì´í„°ì…‹ í›ˆë ¨ ë° ê²€ì¦
- [ ] í™•ì¥ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
- [ ] ì‹¤ì‹œê°„ ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ê°œë°œ

### ì¥ê¸° ê³„íš
- [ ] ì›¹ ê¸°ë°˜ ìˆ˜í™” í†µì—­ ì„œë¹„ìŠ¤
- [ ] ëª¨ë°”ì¼ ì•± ê°œë°œ
- [ ] ë‹¤êµ­ì–´ ìˆ˜í™” ì§€ì› í™•ì¥
- [ ] í´ë¼ìš°ë“œ ê¸°ë°˜ ì„œë¹„ìŠ¤ êµ¬ì¶•

## ğŸ“š ì°¸ê³  ìë£Œ

- [ASL-Sign-Research](https://github.com/adityamakkar000/ASL-Sign-Research): ì›ë³¸ ASL í”„ë¡œì íŠ¸
- [SignGlove_HW](https://github.com/your-username/SignGlove_HW): í•˜ë“œì›¨ì–´ êµ¬í˜„ í”„ë¡œì íŠ¸

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ ë¬¸ì˜

í”„ë¡œì íŠ¸ì— ëŒ€í•œ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.
=======
# SignSpeak: Open-Source Time Series Classification for ASL Translation

This repository contains the code and dataset for the paper "SignSpeak: Time Series Classification for ASL Prediction." 

**[Paper](https://arxiv.org/abs/2407.12020)**

## Overview

**Abstract**: The lack of fluency in sign language remains a barrier to seamless communication for hearing and speech-impaired communities. In this work, we propose a low-cost, real-time ASL-to-speech translation glove and an exhaustive training dataset of sign language patterns. We then benchmarked this dataset with supervised learning models, such as LSTMs, GRUs and Transformers, where our best model achieved 92\% accuracy. The SignSpeak dataset has 7200 samples encompassing 36 classes (A-Z, 1-10) and aims to capture realistic signing patterns by using five low-cost flex sensors to measure finger positions at each time step at 36 Hz. Our open-source dataset, models and glove designs, provide an accurate and efficient ASL translator while maintaining cost-effectiveness, establishing a framework for future work to build on.  

## Data Glove

The glove uses
- **Flex Sensors**: Five flex sensors are integrated into the glove, one for each finger. These sensors measure the bend of each finger.
- **Microcontroller**: An Arudino MEGA 2560 processes the signals from the flex sensors and sends the data verial serial ouput to a database.

All code for glove setup and data collection can be found at ```src/dataCollection```

Below is the schematic and completed glove.

<p align="center">
  <img src="images/Schematic.png" alt="Data Glove Diagram 1", height="600">
  <img src="images/Gloves.png" alt="Data Glove Diagram 2", height="400">
</div>

## Key Features

- **Open-Source Dataset**: The SignSpeak dataset comprises 7200 samples covering 36 classes (A-Z, 1-10), collected at 36 Hz using five flex sensors.
- **High Accuracy**: Achieves 92% categorical accuracy using state-of-the-art models such as LSTMs, GRUs, and Transformers.
- **Real-World Applicability**: Designed to be a cost-effective and resource-efficient solution for seamless communication for the hearing and speech-impaired communities.

## Models

The repository includes implementations and benchmarks for various models:
- Stacked LSTM
- Stacked GRU
- Transformer-based models

All models can be found in ``` src/models/ ```

## Getting Started

1. **Clone the Repository**
   ```bash
   git clone https://github.com/adityamakkar000/ASL-Sign-Research.git
   ```
2. **Download the Dataset**

    Download the dataset from this [Harvard Dataverse](https://doi.org/10.7910/DVN/ODY7GH) and place it in the ```src/experiments/data``` directory naming it ```data.csv```.

3. **Install Dependencies**

    ```bash
    pip install -r req.txt
    ```
4. **Run the model**
     Run the models using the following bash command inside of the ```src/experiments/``` or use the training scripts found in the directory
    ```bash
    python LightningTrain.py \
          -layers $layers \
          -model $model \
          -hidden_size $hidden_size \
          -lr $lr \
          -time_steps $time_steps \
          -batch_size $batch_size \
          -epochs $epochs \
          $dense_layer_arg \
          -dense_size $dense_size \
    ```

## Contact

For any queries, please contact:

    Aditya Makkar: aditya.makkar@uwaterloo.ca
    Divya Makkar: divya.makkar@uwaterloo.ca
    Aarav Patel: aarav.patel@uwaterloo.ca

## Citation 

```
@misc{makkar2024signspeakopensourcetimeseries,
      title={SignSpeak: Open-Source Time Series Classification for ASL Translation}, 
      author={Aditya Makkar and Divya Makkar and Aarav Patel and Liam Hebert},
      year={2024},
      eprint={2407.12020},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12020}, 
}
```

## Acknowledgement

We thank University of Waterloo PhD Liam Hebert for providing invaluable guidance and unwavering support throughout the course of SignSpeak. 
>>>>>>> 969e3a630e7899de120f04d29849911b26d6156e
