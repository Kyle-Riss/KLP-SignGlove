"""
A-GRU í´ë˜ìŠ¤ë³„ ì¤‘ìš”ë„ íŒ¨í„´ ë¶„ì„
ììŒ vs ëª¨ìŒì—ì„œ A-Netì˜ ì‘ë™ ì°¨ì´ ë¶„ì„
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from tqdm import tqdm

from src.models.AGRUModels import AGRUModel
from src.misc.DynamicDataModule import DynamicDataModule

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# ì¶œë ¥ ë””ë ‰í† ë¦¬
OUTPUT_DIR = Path("visualizations/agru_class_analysis")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def load_trained_model(checkpoint_path):
    """í•™ìŠµëœ A-GRU ëª¨ë¸ ë¡œë“œ"""
    model = AGRUModel.load_from_checkpoint(
        checkpoint_path,
        learning_rate=0.001,
        input_size=8,
        hidden_size=64,
        classes=24,
        layers=2,
        dropout=0.3,
        gamma=1.0
    )
    model.eval()
    return model


def analyze_class_patterns(model, datamodule):
    """
    í´ë˜ìŠ¤ë³„ A-Net ì¤‘ìš”ë„ íŒ¨í„´ ë¶„ì„
    
    ëª©í‘œ:
    1. ììŒ vs ëª¨ìŒì˜ ì¤‘ìš”ë„ íŒ¨í„´ ì°¨ì´
    2. ê° í´ë˜ìŠ¤ì˜ ì¤‘ìš”ë„ ì‹œê°„ì  ë¶„í¬
    3. ì±„ë„ë³„ ì¤‘ìš”ë„ ì°¨ì´ (Flex vs IMU)
    """
    datamodule.setup('test')
    test_loader = datamodule.test_dataloader()
    
    # ëª¨ë¸ì„ CPUë¡œ ì´ë™
    device = torch.device('cpu')
    model = model.to(device)
    
    # í´ë˜ìŠ¤ ì •ì˜
    consonants = ['ã„±', 'ã„´', 'ã„·', 'ã„¹', 'ã…', 'ã…‚', 'ã……', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
    vowels = ['ã…', 'ã…‘', 'ã…“', 'ã…•', 'ã…—', 'ã…›', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£']
    all_classes = consonants + vowels
    
    # ê²°ê³¼ ì €ì¥ìš©
    class_results = {
        'consonants': {
            'importances': [],      # A-Net ì¤‘ìš”ë„
            'flex_signals': [],     # Flex ì„¼ì„œ
            'imu_signals': [],      # IMU ì„¼ì„œ
            'labels': []
        },
        'vowels': {
            'importances': [],
            'flex_signals': [],
            'imu_signals': [],
            'labels': []
        }
    }
    
    print("ğŸ“Š í´ë˜ìŠ¤ë³„ ì¤‘ìš”ë„ ì¶”ì¶œ ì¤‘...")
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Processing batches"):
            x = batch['measurement'].to(device)  # (batch, time, 8)
            y = batch['label'].cpu().numpy()
            
            # A-GRU forward pass
            outputs, h_n, all_importances = model.agru(x)
            
            # Layer 1ì˜ importance (ë” rawí•œ íŒ¨í„´)
            importance = all_importances[0].cpu().numpy()  # (batch, time, 8)
            x_np = x.cpu().numpy()
            
            # ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œ ì²˜ë¦¬
            for i in range(x.size(0)):
                class_idx = y[i]
                class_name = all_classes[class_idx]
                
                # ììŒ vs ëª¨ìŒ êµ¬ë¶„
                if class_name in consonants:
                    category = 'consonants'
                else:
                    category = 'vowels'
                
                class_results[category]['importances'].append(importance[i])
                class_results[category]['flex_signals'].append(x_np[i, :, :5])  # Flex 5ê°œ
                class_results[category]['imu_signals'].append(x_np[i, :, 5:])   # IMU 3ê°œ
                class_results[category]['labels'].append(class_name)
    
    # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
    for category in ['consonants', 'vowels']:
        class_results[category]['importances'] = np.array(class_results[category]['importances'])
        class_results[category]['flex_signals'] = np.array(class_results[category]['flex_signals'])
        class_results[category]['imu_signals'] = np.array(class_results[category]['imu_signals'])
    
    print(f"âœ… ììŒ ìƒ˜í”Œ: {len(class_results['consonants']['labels'])}ê°œ")
    print(f"âœ… ëª¨ìŒ ìƒ˜í”Œ: {len(class_results['vowels']['labels'])}ê°œ")
    
    return class_results, all_classes


def plot_category_comparison(class_results):
    """ììŒ vs ëª¨ìŒ ë¹„êµ í”Œë¡¯"""
    fig, axes = plt.subplots(3, 2, figsize=(14, 12))
    
    categories = ['consonants', 'vowels']
    titles = ['Consonants', 'Vowels']
    
    for col, (category, title) in enumerate(zip(categories, titles)):
        importance = class_results[category]['importances']  # (N, time, 8)
        flex = class_results[category]['flex_signals']       # (N, time, 5)
        imu = class_results[category]['imu_signals']         # (N, time, 3)
        
        # í‰ê·  ê³„ì‚°
        mean_importance = importance.mean(axis=0)  # (time, 8)
        mean_flex = flex.mean(axis=0)              # (time, 5)
        mean_imu = imu.mean(axis=0)                # (time, 3)
        
        # 1. Flex ì„¼ì„œ ì‹ í˜¸
        ax = axes[0, col]
        for i in range(5):
            ax.plot(mean_flex[:, i], alpha=0.7, label=f'Flex {i+1}')
        ax.set_title(f'{title} - Flex Sensors', fontsize=12, fontweight='bold')
        ax.set_ylabel('Signal Value')
        ax.legend(loc='upper right', fontsize=8)
        ax.grid(True, alpha=0.3)
        
        # 2. A-Net ì¤‘ìš”ë„ (Flex ì±„ë„)
        ax = axes[1, col]
        for i in range(5):
            ax.plot(mean_importance[:, i], alpha=0.7, label=f'Importance Ch{i+1}')
        ax.set_title(f'{title} - A-Net Importance (Flex Channels)', fontsize=12, fontweight='bold')
        ax.set_ylabel('Importance')
        ax.legend(loc='upper right', fontsize=8)
        ax.grid(True, alpha=0.3)
        
        # 3. í‰ê·  ë¹„êµ (Flex vs Importance)
        ax = axes[2, col]
        avg_flex = mean_flex.mean(axis=1)
        avg_importance = mean_importance[:, :5].mean(axis=1)
        
        ax.plot(avg_flex, label='Avg Flex Signal', linewidth=2, color='blue', alpha=0.7)
        ax.plot(avg_importance, label='Avg A-Net Importance', linewidth=2, color='red', alpha=0.7)
        ax.set_title(f'{title} - Average Patterns', fontsize=12, fontweight='bold')
        ax.set_xlabel('Time Steps')
        ax.set_ylabel('Value')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_path = OUTPUT_DIR / "category_comparison.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ììŒ/ëª¨ìŒ ë¹„êµ í”Œë¡¯ ì €ì¥: {output_path}")
    plt.close()


def plot_channel_importance_heatmap(class_results):
    """ì±„ë„ë³„ ì¤‘ìš”ë„ íˆíŠ¸ë§µ (Flex vs IMU)"""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    categories = ['consonants', 'vowels']
    titles = ['Consonants', 'Vowels']
    
    for ax, category, title in zip(axes, categories, titles):
        importance = class_results[category]['importances']  # (N, time, 8)
        
        # ì‹œê°„ ì¶• í‰ê·  â†’ (N, 8)
        mean_over_time = importance.mean(axis=1)
        
        # ì „ì²´ í‰ê·  â†’ (8,)
        channel_importance = mean_over_time.mean(axis=0)
        
        # íˆíŠ¸ë§µ ë°ì´í„° ì¤€ë¹„ (8ê°œ ì±„ë„ì„ ì„¸ë¡œë¡œ í‘œì‹œ)
        data = channel_importance.reshape(-1, 1)  # (8, 1)
        
        # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
        channel_labels = [f'Flex {i+1}' for i in range(5)] + [f'IMU {i+1}' for i in range(3)]
        sns.heatmap(data.T, annot=True, fmt='.3f', cmap='YlOrRd', 
                   xticklabels=channel_labels,
                   yticklabels=['Importance'],
                   cbar_kws={'label': 'Importance'},
                   ax=ax)
        
        ax.set_title(f'{title} - Channel Importance', fontsize=12, fontweight='bold')
        ax.set_xlabel('Channels')
    
    plt.tight_layout()
    output_path = OUTPUT_DIR / "channel_importance_heatmap.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ì±„ë„ ì¤‘ìš”ë„ íˆíŠ¸ë§µ ì €ì¥: {output_path}")
    plt.close()


def plot_temporal_importance_distribution(class_results):
    """ì‹œê°„ì  ì¤‘ìš”ë„ ë¶„í¬ ë¹„êµ"""
    fig, axes = plt.subplots(2, 1, figsize=(14, 8))
    
    categories = ['consonants', 'vowels']
    titles = ['Consonants', 'Vowels']
    colors = ['blue', 'red']
    
    for ax, category, title, color in zip(axes, categories, titles, colors):
        importance = class_results[category]['importances']  # (N, time, 8)
        
        # ì±„ë„ í‰ê·  â†’ (N, time)
        importance_over_channels = importance.mean(axis=2)
        
        # ì „ì²´ ìƒ˜í”Œ í‰ê· ê³¼ í‘œì¤€í¸ì°¨
        mean_importance = importance_over_channels.mean(axis=0)
        std_importance = importance_over_channels.std(axis=0)
        
        # í”Œë¡¯
        time_steps = np.arange(len(mean_importance))
        ax.plot(time_steps, mean_importance, color=color, linewidth=2, label='Mean Importance')
        ax.fill_between(time_steps, 
                        mean_importance - std_importance,
                        mean_importance + std_importance,
                        color=color, alpha=0.2, label='Â±1 std')
        
        ax.set_title(f'{title} - Temporal Importance Distribution', fontsize=12, fontweight='bold')
        ax.set_xlabel('Time Steps')
        ax.set_ylabel('A-Net Importance')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_path = OUTPUT_DIR / "temporal_importance_distribution.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ì‹œê°„ì  ì¤‘ìš”ë„ ë¶„í¬ ì €ì¥: {output_path}")
    plt.close()


def calculate_statistics(class_results):
    """ì •ëŸ‰ì  í†µê³„ ê³„ì‚°"""
    stats = {}
    
    for category in ['consonants', 'vowels']:
        importance = class_results[category]['importances']  # (N, time, 8)
        flex = class_results[category]['flex_signals']       # (N, time, 5)
        
        # Flex ì±„ë„ë§Œ ì‚¬ìš© (ì²« 5ê°œ ì±„ë„)
        importance_flex = importance[:, :, :5]  # (N, time, 5)
        
        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        correlations = []
        peak_diffs = []
        
        for i in range(importance.shape[0]):
            # í‰ê·  Flex vs í‰ê·  Importance
            avg_flex = flex[i].mean(axis=1)
            avg_importance = importance_flex[i].mean(axis=1)
            
            # Pearson ìƒê´€ê³„ìˆ˜
            corr = np.corrcoef(avg_flex, avg_importance)[0, 1]
            correlations.append(corr)
            
            # í”¼í¬ ìœ„ì¹˜ ì°¨ì´
            flex_peak = np.argmax(avg_flex)
            importance_peak = np.argmax(avg_importance)
            peak_diffs.append(importance_peak - flex_peak)
        
        stats[category] = {
            'correlation': {
                'mean': np.mean(correlations),
                'std': np.std(correlations)
            },
            'peak_diff': {
                'mean': np.mean(peak_diffs),
                'std': np.std(peak_diffs)
            },
            'importance_mean': importance.mean(),
            'importance_std': importance.std(),
            'flex_mean': flex.mean(),
            'flex_std': flex.std()
        }
    
    return stats


def print_statistics(stats):
    """í†µê³„ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ“Š í´ë˜ìŠ¤ë³„ ì •ëŸ‰ì  ë¶„ì„ ê²°ê³¼")
    print("="*60)
    
    for category in ['consonants', 'vowels']:
        category_name = "Consonants" if category == 'consonants' else "Vowels"
        print(f"\n[{category_name}]")
        print(f"  ìƒê´€ê³„ìˆ˜: {stats[category]['correlation']['mean']:.3f} Â± {stats[category]['correlation']['std']:.3f}")
        print(f"  ì‹œê°„ ì°¨ì´: {stats[category]['peak_diff']['mean']:.1f} Â± {stats[category]['peak_diff']['std']:.1f} steps")
        print(f"  í‰ê·  ì¤‘ìš”ë„: {stats[category]['importance_mean']:.3f} Â± {stats[category]['importance_std']:.3f}")
        print(f"  í‰ê·  Flex ì‹ í˜¸: {stats[category]['flex_mean']:.3f} Â± {stats[category]['flex_std']:.3f}")
    
    # ììŒ vs ëª¨ìŒ ë¹„êµ
    print("\n" + "-"*60)
    print("[Consonants vs Vowels Comparison]")
    
    corr_diff = stats['consonants']['correlation']['mean'] - stats['vowels']['correlation']['mean']
    print(f"  ìƒê´€ê³„ìˆ˜ ì°¨ì´: {corr_diff:.3f}")
    
    peak_diff = stats['consonants']['peak_diff']['mean'] - stats['vowels']['peak_diff']['mean']
    print(f"  ì‹œê°„ ì°¨ì´ ì°¨ì´: {peak_diff:.1f} steps")
    
    importance_diff = stats['consonants']['importance_mean'] - stats['vowels']['importance_mean']
    print(f"  í‰ê·  ì¤‘ìš”ë„ ì°¨ì´: {importance_diff:.3f}")
    
    print("="*60)


def main():
    print("ğŸ”¬ A-GRU í´ë˜ìŠ¤ë³„ ì¤‘ìš”ë„ íŒ¨í„´ ë¶„ì„ ì‹œì‘...\n")
    
    # 1. ëª¨ë¸ ë¡œë“œ
    checkpoint_path = "checkpoints/best_model_epoch=46_val/loss=0.00.ckpt"
    print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
    model = load_trained_model(checkpoint_path)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
    
    # 2. ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    datamodule = DynamicDataModule(
        data_dir='/home/billy/25-1kp/SignGlove_HW/datasets/unified',
        batch_size=32,
        test_size=0.2,
        val_size=0.2,
        use_test_split=True
    )
    print("âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\n")
    
    # 3. í´ë˜ìŠ¤ë³„ ë¶„ì„
    class_results, all_classes = analyze_class_patterns(model, datamodule)
    
    # 4. ì‹œê°í™”
    print("\nğŸ¨ ì‹œê°í™” ìƒì„± ì¤‘...")
    plot_category_comparison(class_results)
    plot_channel_importance_heatmap(class_results)
    plot_temporal_importance_distribution(class_results)
    
    # 5. í†µê³„ ê³„ì‚° ë° ì¶œë ¥
    print("\nğŸ“ˆ í†µê³„ ê³„ì‚° ì¤‘...")
    stats = calculate_statistics(class_results)
    print_statistics(stats)
    
    # 6. ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    np.savez(OUTPUT_DIR / "class_analysis_results.npz",
             **class_results,
             stats=stats)
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}/")


if __name__ == "__main__":
    main()

