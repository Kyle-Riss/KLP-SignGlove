"""
ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
- Improved_2 ì„¤ì •ìœ¼ë¡œ í›ˆë ¨
- 97.5% ì„±ëŠ¥ ë‹¬ì„±
"""
import sys
import os
from pathlib import Path
import pytorch_lightning as pl
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
from pytorch_lightning.loggers import CSVLogger

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from src.misc.DynamicDataModule import DynamicDataModule
from src.StackedGRUModel import StackedGRULightning
from optimal_config import OPTIMAL_CONFIG

def train_optimal_model():
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í›ˆë ¨"""
    print("ğŸš€ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (Improved_2) í›ˆë ¨ ì‹œì‘")
    print("=" * 50)
    
    # ë°ì´í„° ëª¨ë“ˆ ì„¤ì •
    data_module = DynamicDataModule(
        data_dir='/home/billy/25-1kp/SignGlove-DataAnalysis',
        time_steps=OPTIMAL_CONFIG['time_steps'],
        n_channels=OPTIMAL_CONFIG['n_channels'],
        batch_size=OPTIMAL_CONFIG['batch_size'],
        use_test_split=True,
        seed=OPTIMAL_CONFIG['seed']
    )
    
    # ëª¨ë¸ ìƒì„±
    model = StackedGRULightning(
        input_size=OPTIMAL_CONFIG['n_channels'],
        hidden_size=OPTIMAL_CONFIG['hidden_size'],
        num_classes=OPTIMAL_CONFIG['num_classes'],
        num_layers=OPTIMAL_CONFIG['num_layers'],
        dropout=OPTIMAL_CONFIG['dropout'],
        dense_size=OPTIMAL_CONFIG['dense_size'],
        bidirectional=OPTIMAL_CONFIG['bidirectional'],
        learning_rate=OPTIMAL_CONFIG['learning_rate'],
        weight_decay=OPTIMAL_CONFIG['weight_decay'],
        adamw_beta1=OPTIMAL_CONFIG['adamw_beta1'],
        adamw_beta2=OPTIMAL_CONFIG['adamw_beta2'],
        adamw_eps=OPTIMAL_CONFIG['adamw_eps'],
        lr_scheduler_factor=OPTIMAL_CONFIG['lr_scheduler_factor'],
        lr_scheduler_patience=OPTIMAL_CONFIG['lr_scheduler_patience'],
        lr_scheduler_min_lr=OPTIMAL_CONFIG['lr_scheduler_min_lr'],
        lr_scheduler_threshold=OPTIMAL_CONFIG['lr_scheduler_threshold']
    )
    
    # ì½œë°± ì„¤ì •
    early_stopping = EarlyStopping(
        monitor='val/accuracy',
        patience=OPTIMAL_CONFIG['early_stopping_patience'],
        mode='max',
        verbose=True
    )
    
    checkpoint_callback = ModelCheckpoint(
        monitor='val/accuracy',
        dirpath='./checkpoints/',
        filename='optimal-model-{epoch:02d}-{val/accuracy:.4f}',
        save_top_k=1,
        mode='max',
        verbose=True
    )
    
    # ë¡œê±° ì„¤ì •
    logger = CSVLogger(
        save_dir='./logs/',
        name='optimal_model'
    )
    
    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    trainer = pl.Trainer(
        max_epochs=OPTIMAL_CONFIG['max_epochs'],
        logger=logger,
        callbacks=[checkpoint_callback, early_stopping],
        accelerator="auto",
        devices="auto",
        log_every_n_steps=10,
        enable_progress_bar=True
    )
    
    # í›ˆë ¨
    trainer.fit(model, data_module)
    
    # í…ŒìŠ¤íŠ¸
    test_results = trainer.test(model, data_module)
    
    print("\n" + "=" * 50)
    print("ğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
    print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_results[0]['test/accuracy']:.4f}")
    print(f"í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_results[0]['test/loss']:.4f}")
    print("=" * 50)

if __name__ == "__main__":
    train_optimal_model()
