#!/usr/bin/env python3
"""
ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸
ì—¬ëŸ¬ ëª¨ë¸ì˜ í•™ìŠµ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ì‹œê°í™”í•©ë‹ˆë‹¤.
"""

import os
import re
import numpy as np
import matplotlib.pyplot as plt
import glob
from typing import Dict, List, Tuple, Optional
import argparse

def parse_log_file(log_file: str) -> Dict[str, List[float]]:
    """ë¡œê·¸ íŒŒì¼ì—ì„œ ë©”íŠ¸ë¦­ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    metrics = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'val_f1': [],
        'learning_rate': []
    }
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Epoch-averaged ë¡œê·¸ í˜•ì‹ íŒŒì‹±
        epoch_pattern = r'Epoch (\d+):.*?train/loss=([\d.]+).*?train/accuracy=([\d.]+).*?val/loss=([\d.]+).*?val/accuracy=([\d.]+|nan\.0).*?val/f1_score=([\d.]+).*?learning_rate=([\d.]+)'
        matches = re.findall(epoch_pattern, content)
        
        for match in matches:
            epoch, train_loss, train_acc, val_loss, val_acc, val_f1, lr = match
            
            # NaN ê°’ ì²˜ë¦¬
            val_acc = 0.0 if val_acc == 'nan.0' else float(val_acc)
            
            metrics['train_loss'].append(float(train_loss))
            metrics['train_acc'].append(float(train_acc))
            metrics['val_loss'].append(float(val_loss))
            metrics['val_acc'].append(val_acc)
            metrics['val_f1'].append(float(val_f1))
            metrics['learning_rate'].append(float(lr))
            
    except Exception as e:
        print(f"âŒ ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {log_file} - {e}")
        return {}
    
    return metrics

def plot_model_comparison(model_results: Dict[str, Dict], save_dir: str = "model_comparison"):
    """ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"""
    os.makedirs(save_dir, exist_ok=True)
    
    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']
    
    # 1. Loss ë¹„êµ
    plt.figure(figsize=(15, 10))
    
    # Train Loss
    plt.subplot(2, 3, 1)
    for i, (model_name, metrics) in enumerate(model_results.items()):
        if metrics['train_loss']:
            epochs = list(range(1, len(metrics['train_loss']) + 1))
            plt.plot(epochs, metrics['train_loss'], 
                    label=f'{model_name} (Train)', 
                    color=colors[i % len(colors)], 
                    linewidth=2, alpha=0.8)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training Loss Comparison', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Validation Loss
    plt.subplot(2, 3, 2)
    for i, (model_name, metrics) in enumerate(model_results.items()):
        if metrics['val_loss']:
            epochs = list(range(1, len(metrics['val_loss']) + 1))
            plt.plot(epochs, metrics['val_loss'], 
                    label=f'{model_name} (Val)', 
                    color=colors[i % len(colors)], 
                    linewidth=2, alpha=0.8)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Validation Loss Comparison', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Train Accuracy
    plt.subplot(2, 3, 3)
    for i, (model_name, metrics) in enumerate(model_results.items()):
        if metrics['train_acc']:
            epochs = list(range(1, len(metrics['train_acc']) + 1))
            plt.plot(epochs, metrics['train_acc'], 
                    label=f'{model_name} (Train)', 
                    color=colors[i % len(colors)], 
                    linewidth=2, alpha=0.8)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training Accuracy Comparison', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Validation Accuracy/F1
    plt.subplot(2, 3, 4)
    for i, (model_name, metrics) in enumerate(model_results.items()):
        if metrics['val_acc'] and any(acc > 0 for acc in metrics['val_acc']):
            epochs = list(range(1, len(metrics['val_acc']) + 1))
            plt.plot(epochs, metrics['val_acc'], 
                    label=f'{model_name} (Val Acc)', 
                    color=colors[i % len(colors)], 
                    linewidth=2, alpha=0.8)
        elif metrics['val_f1']:
            epochs = list(range(1, len(metrics['val_f1']) + 1))
            plt.plot(epochs, metrics['val_f1'], 
                    label=f'{model_name} (Val F1)', 
                    color=colors[i % len(colors)], 
                    linewidth=2, alpha=0.8)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy/F1 Score')
    plt.title('Validation Performance Comparison', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # F1 Score
    plt.subplot(2, 3, 5)
    for i, (model_name, metrics) in enumerate(model_results.items()):
        if metrics['val_f1']:
            epochs = list(range(1, len(metrics['val_f1']) + 1))
            plt.plot(epochs, metrics['val_f1'], 
                    label=f'{model_name}', 
                    color=colors[i % len(colors)], 
                    linewidth=2, alpha=0.8)
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.title('Validation F1 Score Comparison', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Learning Rate
    plt.subplot(2, 3, 6)
    for i, (model_name, metrics) in enumerate(model_results.items()):
        if metrics['learning_rate']:
            epochs = list(range(1, len(metrics['learning_rate']) + 1))
            plt.plot(epochs, metrics['learning_rate'], 
                    label=f'{model_name}', 
                    color=colors[i % len(colors)], 
                    linewidth=2, alpha=0.8)
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/model_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. ìµœì¢… ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸”
    create_performance_summary(model_results, save_dir)
    
    print(f"âœ… ëª¨ë¸ ë¹„êµ ì‹œê°í™” ì™„ë£Œ: {save_dir}/")

def create_performance_summary(model_results: Dict[str, Dict], save_dir: str):
    """ìµœì¢… ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸” ìƒì„±"""
    summary_data = []
    
    for model_name, metrics in model_results.items():
        if not metrics:
            continue
            
        # ìµœì¢… ì„±ëŠ¥ ì¶”ì¶œ
        final_train_loss = metrics['train_loss'][-1] if metrics['train_loss'] else 0
        final_train_acc = metrics['train_acc'][-1] if metrics['train_acc'] else 0
        final_val_loss = metrics['val_loss'][-1] if metrics['val_loss'] else 0
        final_val_acc = metrics['val_acc'][-1] if metrics['val_acc'] else 0
        final_val_f1 = metrics['val_f1'][-1] if metrics['val_f1'] else 0
        
        # ìµœê³  ì„±ëŠ¥ ì¶”ì¶œ
        best_val_loss = min(metrics['val_loss']) if metrics['val_loss'] else 0
        best_val_f1 = max(metrics['val_f1']) if metrics['val_f1'] else 0
        
        summary_data.append({
            'Model': model_name,
            'Final Train Loss': f"{final_train_loss:.4f}",
            'Final Train Acc': f"{final_train_acc:.4f}",
            'Final Val Loss': f"{final_val_loss:.4f}",
            'Final Val Acc': f"{final_val_acc:.4f}",
            'Final Val F1': f"{final_val_f1:.4f}",
            'Best Val Loss': f"{best_val_loss:.4f}",
            'Best Val F1': f"{best_val_f1:.4f}",
            'Epochs': len(metrics['train_loss'])
        })
    
    # í…Œì´ë¸” ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(16, 8))
    ax.axis('tight')
    ax.axis('off')
    
    if summary_data:
        # ë°ì´í„° ì¤€ë¹„
        headers = list(summary_data[0].keys())
        rows = [[row[header] for header in headers] for row in summary_data]
        
        # í…Œì´ë¸” ìƒì„±
        table = ax.table(cellText=rows, colLabels=headers, cellLoc='center', loc='center')
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1.2, 2)
        
        # ìŠ¤íƒ€ì¼ë§
        for i in range(len(headers)):
            table[(0, i)].set_facecolor('#40466e')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        for i in range(1, len(rows) + 1):
            for j in range(len(headers)):
                if i % 2 == 0:
                    table[(i, j)].set_facecolor('#f0f0f0')
                else:
                    table[(i, j)].set_facecolor('white')
    
    plt.title('Model Performance Summary', fontsize=16, fontweight='bold', pad=20)
    plt.savefig(f'{save_dir}/performance_summary.png', dpi=300, bbox_inches='tight')
    plt.close()

def main():
    parser = argparse.ArgumentParser(description='Model Comparison Visualization')
    parser.add_argument('--log_dir', type=str, default='.', help='Directory containing log files')
    parser.add_argument('--save_dir', type=str, default='model_comparison', help='Directory to save comparison plots')
    parser.add_argument('--models', nargs='+', help='Specific models to compare (e.g., GRU LSTM MSCSGRU)')
    
    args = parser.parse_args()
    
    # ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
    log_files = glob.glob(f"{args.log_dir}/training_output_*.log")
    
    if not log_files:
        print("âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ” {len(log_files)}ê°œì˜ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    
    # ëª¨ë¸ë³„ ê²°ê³¼ ìˆ˜ì§‘
    model_results = {}
    
    for log_file in log_files:
        # íŒŒì¼ëª…ì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ
        filename = os.path.basename(log_file)
        if 'training_output_' in filename:
            model_name = filename.replace('training_output_', '').replace('.log', '')
        else:
            model_name = os.path.splitext(filename)[0]
        
        # íŠ¹ì • ëª¨ë¸ë§Œ ë¹„êµí•˜ëŠ” ê²½ìš° í•„í„°ë§
        if args.models and model_name not in args.models:
            continue
        
        print(f"ğŸ“Š {model_name} ëª¨ë¸ íŒŒì‹± ì¤‘...")
        metrics = parse_log_file(log_file)
        
        if metrics:
            model_results[model_name] = metrics
            print(f"  âœ… {len(metrics['train_loss'])} ì—í¬í¬ ë°ì´í„° íŒŒì‹± ì™„ë£Œ")
        else:
            print(f"  âŒ íŒŒì‹± ì‹¤íŒ¨")
    
    if not model_results:
        print("âŒ íŒŒì‹±ëœ ëª¨ë¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“ˆ {len(model_results)}ê°œ ëª¨ë¸ ë¹„êµ ì‹œê°í™” ì‹œì‘...")
    plot_model_comparison(model_results, args.save_dir)
    print("âœ… ëª¨ë¸ ë¹„êµ ì™„ë£Œ!")

if __name__ == "__main__":
    main()

